{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|><|introduction|><|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with Dylan Patel and Nathan Lambert. Dylan runs SemiAnalysis, a well-respected research and analysis company that specializes in semiconductors, GPUs, CPUs, and AI hardware in general. Nathan is a research scientist at the Allen Institute for AI and is the author of the amazing blog on AI called Interconnects. They are both highly respected, read and listened to by the experts, researchers and engineers in the field of AI. And personally, I'm just a fan of the two of them, so I used the DeepSeek moment that shook the AI world a bit as an opportunity to sit down with them and lay it all out from DeepSeek, OpenAI, Google XAI, Meta, Anthropic to NVIDIA and DSMC, and to US-China-Taiwan relations and everything else that is happening at the cutting edge of AI. This conversation is a deep dive into many critical aspects of the AI industry. While it does get super technical, we try to make sure that it's still accessible to folks outside of the AI field by defining terms, stating important concepts explicitly, spelling out acronyms, and in general, always moving across the several layers of abstraction and levels of detail. There is a lot of hype in the media about what AI is and isn't. The purpose of this podcast in part is to cut through the hype, through the bullshit and the low resolution analysis and to discuss in detail how stuff works and what the implications are. Let me also, if I may comment on the new OpenAI o3-mini reasoning model, the release of which we were anticipating during the conversation and it did indeed come out right after. Its capabilities and costs are on par with our expectations as we stated. OpenAI o3-mini is indeed a great model, but it should be stated that DeepSeek-R1 has similar performance on benchmarks, is still cheaper and it reveals its chain of thought reasoning, which o3-mini does not. It only shows a summary of the reasoning, plus R1 is open weight and o3-mini is not. By the way, I got a chance to play with o3-mini and anecdotal vibe check wise, I felt that o3-mini, specifically o3-mini high is better than R1. Still for me personally, I find that Claude Sonnet 3.5 is the best model for programming except for tricky cases where I will use o1 Pro to brainstorm. Either way, many more better AI models will come including reasoning models both from American and Chinese companies. They'll continue to shift the cost curve, but the quote \"DeepSeek moment\" is indeed real. I think it will still be remembered five years from now as a pivotal event in tech history due in part to the geopolitical implications, but for other reasons to, as we discuss in detail from many perspectives in this conversation. This is the Lex Fridman podcast, to support it please check out our sponsors in the description. And now, dear friends, here's Dylan Patel and Nathan Lambert. A lot of people are curious to understand China's DeepSeek AI models, so let's lay it out. Nathan, can you describe what DeepSeek-V3 and DeepSeek-R1 are, how they work, how they're trained? Let's look at the big picture and then we'll zoom in on the details.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of them are. There's a lot of technical specifics on training and go through them high level to specific and go through each of them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's so many places we can go here, but maybe let's go to open weights first. What does it mean for a model to be open weights and what are the different flavors of open source in general?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>pay to rent those yourselves. But without the data, the replication cost is going to be far, far higher. And same goes for the code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We should also say that this is probably one of the more open models out of the frontier models.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>In this full spectrum where probably the fullest open source, like you said, open code, open data, open weights, this is not open code, this is probably not open data and this is open weights and the licensing is MIT license or it's... There's some nuance in the different models, but it's towards the free... In terms of the open source movement, these are the good guys.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>could get into this whole rabbit hole, I think. We'll make sure we want to go down the license rabbit hole before we do specifics.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It should be stated that one of the implications that DeepSeek, it puts pressure on Llama and everybody else on OpenAI to push towards open source. And that's the other side of open source is that you mentioned is how much is published in detail about it, so how open are you with the insights behind the code? How good is the technical reports? Are there hand wavy or is there actual details in there? And that's one of the things that DeepSeek did well is they published a lot of the details.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and some of them are at DeepSeek. These types of people are at DeepSeek and leading American frontier labs, but there are not many places.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To help people understand the other implication of open weights, just there's a topic we'll return to often here. There's a fear that China, the nation might have interest in stealing American data, violating privacy of American citizens. What can we say about open weights to help us understand what the weights are able to do in terms of stealing people's data?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are you have your fate of data in your own hands, and that is something that is deeply connected to the soul of open source.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's not the model that steals your data, it's whoever is hosting the model, which could be China if you're using the DeepSeek app or it could be Perplexity. You're trusting them with your data or OpenAI, you're trusting them with your data. And some of these are American companies, some these are Chinese companies, but the model itself is not doing the stealing, it's the host. All right, so back to the basics. What's the difference between DeepSeek-V3 and DeepSeek-R1? Can we try to lay out the confusion potential?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but also it is a far newer set of techniques for the AI community, so it is a much more rapidly evolving area of research.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe we should also say the big two categories of training of pre-training and post-training. These are umbrella terms that people use, so what is pre-training and what is post-training and what are the different flavors of things underneath the post-training umbrella?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>for multiple years. This is the idea of using reinforcement learning with language models and it has been taking off especially in this DeepSeek moment.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we should say that there's a lot of exciting stuff going on again across the stack, but the post-training probably this year, there's going to be a lot of interesting developments in the post-training. We'll talk about it. I almost forgot to talk about the difference between DeepSeek-V3 and R1 on the user experience side. Forget the technical stuff, forget all of that, just people that don't know anything about AI, they show up. What's the actual experience, what's the use case for each one when they actually type and talk to it? What is each good at and that kind of thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it'll click through, it'll say breaking down the problem, making X calculation, cleaning the result, and then the answer will come for something like OpenAI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe it's useful here to go through an example of a DeepSeek-R1 reasoning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>157 seconds with the drop-down arrow. Underneath that if we were in an app that we were running, the drop-down arrow would have the reasoning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>In this case, the specific question which I'm philosophically/ pothead inclined, so this is asking DeepSeek-R1 for one truly novel insight about humans. And it reveals the reasoning and basically the truly novel aspect is what's pushing the reasoning to constantly... The model asking itself, is this truly novel? It's actually challenging itself to be more novel, more counterintuitive, less cringe, I suppose. Some of the reasoning says... This is just snapshots. Alternatively, humans have a unique meta-emotion where they feel emotions about their own emotions e.g. feeling guilty about being angry. This recursive emotional layering creates complex motivational drives that don't exist in other animals. The insight is that human emotions are nested. It's reasoning through how humans feel emotions, it's reasoning about meta emotions.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's going to have pages and pages of this. It's almost too much to actually read, but it's nice to skim as it's coming.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a James Joyce like stream of consciousness. And then it goes, wait, the user wants something that's not seen anywhere else. Let me dig deeper. And consider the human ability to hold contradictory beliefs simultaneously. Cognitive dissonance is known, but perhaps the function is to allow flexible adaptation, so on and so forth. That really captures the public imagination that holy shit, this isn't intelligence/almost like an inkling of sentience because you're thinking through, you're self-reflecting, you're deliberating. And the final result of that after 157 seconds is, humans instinctively convert selfish desires into cooperative systems by collectively pretending abstract rules, money, laws, rights are real. These shared hallucinations act as quote \"games\" where competition is secretly redirected to benefit the group, turning conflict into society's fuel. Pretty profound.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a at least interesting example I think depending on how open-minded you are, you find language models interesting or not, and there's a spectrum there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We'll talk about different benchmarks and so on but some has just a vibe. That in itself is a, let's say quote \"fire\" tweet. If I'm trying to produce something where people are like, \"Oh, shit.\" Okay, so that's a chance probably return to it more. How were they able to achieve such low cost on the training and the inference? Maybe you could talk to the training first.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>training and inference with this mixture of experts architecture. Should we break down where it actually applies and go into the transformer? Is that useful?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's go. Let's go into the transformer.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>gains from those mixture of experts on parameter efficiency at training and inference because you get this efficiency by not activating all of these parameters.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We should also say that a transformer is a giant neural network.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then there's, for 15 years now, there's what's called the deep learning revolution. Network's gotten larger and larger. At a certain point, the scaling laws appeared where people realized-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>This is a scaling law shirt by the way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Representing scaling laws. Where it became more and more formalized that bigger is better across multiple dimensions of what bigger means. But these are all neural networks we're talking about, and we're talking about different architectures of how to construct these neural networks such that the training and the inference on them is super efficient.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but they actually shared them with the world, which is really nice to be like, this is the cutting edge of efficient language model training.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And some of this requires low level engineering, just it is a giant mess in trickery. As I understand they went below CUDA, so they go super low programming of GPUs.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>every single layer of the model, and you may have over a hundred layers. What does NCCL stand for? It's NCCL. Nvidia Communications Collectives Library.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Nice. Damn.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>detailed work that they figured out in the public first, and potentially second or third in the world and maybe even first in some cases.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What lesson do you, in the direction of The Bitter Lesson do you take from all of this? Is this going to be the direction where a lot of the gain is going to be, which is this kind of low-level optimization or is this a short-term thing where the biggest gains will be more on the algorithmic high-level side of post-training? Is this a short-term leap because they've figured out a hack because constraints necessitate the mother of invention or is there still a lot of gains?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to give them the simple loss landscape where you put compute through the model and they will learn, and getting barriers out of the way.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's where the power of something like nickel comes in, where standardized code that could be used by a lot of people to create simple innovations that can scale, which is why the hacks, I imagine, the code base for DeepSeek is probably a giant mess.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>stuff that trains the DeepSeek V3 and DeepSeek-R1, those libraries, if you were to present them to us, I would guess are extremely high-quality code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, high-quality, readable code. Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in many cases, but it's worthwhile for them to do the specific optimization for the specific run given the constraints that they have regarding compute.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I wonder how stressful it is to these frontier models, like initiate training to have the code-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Push the button.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... to push the button that you're now spending a large amount of money and time to train this. I mean, there must be a lot of innovation on the debugging stage of making sure there's no issues, that you're monitoring and visualizing every aspect of the training, all that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>just like, \"Is the loss ... Is the loss spike okay?\" Yeah. It's like tokens per second. Loss not blown up. They're just watching this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the heart rate goes up if there's a spike.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's not a gradual process, and that's what humans are like. That's what models are like. So it's really a stressful task, as you mentioned.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the whole time the dollar count is going up.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>yes, it's noteworthy because it's a lot of money and it can be week to a month setback, but it is part of the process.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But if you're DeepSeek, how do you get to a place where holy shit, there's a successful combination of hyperparameters?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>A lot of small failed runs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, rapid iteration through failed runs until-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And successful ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then you build up some intuition, like this mixture of expert works and then this implementation of MLA works.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>complexity, and it's just continuing to build the capabilities that they have. There's the concept of a YOLO run. So YOLO, you only live once.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yep.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to work in post-training because the GPU cost for training is lower. So you can make a higher percentage of your training runs YOLO runs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, for now.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For now. For now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So some of this is fundamentally luck, still.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Luck is skill, right, in many cases?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I mean, it looks lucky, right, when you're-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>reason was, maybe they weren't as skilled. Whatever, you can call it luck if you want, but at the end of the day, it's skill.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So 2025 is the year of the YOLO run. It seems like all the labs are going in.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"Hey, let me spend a couple of hundred million dollars, which is all of the money I have on this model.\" That is truly YOLO.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>than what OpenAI did in 2022, or maybe what DeepSeek did now or sort of like, \"Hey, we're just going to throw everything at it.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>The big winners throughout human history are the ones who are willing to do YOLO at some point. Okay. What do we understand about the hardware it's been trained on, DeepSeek?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to do this, we need to do something different.\" DeepSeek is his way of doing this. Some of the translated interviews with him are fantastic.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So he has done interviews?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think you would do a western interview, or no, or is there controls on the channel?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There hasn't been one yet, but I would try it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. All right. Well, I just got a Chinese translator, so it was great. This is a push. So fascinating figure, engineer pushing full on into AI, leveraging the success from the high-frequency trading.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do this crazy SM scheduling stuff. So going back to that, it's like this is obviously not true in terms of their total GPU count.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Obvious available GPUs, but for this training run, you think 2,000 is the correct number, or no?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>for any given model that is a notable advancement, you're going to do two to 4x compute of the full training run in experiments alone.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So a lot of this compute that's being scaled up is probably used in large part at this time for research?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>begets the new ideas that lets you get huge efficiency. Research gets you o1. Research gets you breakthroughs and you need to bet on it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So some of the pricing strategy that we'll discuss has the research baked into the price?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they had 10,000 GPUs in 2021. So, some of the research that we've found is that we actually believe they have closer to 50,000 GPUs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We as semi-analysis. So we should say that you're sort of one of the world experts in figuring out what everybody's doing in terms of the semiconductor, in terms of cluster buildouts, in terms of who is doing what in terms of training runs. So yeah, that's the we. Okay, go ahead.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Their compute allocation is one of the top few in the world that's not OpenAI, Anthropic, et cetera, but they have a lot of compute.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you in gentlemen actually just zoom out and also talk about the Hopper architecture, the Nvidia Hopper GPU architecture and the difference between H100 and H800, like you mentioned, the interconnects?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>bandwidth and memory capacity. So Nvidia is working within the constraints of what the government sets and then builds the best possible GPU for China.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can we take this actual tangent and we'll return back to the hardware, is the philosophy, the motivation, the case for export controls? What is it? Dario Amodei just published a blog post about export controls. The case he makes is that if AI becomes super powerful and he says by 2026, we'll have AGI or super powerful AI and that's going to give a significant ... Whoever builds that will have a significant military advantage. And so because The United States is a democracy and as he says, China is authoritarian or has authoritarian elements, you want a unipolar world where the super powerful military, because of the AI is one that's a democracy. It's a much more complicated world geopolitically when you have two superpowers with super powerful AI and one is authoritarian. So, that's the case he makes. And so the United States wants to use export controls to slow down, to make sure that China can't do these gigantic training runs that will be presumably required to build the AGI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>if the main result for o3 is also a spectacular coding performance, and if that feeds back into AI companies being able to experiment better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So presumably, the idea is for an AGI, a much larger fraction of the compute would be used for this test-time compute, for the reasoning, for the AGI goes into a room and thinks about how to take over the world and come back in 2.7 hours-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>This is what-<|eot_id|><|start_header_id|>assistant<|end_header_id|>... and that it's going to take a lot of compute.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they're independent and they can do tasks that aren't in the training data is what the few-year outlook that these AI companies are driving for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think the terminology here that Dario uses is super powerful AI. So I agree with you on the AGI. I think we already have something like that's exceptionally impressive that Alan Turing would for sure say is AGI, but he's referring more to something once in possession of, then you would have a significant military and geopolitical advantage over other nations. So it's not just like you can ask it how to cook an omelet.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>competent I am, and if AI can revolutionize biology. I am safe saying that AI is going to accelerate the progress of any computational science.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we're doing a depth-first search here on topics, taking tangent of a tangent, so let's continue on that depth-first search. You said that you're both feeling the AGI. What's your timeline? Dario is 2026 for the super powerful AI that's basically agentic to a degree where it's a real security threat, that level of AGI. What's your timeline?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>way of describing this, and I just think that it's like there's going to be more than that, so just expect these things to come.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm going to have to try to pin you down to a date on the AGI timeline. Like the nuclear weapon moment, so moment where on the geopolitical stage, there's a real... Because we're talking about export controls, when do you think, just even to throw out a date, when do you think that would be? For me, it's probably after 2030, so I'm not as-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>countries that are allies. Singapore. They freaking have F-35s and we don't let them buy GPUs. This to me is already to the scale of...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that just means that the US military is really nervous about this new technology. That doesn't mean that technology is already there. So they might be just very cautious about this thing that they don't quite understand. But that's a really good point. The robocalls, swarms of semi-intelligent bots could be a weapon, could be doing a lot of social engineering.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>wants to... Because it'll be more capitalistically allocated just towards whatever is the highest return on income, which might be building factories better or whatever.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So everything I've seen, people's intuition seems to fail on robotics. So you have this kind of general optimism. I've seen this on self-driving cars. People think it's much easier problem than it is. Similar with drones, here, I understand it a little bit less, but I've just seen the reality of the war in Ukraine and the usage of drones on both sides. And it seems that humans still far outperform any fully autonomous systems. AI is an assistant, but humans drive. FPV drones where the human's controlling most of it, just far, far, far outperforms AI systems. So I think it's not obvious to me that we're going to have swarms of autonomous robots anytime soon in the military context. Maybe the fastest I can imagine is 2030, which is why I said 2030 for the super powerful AI. Whenever you have large scale swarms of robots doing military actions, that's when the world just starts to look different to me. So that's the thing I'm really worried about. But there could be cyber war, cyber war type of technologies that from social engineering to actually just swarms of robots that find attack vectors in our code bases and shut down power grids, that kind of stuff. And it could be one of those things like on any given weekend or something, power goes out, nobody knows why, and the world changes forever. Just power going out for two days in all of the United States, that will lead to murder, to chaos. But going back to export controls, do you see that as a useful way to control the balance of power geopolitically in the context of AI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>build the largest data center in the world, if they had access to the chips, could. So it's just a question of when, not if.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So their industrial capacity far exceeds the United States'?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>They manufacture stuff. So long-term, they're going to be manufacturing chips there?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>nanometer chips that are not profitable and things like this. So this is how it all feeds back into the economic engine of export controls.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, so you're saying that for now, Xi Jinping has not felt the AGI, but it feels like the DeepSeek moment, there might be meetings going on now where he's going to start wearing the same t-shirt and things are going to escalate.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>met the second command guy, and they had a meeting, and then the next day, they announced the AI subsidies, which are a trillion RMB.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it's possible that this DeepSeek moment is truly the beginning of a cold war.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>what a lot of people are worried about. People in AI have been worried that this is going towards a cold war or already is.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But it's not DeepSeek's fault, but there's something, a bunch of factors came together where-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's how history works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... it's like this explosion. I mean, it all has to do with NVIDIA's not going down properly, but it's just some [inaudible 01:27:28] mass hysteria that happened that eventually led to Xi Jinping having meetings and waking up to this idea.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think, to at least National Security Council and those sort of folks, that this was where the world is headed, this cold war that's happening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So is there any concerns that the export controls push China to take military action on Taiwan?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or whatever. You would know more about this, but there's all sorts of... Everything about semiconductors is so integral to every part of our lives.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So can you explain the role of TSMC in the story of semiconductors and maybe also how the United States can break the reliance on TSMC?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of companies build their own chips, and XP, but more and more of these companies are outsourcing to TSMC and have been for multiple decades.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain the supply chain there and where most of TSMC is in terms of manufacturing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>era of foundry. Every other company started before then, and at some point had fabs, which is actually incredible. Like AMD and Intel and Broadcom-<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 01:33:48].<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>now, everyone relies on TSMC. Including Intel, their latest PC chip uses TSMC chips. It also uses some Intel chips, but it uses TSMC process.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain why the foundry model is so successful for these companies? Why are they going with-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Economies of scale.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Scale?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they make good money and they're able to aggregate all this demand and continue to build the next fab, the next fab, the next fab.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So why is Taiwan so special for TSMC? Why is it happening there? Can it be replicated inside the United States?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have have been TSMC, but Texas Semiconductor Manufacturing instead of Texas Instruments. So there is that whole story there. But the- Sitting here in Texas.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that sounds like a human story. He didn't get promoted.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>say is if I had a few missiles, I know exactly where I could cause the most economic damage. It's not targeting the White House.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the R&D centers.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's the R&D centers for TSMC, Intel, Samsung. And then some of the memory guys, Micron and Hynix.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because they define the future evolution of these semiconductors, and everything's moving so rapidly that it really is fundamentally about R&D. And it is all about TSMC. Huh.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>things you can purchase ironically, is a Texas Instruments graphing calculator because they actually manufacture in Texas. But outside of that, a laptop, a phone.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's depressing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>economic discussion as well, which is they have far more than we do. And having supply for things like this is crucial to normal life.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So they're starting to invest in high-volume manufacturer, but they're not doing R&D as much?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it's going to take a ton of money. I truly think to revolutionize and completely in-source semiconductors would take a decade and a trillion dollars.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is some of it also culture, like you said, extreme competence, extreme work ethic in Taiwan?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in the past, they could do it now. We can just import, I say import, the best people in the world if we want to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's where the immigration conversation is a tricky one and there's been a lot of debate over that. But yeah, it seems absurdly controversial to import the best people in the world. I don't understand why it's controversial. That's one of the ways of winning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of things to get much more expensive, but does it change the equation for TSMC building more fabs in the US? That's what he's positing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we laid out the importance ... By the way, it's incredible how much you know about so much.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We told you Dylan knows all this stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Okay. You laid out why TSMC is really important. If we look out into the future 10, 20 years out, US-China relationship, it seems like it can go to a dark place of Cold War, escalated Cold War, even hot war, or to a good place of anything from frenemies, to cooperation, to working together. So in this game theory, complicated game, what are the different trajectories? What should US be doing? What do you see as the different possible trajectories of US-China relations as both leaders start to feel the AGI more and more and see the importance of chips and the importance of AI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>zero idea, and I would love if we could all hold hands and sing Kumbaya, but I have zero idea how that could possibly happen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is the divergence good or bad for avoiding war? Is it possible that the divergence in terms of manufacturer chips of training AI systems is actually good for avoiding military conflict?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>position, and what China is trying to do is become the top position. And obviously there's butting of heads here, in the most simple terms.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that could take shape in all kinds of ways, including proxy wars. And now-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>as advantageous of a position if that happens, but this is the reality of what's being done and the actions that are being carried out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can we go back to the specific detail of the different hardware? There's this nice graphic in the export controls of which GPUs are allowed to be exported and which are not. Can you explain the difference? From a technical perspective, are the H20s promising?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>as important. And especially when we come to this new paradigm, which the world is only just learning about over the last six months: reasoning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And do we understand firmly which of the three dimensions is best for reasoning? So interconnect, the FLOPS don't matter as much, is it memory?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fast. I would say there's two articles in this one that I could show maybe graphics that might be interesting for you to pull up.<|eot_id|><|start_header_id|>assistant<|end_header_id|>For the listeners, we're looking at the section of 01 inference architectures tokenomics.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>How each token, each word in the context length, is relatively connected to each other. And I think, Nathan, you can explain KV cache better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>KV cache is one of the optimization [inaudible 02:00:33]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And then there's innovations on attention to make this memory usage and the ability to attend over long contexts much more accurate and high performance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And those innovations are going to help you with ... I mean, your highly memory constrained in this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>difference is that I can do that input, i.e. the prefill, i.e. the prompt, simultaneously in a batch nature and therefore it is all FLOP.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think the pricing model mostly they use for input tokens is about one fourth of price of the output tokens.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going way up with these reasoning models and you still have a lot of users, so effectively the cost to serve multiplies by a ton.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we're looking at a plot when the x-axis is sequence length.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I put in a book, that's a million tokens. But if I put in \"the sky is blue,\" then that's like six tokens or whatever.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we should say that what we're calling reasoning and chain of thought is extending this sequence length.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>capped or the number of users you could serve- Let's say the model. So this is showing for a 405B model in batch size 64.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Llama 3.1.405B. Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>other aspect is they did it so cheap. And the so cheap, we talked about on the training side why it was so cheap slash-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, let's talk about why it's so cheap on the inference. It works well and it's cheap. Why is R1 so damn cheap?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And this dramatically reduces the memory pressure. It's still there, it's still attention, it's still quadratic, it's just dramatically reduced it relative to prior forms.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. That's the memory pressure. I should say, in case people don't know, R1 is 27 times cheaper than o1.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We think that OpenAI had a large margin built in.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, so that's one-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's multiple factors. We should break down the factors, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's two bucks per million token output for R1 and $60 per million token output for o1.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>there's two factors here. Their model is cheaper. It is 27 times cheaper. I don't remember the number exactly off the top of my head.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We're looking at a graphic that's showing different places serving V3, DeepSeek-V3, which is similar to DeepSeek-R1. And there's a vast difference in-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>In serving cost.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... in serving cost. And what explains that difference?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>factor right there of the cost difference, is that OpenAI is just making crazy amounts of money because they're the only one with the capability.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do they need that money? Are they using it for R&D?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>yes, they need that money because the revenue and margins pay for continuing to build the next thing, as long as I'm raising more money.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the suggestion is that DeepSeek is really bleeding out money.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>efficiency differences. It's like all their low-level libraries that we talked about in training, some of them probably translate to inference and those weren't released.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we may go a bit into conspiracy land, but is it possible the Chinese government is subsidizing DeepSeek?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>CCP might necessarily want. Now, to be clear, does he have a loss leader because he can fund it through his hedge fund? Yeah, sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the hedge fund might be subsidizing it, [inaudible 02:16:17]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>tool. Open sourcing. Open sourcing, recruiting tool. Mete, they were so far behind and they got so much talent because they just open sourced stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>More conspiracy thoughts. Is it possible, since they're a hedge fund, that they timed everything with this release and the pricing and they shorted Nvidia stock and stock of USA AI companies and released it with Stargate ... just perfect timing to be able to make money.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>know what is on the international calendar, but I mean, I don't expect them to. If you listen to their motivations for AI, it's like-<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, if you-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people are infatuated with ... you're like, \"You're telling me this is a high value thing and it works and it's doing this?\" It's amazing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's incredible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>response is very different, so the bar for safety is much higher, at least this is Anthropics' case, right? For DeepSeek, they're like, \"Ship,\" right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. So, the bar for safety is probably lowered a bit because of DeepSeek. There's parallels here to the space race. The reason the Soviets probably put a man in space first is because their approach to safety, the bar for safety, was lowered<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And they killed that dog, and all these things, so it's like...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Less risk averse than the US Space Program. And there's parallels here, but there's probably going to be downward pressure on that safety bar for the US companies.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people, especially not in AI, is that this is coming. There are some structural things in a global interconnected world that you have to accept.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You sent me something that Mark Zuckerberg mentioned on the earnings call. He said that, \"I think in light of some of the recent news, the new competitor DeepSeek from China, I think it's one of the things that we're talking about is there's going to be an open-source standard globally. And I think for our kind of national advantage, it's important that it's an American standard, so we take that seriously. We want to build the AI system that people around the world are using. And I think that, if anything, some of the recent news has only strengthened our conviction that this is the right thing to be focused on.\" So yeah, open-sourcing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>British English is dead because American LLMs won and the internet is American, and therefore, color is spelled the way Americans spell, and this is-<|eot_id|><|start_header_id|>assistant<|end_header_id|>A lot of strong words right now.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>LLMs. [inaudible 02:24:35] English is the hottest programming language and that English is defined by a bunch of companies that primarily are in San Francisco.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The right way to spell optimization is with a Z, just in case. I think it's an S in British English.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>into a social context that we're used to, which makes people very... We don't know the extent that which people can be impacted by that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, this is an actual concern with a Chinese company that is providing open-weights models, is that there could be some secret Chinese government requirement for these models to have a certain back door. To have some kind of thing where-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>subtle things. We don't know how they're going to generate tokens, what information they're going to represent, and what the complex representations they have are.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, we're talking about an Anthropic, which is generally just is permeated with good humans trying to do good in the world. We just don't know of any labs... This would be done in a military context that are explicitly trained to... Okay. The front door looks like a happy LLM, but underneath it's a thing that will over time do the maximum amount of damage to our, quote, unquote, \"enemies.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the model maker is, right? And again, today, I truly don't believe DeepSeek has done this, but it is a sign of what could happen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So one of the dystopian worlds is described by Brave New World, so we could just be stuck scrolling Instagram looking at cute puppies or worse, and then talking to bots that are giving us a narrative and we completely get lost in that world that's controlled by somebody else versus thinking independently. And that's a major concern as we rely more and more on these systems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>models are going to be trained. It's naive stuff, like talk to an anime girl, but it can be. Yeah, this is a risk, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a bit of a cliche thing to say, but I've, over the past year, I had a few stretches of time where I didn't use social media or the internet at all and just read books and was out in nature. And it clearly has a different effect on the mind where I feel I'm returning... Of course I was raised before the internet really took off, but I'm returning to some more-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>where you're going. You can see it physiologically. I take three days if I'm backpacking or something and you're literally, you're breaking down addiction cycles.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I feel I'm more in control of my mind. There feels like a sovereignty of intelligence that's happening when I'm disconnected from the internet. I think the more I use the internet and social media, the more other people are controlling my mind. That's definitely a feeling. And then in the future, that will be not other people, but algorithms, or other people presented to me via algorithms.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>already being used there. Obviously, video streaming and other technologies that have gone there first, it's going to come to the rest of society too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a general concern that models get censored by the companies that deploy them. So, one case where we've seen that, and maybe censorship is one word, alignment maybe via RLHF or some other way is another word. So we saw that with black Nazi image generation with Gemini. As you mentioned, we also see that with Chinese models refusing to answer what happened in June 4th, 1989, at Tiananmen Square, so how can this be avoided? And maybe can you just in general talk about how this happens, and how can it be avoided.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>pre-training data and look at all of this, and then that's terabytes of files and look for very specific words or hints of the words-<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, one way to say it is that you can insert censorship or alignment at various stages in the pipeline, and what you refer to now is at the very beginning of the data selection.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>try really, really hard, you can eventually get it to tell you about anthrax because they didn't filter it from the pre-training data set, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>But by the way, removing facts has such a ominous dark feel to it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I almost think it's practically impossible because you effectively have to remove them from the internet. You're taking on a-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Did they remove the mm-thing from the subreddits? The mmmm.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is this text? Is it close to a Wikipedia article? Which is a good thing that we want language models to be able to imitate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, couldn't you do a small language model that filter mentions at Tiananmen Square in the data?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And guess what? That's left-leaning. And so there are some aspects that you just can't censor unless you try really, really, really, really, really hard.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the base model will always have some TDS, Trump Derangement Syndrome, because it's trained so much.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It'll have the ability to express it. But what if-<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a wide representation in the data.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>them your date cutoff is this. Today's date is this.\" It's a lot of really useful context for how can you answer a question well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And Anthropic publishes their system prompt.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>most knowledgeable person, at least in the combination of execution and sharing, she's the person that should talk about system prompts and character of models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then people should read these system prompts because you're trying to nudge sometimes through extreme politeness the model to be a certain way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it was just a mess-up in execution that led to this ridiculous thing, but at the system level, the model weights might have been fine.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, at the very end of the pipeline there was a rewriting.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>something that is used at multiple levels of industry, and it's used effectively for image models. And the Gemini example is just a failed execution.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Big philosophical question here with RLHF. So, to generalize, where is human input, human in the loop, human data the most useful at the current stage?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>scale faster than the human part. But among the research that we have access to is that humans are in this kind of preference loop.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, as reasoning becomes bigger and bigger and bigger, as we said, where's the role of humans in that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>RL training where you reward the model for getting the question right, and therefore it's trying multiple solutions and it emerges this chain of thought.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This might be a good place to mention the eloquent and the insightful tweet of the great and the powerful Andrej Karpathy. I think he had a bunch of thoughts, but one of them, \"Last thought. Not sure if this is obvious. You know something profound is coming when you're saying it's not sure if it's obvious. There are two major types of learning in both children and in deep learning. There's one, imitation learning, watch and repeat i.e. pre-training, supervised fine-tuning, and two, trial-and-error learning, reinforcement learning. My favorite simple example is AlphaGo. One, is learning by imitating expert players. Two, is reinforcement learning to win the game. Almost every single shocking result of deep learning and the source of all magic is always two. Two is significantly more powerful. Two is what surprises you. Two is when the paddle learns to hit the ball behind the blocks in Breakout. Two is when AlphaGo beats even Lee Sedol. And two is the \"aha moment\" when the DeepSeek or o1, et cetera, discovers that it works well to reevaluate your assumptions, backtrack, try something else, et cetera. It's the solving strategies you see this model use in its chain of thought. It's how it goes back and forth thinking to itself. These thoughts are emergent. Three exclamation points. And this is actually seriously incredible, impressive, and new, and is publicly available and documented. The model could never learn this with the imitation because the cognition of the model and the cognition of the human labeler is different. The human would never know to correctly annotate these kinds of solving strategies and what they should even look like. They have to be discovered during reinforcement learning as empirically and statistically useful towards the final outcome.\" Anyway, the AlphaZero metaphor analogy here. Can you speak to that? The magic of the chain of thought that he's referring to.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of focal point, but that doesn't mean that the approach to technology is different, and the impact of the general training it's still incredibly new.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think that point would be? What would be move 37 for Chain of Thought for reasoning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>thousands of followers that are real and real engagement on Twitter because, all of a sudden, this is one of the things that are verifiable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And maybe not just engagement, but make money.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That could be the thing where almost fully automated, it makes $10 million by being an influencer, selling a product, creating the product. And I'm not referring to a hype product, but an actual product or like, \"Holy, shit, this thing created a business. It's running it. It's the face of the business,\" that kind of thing. Or maybe a number one song. It creates the whole infrastructure required to create the song, to be the influencer that represents that song, that kind of thing. And makes a lot of them. That could be the... Our culture respects money in that kind of way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And it's verifiable, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's verifiable, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The bank account can't lie.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are some basic things that we have seen before where it's at least expectable that there's a domain and there's a chance that this works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. So, we have fun things happening in real time. This is a good opportunity to talk about other reasoning models, o1, o3, just now OpenAI, as perhaps expected, released o3-mini. What are we expecting from the different flavors? Can you just lay out the different flavors of the o models and from Gemini, the reasoning model?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this training stack where you're doing this large scale RL- Ask it the same question from earlier, the one about the- The human nature. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What was the human nature one?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is much lighter. So there's essentially a gradation and just how much of this RL training you put into it determines how the output looks.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, we're now using Gemini 2.0 Flash Thinking Experimental 121.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It summarized the problem as humans self-domesticated apes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. All right. So, wait, is this reviewing the reasoning? Here's why this is a novel. Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You can click to expand. Oh, yeah, click to expand.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Analyze the request. Novel is the keyword.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>See how it just looks a little different? It looks like a normal output.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I mean in some sense, it's better structured. It makes more sense. And-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, and it latched onto human and then it went into organisms and... Oh, wow.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Apex Predator. Focus on domestication. Apply domestication to humans. Explore the idea of self-domestication.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not good, not good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Where is this going? Refine, articulate the insight. Greater facial expressiveness and communication ability, yes. Plasticity and adaptability, yes. Dependence on social groups, yes. All right. And self-critique, refine further. Wow. Is this truly novel? Is it well-supported? So on and so forth. And the insight it's getting at is humans are not just social animals but profoundly self-domesticated apes. And this self-domestication is the key to understanding our unique cognitive and social abilities. Self-domesticated apes. Self-domesticated-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I prefer the DeepSeek response.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean it's novel. The insight is novel. I mean that's like a good book title; Self-Domesticated Apes. There could be a case made for that. I mean, yeah, it's cool and it's revealing the reasoning. It's magical. It's magical. This is really powerful. Hello, everyone, this is Lex with a quick intermission recorded after the podcast since we've reviewed responses from DeepSeek R1 and Gemini Flash 2.0 Thinking during this conversation, I thought at this moment it would be nice to insert myself quickly doing the same for OpenAI o1-pro and o3-mini with the same prompt. The prompt being, give one truly novel insight about humans. And I thought I would, in general, give my vibe check and vibe based anecdotal report on my own experiences with the new o3-mini model now that I got a chance to spend many hours with it in different kinds of context and applications. So, I would probably categorize this question as let's say open- ended philosophical question. And in particular, the emphasis on novelty I think is a nice way to test one of the capabilities of the model, which is come up with something that makes you pause and almost surprise you with brilliance. So that said, my general review after running each of the models on this question a bunch of times is that o1-pro consistently gave brilliant answers, ones that gave me pause and made me think, both cutting in its insight and just really nicely phrased with wit, with clarity, with nuance over and over, consistently generating the best answers. After that is R1, which was less consistent, but again, delivered brilliance. Gemini Flash 2.0 Thinking was third and last was o3-mini actually. It often gave quite a generic answer, at least to my particular sensibilities. That said, in a bunch of other applications that I tested for brainstorming purposes, it actually worked extremely well and often outperformed R1. But on this open-ended philosophical question, it did consistently worse. Now another important element for each of these models is how the reasoning is presented. DeepSeek R1 shows the full chain of thought tokens, which I personally just love. For these open-ended philosophical questions, it's really, really interesting to see the model think through it, but really also just stepping back, me as a person who appreciates intelligence and reasoning and reflection, reading these kind of chain of thought raw tokens of R1, there's something genuinely beautiful about observing the path of deliberation in an intelligent system. I think we don't always have that explicitly laid out for us humans. So, to see it in another intelligence system, the nonlinearity of it akin to the Ulysses, Finnegans Wake by James Joyce. It's just beautiful to watch. Anyways, we discussed in the episode DeepSeek R1 talked about humans being able to convert selfish desires into cooperative systems by collectively pretending abstract rules like money laws and rights are real. And these shared hallucinations act as games where competition is secretly redirected to benefit the group turning conflict into society's fuel. Gemini 2.0 Flash Thinking said, \"Humans are not just social animals but self-domesticated apes. And this self domestication is the key to understanding our unique cognitive and social abilities.\" Now, it's important to say that the chain of thought there was really interesting. It was looking through the entire evolution of life on earth considering apex predators and considering how from that, we ended up to where we are. I think that domestication by choice is a really interesting angle. Again, it's one of those things when somebody presents a different angle on a seemingly obvious thing, it just makes me smile. And the same with DeepSeek R1, that these hallucinations of money laws and rights and us collectively pretending like it's real and we play games with them that look like competition when secretly we're just cooperating with each other and that is the fuel of progress. Beautifully put. Now, OpenAI o1-pro consistently, over and over delivered bangers. I can go through many of them, but the first one was, \"Humans are the only species that turns raw materials into symbolic resources. Then uses those symbols to reorganize the very materials that came from creating a closed feedback loop between meaning and matter.\" Here, I just ran it again. Banger after banger, I'm telling you. \"Humans are unique among known species in that they simultaneously rewrite two layers of reality; the external world and their own private mental landscapes. And then merge these two rewritten layers into a continuous personal narrative that feels objectively true.\" Feels true. This is poetry. Okay. And then o3-mini high, for me, was smart, fast actually, and kind of generic. Never quite got there for me. So here's the first one I got from o3-mini, \"Humans are not fixed beings, but rather ongoing narratives, dynamic stories that we continuously write, edit, and reinterpret. This narrative plasticity is more than just memory or self-reflection. It's an intrinsic cognitive process that acts like an internal error correction system. It allows us to adapt our identities and values over time in response to new experiences, challenges, and social contexts.\" Now, it almost sneaks up to something approximating cutting insight with narrative plasticity in quotes. But then it goes back to the generic. I don't know. All of these models are incredible for different reasons. There's a lot of concerns as we discussed in this episode, but there's a lot of reasons to be excited as well. And I've probably spoken for too long. I am severely sleep-deprived, borderline delirious. So hopefully some of this made sense. And now, dear friends, back to the episode.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they're doing or if it's something more complex, but they changed the training and they know that the inference mode is going to be different.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we're talking about o1-pro, $200 a month and they're losing money. The thing that we're referring to, this fascinating exploration of the test time compute space, is that actually possible? Do we have enough compute for that? Does the financials make sense?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>was ridiculous. Now as we scaled forward two years, we've had a 1200X reduction in cost to achieve the same level of intelligence as GPT-3.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So here on the x-axis is time over just a couple of years, and on the y-axis is log scale dollars to run inference on a million tokens.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's dollar to million.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you have just a linear decline on log scale from GPT-3 through 3.5 to Lama-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and so on and so forth, these search techniques, too expensive today, but they will get cheaper and that's what's going to unlock the intelligence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, it'll get cheaper and cheaper and cheaper. The big DeepSeek R1 release freaked everybody out because of the cheaper. One of the manifestations of that is NVIDIA stock plummeted. Can you explain what happened? And also just explain this moment and if NVIDIA is going to keep winning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a demo and it wasn't that easy. We were trying to get just 16 or 32 H100s for demo and it was not very easy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So for people who don't know, Jevons paradox is when the efficiency goes up, somehow magically, counter intuitively, the total resource consumption goes up as well.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>2X every two years, this is 1200X in like three years. So it's like the scale of improvement is hard to wrap your head around.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I was confused because to me, NVIDIA stock on that should have gone up, but maybe it went down because there's suspicion of foul play on the side of China, something like this. But if you just look purely at the actual principles at play here, it's obvious. Yeah, the Jevons paradox-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>higher the derivative is, the sooner the market's going to be bigger and expanding and NVIDIA's the only one that does everything reliably right now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, because it's not like an NVIDIA competitor arose. It's another company that's using NVIDIA-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Who historically has been a large NVIDIA customer. And has press releases about them cheering about being China's biggest NVIDIA customer, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I mean-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>how many GPUs they have because hey, yes, they have H800s, yes, they have H20s, they also have some H100s, right? Which were smuggled in.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you speak to that, to the smuggling? What's the scale of smuggling that's feasible for a nation state to do for companies? Is it possible to-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>your request approved, right? Because there's just no capacity because they just don't have enough GPUs to serve the model, even though it's incredibly efficient.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It'd be fascinating to watch the smuggling. Because I mean there's drug smuggling, right? That's a market. There's weapons smuggling. And GPUs will surpass that at some point.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>IP closely, you distill from your own models. If you are a researcher and you're not building any products, you distill from the OpenAI models-<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is a good opportunity. Can you explain big picture distillation as a process? What is distillation? What's the process of distillation?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>distilling, but they used 405B as a reward model for 70B in their Llama 3.2 or 3.3 rule- Yes. This is all the same topic.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, is this ethical? Is this legal? Why is that Financial Times article headline say, \"OpenAI says that there's evidence that China's DeepSeek used its model to train competitor.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>aspect of it is like, why is it unethical for me to train on your model when you can train on the internet's text? Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's a bit of a hypocrisy because OpenAI and potentially most of the companies trained on the internet's text without permission.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it, if they're using it for a specific task, they're going to tailor it to that and it doesn't matter that it's saying it's ChatGPT.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, I guess one of the ways to do that is like a system prompt or something like that? If you're serving it to say that you're-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>also are benefited... ... model trained by the Allen Institute for AI. We also are benefited from OpenAI data because it's a great research tool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think there's any truth and value to the OpenAI's claim that there's evidence that China's DeepSeek used this model to train?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>don't know about that, but- It could break contracts. I don't think it's illegal in any legal... No one's going to jail for this, ever.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Fundamentally, I think it's ethical, or I hope it's ethical because the moment it becomes... We ban that kind of thing, it's going to make everybody much worse off. And I also, actually... This is difficult, but I think you should be allowed to train on the internet. I know a lot of authors and creators are very sensitive about it. That's a difficult question. But the moment you're not allowed to train on the internet-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I agree. I have a schizo take on how you can solve this. Because it already works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I have a reasonable take out of it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right, [inaudible 03:32:10].<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>buts. And now, the models have no potential copyright lawsuit from New York Times or anything like that. No, it's just completely legal. Now, so-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Genius.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a rough journey, but there will be some solutions like that that makes sense. But there's a long tail where it's just on the internet.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think one of the other aspects of that Financial Times article implied, and so that leads to a more general question. Do you think there's... How difficult is spying, espionage, and stealing of actual secret code and data from inside of companies? How much of that is being attempted?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is a single dude who's in their 20s and 30s. Not everyone, but insane amount of... Insane percentages. So, there's always all these... And obviously-<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, honeypotted is like a female spy approaches you and...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Francisco. But as a single dude, I will say in his late 20s, we are very easily corrupted. Not corrupted myself, but we are. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Everybody else. Not me.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm too oblivious that I am not single, so I'm safe from one espionage access.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You have to make sure to close all security vulnerabilities. So you, Dylan, collect a lot of information about each of the mega clusters for each of the major AI companies. Can you talk about the buildouts for each one that stand out?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Like, hey, OpenAI or, say, Meta's adding a gigawatt, but most of it is distributed through the US for inference and all these other things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So maybe, we should lay out what a cluster is. So, does this include AWS? Maybe, it's good to talk about the different kinds of clusters. What you mean by mega clusters? What's the GPU and what's a compute or... And what [inaudible 03:37:41]-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Not that far back, but yeah. So, what do we mean by the clusters? The buildouts?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>associated stuff around it. So once you count all of that, it's roughly 1,200 to 1,400 watts for everything. Networking, CPUs, memory, blah, blah, blah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we should also say, what's required, you said power. So, a lot of power is required. A lot of heat is generated, so the cooling is required. And because there's a lot of GPUs or CPUs or whatever, they have to be connected. So, there's a lot of networking, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Delivered to a single cluster that's connected to do training. To train these models, to do both the pre-training, the post-training, all of this stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is insane.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Virginia, it costs more to transmit power than it costs to generate it, which is there's all sorts of second-order effects that are insane here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can the power grid support this kind of growth?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hopefully reduced the regulations to where, yes, things can be built. But yeah, this is a big, big challenge. Is building enough power fast enough?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are you going to basically have a nuclear power plant next to a data center for each one of these?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>labs have, which is like, \"Yeah, it's great. We'll just use gas,\" because the race is that important. And if we lose, that's way worse.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I should say that I got a chance to visit the Memphis data center.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, wow.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And it's incredible. I mean, I visited with Elon. Just the teams and the rate of innovation there is insane. My sense is that nobody's ever done anything of this scale, and nobody has certainly ever done anything of this scale at the rate that xAI is doing. So, they're figuring out... I was sitting in on all of these meetings where they're brainstorming. It's insane. It's exciting because they're trying to figure out what the bottlenecks are, how to remove the bottlenecks, how to make sure that... There's just so many really cool things about putting together a data center because everything has to work. The people that do the sys admin, the machine learning and all of that is the exciting thing, so on. But really, the people that run everything are the folks that know the low-level software and hardware that runs everything, the networking, all of that. So, you have to make sure you have procedures that test everything. I think they're using ethernet. I don't know how they're doing the networking, but-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They're using NVIDIA Spectrum-X Ethernet. I think the unsung heroes are the cooling in electrical systems which are just glossed over.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hug the guy because it says PyTorch... It's like PyTorch.powerplant no blow up equals 0 or equal 1. And what it does is amazing, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fake numbers so the power doesn't spike too much, and so then the power plants don't blow up because the transient spikes screw stuff up.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that makes sense. You have to do that kind thing. [inaudible 03:51:57] You have to make sure they're not idle.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And Elon's solution was like, \"Let me throw a bunch of Tesla Megapacks and a few other things.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, to symbolize that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is just like, set this operator. And what this operator does is it just makes the GPUs compute nothing so that the power doesn't spike.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that just tells you how much power you're working with. I mean, it's insane. It's insane.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>watt to a kilowatt to a megawatt. You look and stare at that, and you're how high on the list a gigawatt is, it's mind-blowing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you say something about the cooling? I know Elon's using liquid cooling, I believe, in all cases. That's a new thing. Most of them don't use liquid cooling. Is there something interesting to say about the cooling?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you want to go water cooling is because you can just put the chips right next to each other, and therefore get higher speed connectivity.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I got to ask you, in one of your recent posts, there's a section called cluster measuring contest. So...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's another word there, but I won't say it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Who's got the biggest now and who's going to have the biggest?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Today, individual largest is Elon. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. Elon's cluster.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>than existing ones. Hopper's 700 watts, Blackwell goes to 1,200 watts. So, the power per chip is growing and the number of chips is growing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Nuts. Elon said he'll get to a million. Do you think that's actually feasible?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and stuff is open record, but it's not quite clear what the time scales are. I just never doubt Elon. He's going to surprise us.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, what's the idea with these clusters? If you have a million GPUs, what percentage in a, let's say 2 or 3 years, is used for training? What percent pre-training, and what percent is used for the actual computation?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>FLOPS is going to be less of a useful term. And then as the infrastructure gets better, it's probably going to go back to FLOPS.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, all of the things we've been talking about is most likely going to be NVIDIA, right? Is there any competitors of GPU?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Google kind of ignored them. I was getting- I was like, \"Ah?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the story with TPU? What's the...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>from each other. And they're not GPUs, TPUs. In Iowa and Nebraska, they have four data centers that are just right next to each other.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why doesn't Google flex its cluster size?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an image of what a standard Google data center looks like. By the way, their data centers look very different than anyone else's data centers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are we looking at here?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Google has the biggest cluster but you have to spread over three sites, and by a significant margin. We have to go across multiple sites.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why doesn't Google compete with NVIDIA? Why don't they sell TPUs?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They all loved it. But as soon as you step outside- A lot of them go back. They leave Google and then they go back.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that they should just buy twice as many and figure out how to sell them, and make $30 billion? Who cares about making $30 billion?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Won't that 30 billion exceed actually the search profit eventually?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You're always going to make more money on services than...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Always.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>clear, today, people are spending a lot more on hardware than they are with the services because the hardware front runs the service spend. But-<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're investing, yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>team, which is a separate organization from the DeepMind team, which is a separate organization from the Search team. There's a lot of bureaucracy here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Wait. Google Cloud is a separate team than the TPU team?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and XLA and stuff, they more so serve DeepMind and Search, right? And so their customer is different. They're not building a product for them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you understand why AWS keeps winning versus Azure for cloud versus Google Cloud?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, there's-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Google Cloud is tiny, isn't it, relative to AWS?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Google Cloud is third. Yeah. Microsoft is the second biggest, but Amazon is the biggest, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>way bigger. Why? Because using AWS is better and easier. And in many cases, it's cheaper- It was first. And it's first. It was first.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. But there's a lot of things that are first that lose the-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, it's harder to switch than it is to-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>AWS is-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because there's large-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's big fees for switching too. AWS generates over 80% of Amazon's profit. I think over 90%.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's insane.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>make money from this, but they haven't yet, right? They make tiny little profit from it. Yeah, one day Amazon Prime will triple in price.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You would think they would improve AWS interface because it's horrible. It's clunky, but everybody is.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, one would think. I think actually Google's interface is sometimes nice, but it's also they don't care about anyone besides their top customers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And their customer service sucks and they have a lot less-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, all these companies, they optimize for the big customers. Yeah, it's supposed to be for business.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>applications of high-performance computing can very rapidly be evolved on CUDA and NVIDIA chips. And that is entirely different than Google as a services business.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean NVIDIA, it should be said, is a truly special company. I mean there's the culture of everything. They're really optimized for that kind of thing. Speaking of which, is there somebody that can even challenge NVIDIA hardware-wise? Intel? AMD?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>not possible. Intel is obviously in dire straits right now and needs to be saved somehow. Very important for national security, for American technology comments.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain the obviously, so why are they in dire straits?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>was the only person who understood the company well, right? We'll see. He was not the best, but he was pretty good relatively technical guy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Where does Intel make most of its money? The CPUs though.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Apple's doing well. They could get squeezed a little bit in PC, although PC generally I imagine will just stick Intel mostly for Windows side.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's talk about the broad AI race. Who do you think wins? We talked about Google, Meta.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The default leader has been Google because of their infrastructure advantage.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, in the news, OpenAI is the leader.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>best model. They have the best model that people can use and they're experts-Experts. And they have the most AI revenue. Yeah. OpenAI is winning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So who's making money on AI right now? Is anyone making money?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>X now, but know with GPT-4 Turbo and 4.0 And there'll be another one probably cheaper than GPT-4.0 even that comes out at some point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that research costs a lot of money.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep, exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's the thing that I guess is not talked about with the cost, that when you're referring to the cost of the model, it's not just the training or the test runs, it's the actual research, the manpower.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>keep raising money. I think others will keep raising money because the returns from it are going to be eventually huge once we have AGI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So do you think multiple companies will get, let's assume-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't think it's winner take all.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, so let's not call it AGI whatever. It's like a single day. It's a gradual thing-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Powerful AI. Super powerful AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But it's a gradually increasing set of features that are useful and make-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Rapidly increasing set of features.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Rapidly increasing set of features. So you're saying a lot of companies will be... It just seems absurd that all of these companies are building gigantic data centers.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of their services. People are there. People spend time on that as platforms, and it's a way to make more money per user per hour.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It seems like Google X/X AI/ Tesla important to say. And then Meta will benefit not directly from the AI like the LLMs, but from the intelligence, like the additional boost of intelligence to the products they already sell. So whether that's the recommendation system or for Elon who's been talking about Optimus, the robot, potentially the intelligence of the robot, and then you have personalized robots in the home, that kind of thing. He thinks it's a 10 plus trillion dollars business, which...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>billion humans and let's get 8 billion robots and let's pay them the average salary. And there we go. 10 trillion. More than 10 trillions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I mean if there's robots everywhere, why does it have to be just 8 billion robots?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah, of course. Of course. I'm going to have one robot. You're going to have like 20.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I mean I see a use case for that. So yeah, so I guess the benefit would be in the products they sell, which is why OpenAI's in a trickier position because they-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>these reasoning, code, agents, computer use, all this stuff is where OpenAI has to actually go to make money in the future otherwise they're kaputs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But X, Google, and Meta have these other products. So isn't it likely that OpenAI and Anthropic disappear eventually?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Unless they're so good at models, which they are.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But it's such a cutting edge. I mean-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It depends on where you think AI capabilities are going.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You have to keep winning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You have to keep winning as you climb, even if the AI capabilities are going super rapidly awesome into the direction of AGI, there's still a boost for X in terms of data, Google in terms of data, Meta in terms of data, in terms of other products and the money and there's just huge amounts of money.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The whole idea is human data is kind of tapped out. We don't care. We all care about self-play, verifiable task. Think about AWS.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes, self-play, which is an RNG problem.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>tasks and agents and robotics and computer use, and those are the areas where all the value will be delivered, not API, not chat application.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So is it possible you have it all just becomes a commodity and you have the very thin wrapper like Perplexity, just joking.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There are a lot of wrappers making a lot of money.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But do you think it's possible that people would just even forget what OpenAI and Anthropic is just there'll be wrappers around the API and it just dynamically-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>level capabilities is dead. It is a common saying that the best businesses being made now are ones that are predicated on models getting better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. Which would be like wrappers, thing that is riding the wave of the models.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>with model costs coming down, you can just get super high revenue. That revenue is totally untapped and it's not clear technically how it's done.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that is, I mean sort of the AdSense innovation that Google did, the one day you'll have in GPT output an ad and that's going to make billions, if not-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It could be some way of making it so the voice introduces certain things. It's much harder to measure and it takes imagination, but yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And it wouldn't come off shady so that you would receive public blowback, that kind of thing. So you have to do it loud enough to where it's clear it's an ad and balance all of that. So that's the open question they're trying to solve. Anthropic and OpenAI, they need to-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think they care about that at all. They don't care about it right now. I think it's places like Perplexity are experimenting on that more.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, interesting. Yeah, for sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Perplexity, Google, Meta care about this. I think OpenAI and Anthropic are purely laser focused on-<|eot_id|><|start_header_id|>assistant<|end_header_id|>AGI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they're rational actors, which they are mostly what you do in a two-year AGI versus five year versus 10 years, very, very, very different. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think agents are promising? We have to talk about this. This is the excitement of the year that agents are going to rev.. This is the generic hype term that a lot of business folks are using. AI agents are going to revolutionize everything.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to go to Austin in two days. I have XYZ constraints,\" and actually trusting it. I think there's an HCI problem coming back for information.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, what's your prediction there? Because my gut says we're very far away from that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then they have a bunch of teleoperators to make sure it doesn't get stuck. But you can't do that because it doesn't have enough nines.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Self-driving has quite a lot of structure because roads have rules, it's well-defined, there's regulation. When you're talking about computer use for the open web, for example, or the open operating system, it's a mess. So the possibility... I'm always skeptical of any system that is tasked with interacting with the human world, with the open messaging world.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fact that it can't finish loading my dishwasher when I'm unhappy with it. But that's just going to be part of the Tesla service package.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm just imagining an AI agent talking to another AI agent. One company has an AI agent that specializes in helping other AI agents.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we've seen, we see it now with training data for language models where companies are like, \"No, you have to pay.\" Business working it out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That said, I think airlines and hotels have high incentive to make their site work really well, and they usually don't. If you look at how many clicks it takes to order airplane ticket, it's insane.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You actually can't call an American Airlines agent anymore. They don't have a phone number.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, it's horrible on the interface front. And to imagine that agents will be able to deal with that website when I, as a human, struggle, like I have an existential crisis every time I try to book an airplane ticket. I think it's going to be extremely difficult to build an AI agent that's robust in that way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>where the point is where you just start training on enough domains and poof, more domains just start working. And you've crossed the generalization barrier.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, what do you think about the programming context? So software engineering, that's where I personally, and I know a lot of people interact with AI the most.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>years, it'll be well down. But in the few years, there'll never going to be the snap moment where it's like software engineers aren't useful.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think also the nature of what it means to be a programmer and what kind of jobs programmers do changes, because I think there needs to be a human in the loop of everything you've talked about. There's a really important human in that picture of correcting the code, fixing-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thinking larger than the context length.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And debugging also, like debugging by reading the code, understanding the steering the system. No, no, no. You missed the point. Adding more to the prompt like, yes, adding the human-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>design buttons that are so perfect, and it's like how is AI going to do that? They could give you all the ideas. Perfect, fine.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, that's the thing. You can call it taste. One thing humans can do is figure out what other humans enjoy better than AI systems. That's where the preference you loading that in. But ultimately, humans are the greatest preference generator. That's where the preference comes from.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>software engineering could look like. The PR review, here's a few options, here are some potential pros and cons, and they're going to be judges.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think the thing I would very much recommend is programmers start using AI and embracing that role of the supervisor of the AI system and partner the AI system versus writing from scratch or not learning coding at all and just generating stuff because I think there actually has to be a pretty high level of expertise as a programmer to be able to manage increasingly intelligent systems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's that and then becoming a domain expert in something.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure. Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>forefront. So it's like you have to be at the forefront of something and then leverage the rising tide that is AI for everything else.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, yeah. There's so many low hanging fruit everywhere in terms of where software can help automate a thing or digitize a thing in the legal system. That's why DOGE is exciting. I got to hang out with a bunch of the DOGE folks, and I mean, government is so old school. It's like begging for the modernization of software, of organizing the data, all this kind of stuff. I mean, in that case it's by design because bureaucracy protects centers of power and so on. But software breaks down those barriers, so it hurts those that are holding onto power, but ultimately benefits humanity. So there's a bunch of domains of that kind. One thing we didn't fully finish talking about is open source. So first of all, congrats. You released a new model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, this is-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Tulu.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a paper, and it's putting all these together to try to create a recipe that people can fine tune models like GPT-4 to their domain.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To be clear, in the case of Tulu, maybe you can talk about Llama too, but in the case of Tulu, you're taking Llama 3, 405B.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Tulu has been a series of recipes for post-training. So we've done multiple models over years.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And so you're open sourcing everything.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>eval numbers, I mean we have extra months in this, but our eval numbers are much better than the Llama instruct model that they released.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then you also said better than DeepSeek V3?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and it makes it a lot easier. And that's the type of things that is going on among academic and closed frontier research in AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Since you're pushing open source, what do you think is the future of it? Do you think DeepSeek actually changes things since it's open source or open weight or is pushing the open source movement into the open direction?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of things are those, though? Are they like- At this point, I can't pull them off the top of my head, but it'd be like-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Stuff like competitors?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to find in there, but that's what we need to do as an ecosystem if people want open source AI to be financially useful.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We didn't really talk about Stargate. I would love to get your opinion on what the new administration, the Trump administration, everything that's being done from the America side and supporting AI infrastructure and the efforts of the different AI companies. What do you think about Stargate? What are we supposed to think about Stargate and does Sam have the money?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>good fit. Anyways, Trump has made it much easier to do this, right? Generally, Texas has the only unregulated grid in the nation as well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's go Texas.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>invest in OpenAI if they want to. As far as money secured, the first 100,000 GB 200 cluster can be funded. Everything else after that-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Up in the air.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... is up in the air. Money's coming. I believe the money will come. I personally do.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a belief.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to release better models and be able to raise more money. But the actual reality is that Elon's right, the money does not exist.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What does the US government have to do with anything? What does Trump have to do with everything? He's just a hype man?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to enable the regulations to actually allow it to be built. I don't believe there's any US government dollars being spent on this though.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So I think he's also just creating a general vibe that regulation will go down and this is the era of building. So if you're a builder, you want to create stuff, you want to launch stuff, this is the time to do it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that open AI or Trump is sort of championing, people are going to build more and his actions are going to let people build more.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are you excited about these several years that are upcoming in terms of cluster build outs, in terms of breakthroughs in AI, the best possible future you can imagine in the next couple of years, two, three, four years? What does that look like? It could be very specific technical things like breakthroughs on post-training or it could be just size, big impressive clusters.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>with optics and electronics kind of getting closer and closer, whether it be co-packaged optics or some sort of forms of new forms of switching.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is internal to a cluster?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot of interesting stuff happening on that end. Telecom has been really boring since 5G, and now it's really exciting again on the hardware side.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you educate me a little bit about the speed of things? So the speed of memory versus the speed of interconnect versus the speed of fiber between data centers. Are these orders of magnitude different? Can we at some point converge towards a place where it all just feels like one computer?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>all of these layers of the stack, all the way up to the models, human progress is at a pace that's never been seen before.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm just imagining you sitting back in a layer somewhere with screens everywhere, just monitoring the supply chain where all these clusters, all the information you're gathering, you're incredible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's a big team, there's a big team.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You do quite incredible work with semi analysis. I mean just keeping your finger on the pulse of human civilization in the digital world. It's pretty cool just to watch, feel that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, thank you. I guess.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Feel all of us doing shit. Epic shit.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The AGI, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I feel from meme to reality. Nathan, is there breakthroughs that you're looking forward to potentially?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to be the most powerful technology of my lifetime, it's like we need to have a lot of people involved in making that and-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Making it open helps with that. As accessible as possible, as open as possible, yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think then it'll reassess of what is the biggest problem facing AI and tack on a different angle to the wild ride that we're on.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And for me, just from even the user experience, anytime you have like Aparthi said, the aha moments, the magic, seeing the reasoning, the chain of thought, it's like there's something really just fundamentally beautiful about that. It's putting a mirror to ourselves and seeing like, oh, shit. It is solving intelligence as the cliche goal of these companies is, and you get to understand why we humans are special. The intelligence within us is special. And for now also why we're special in terms of we seem to be conscious and the AI systems for now, and we get to explore that mystery, so it's just really cool to get to explore these questions that I don't think I would've never imagined would be even possible back when just watching with excitement, deep blue beat Kasparov, I wouldn't have ever thought this kind of AI would be possible in my lifetime. This is really feels like AI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's incredible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fly up. It would hit the ceiling and stop and catch it. It's like, okay, that is really stupid compared to what's going on now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And now you could probably with natural language tell it to learn to fly and it's going to generate the control algorithm required to do that probably.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's low level blockers. We have to do some weird stuff for that, but you can, you definitely can.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Back to our robotics conversation, yeah, when you have to interact in the actual physical world, that's hard. What gives you hope about the future of human civilization looking into the next 10 years, 100 years, 1000 years, how long do you think we'll make it? You think we've got 1000 years?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>true. I don't think necessarily we're good at long-term credit assignment of risk, but when the risk becomes immediate, we tend to figure things out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>been times where things go backwards, there's times when things don't happen at all, and we're on what should be very positive trajectory right now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, there seems to be progress, but just like with power, there's like spikes of human suffering and we want to try to minimize the amount of spikes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it'll be used for profit-seeking motives, which will increase the abundance and supply of things and therefore reduce suffering, right? That's the goal.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Scrolling on a timeline, just drowning in dopamine-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of the world. That is a positive outcome, right? If I have food tubes and lung down scrolling and I'm happy, that's a positive outcome.<|eot_id|><|start_header_id|>assistant<|end_header_id|>While expanding out into the cosmos. Well, this is a fun time to be alive. And thank you for pushing the forefront of what is possible in humans, and thank you for talking today. This was fun.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thanks for having us. Thanks for having us.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Dylan Patel and Nathan Lambert. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Richard Feynman. \"For a successful technology, reality must take precedence over public relations, for nature cannot be fooled.\" Thank you for listening and I hope to see you next time.<|eot_id|>"}
