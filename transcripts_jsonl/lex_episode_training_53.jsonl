{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>"}
{"text": "<|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with Guillaume Verdon, the man behind the previously anonymous account @BasedBeffJezos on X. These two identities were merged by a doxxing article in Forbes titled, Who Is @BasedBeffJezos, The Leader Of The Tech Elite's E/Acc Movement? So let me describe these two identities that coexist in the mind of one human. Identity number one, Guillaume, is a physicist, applied mathematician, and quantum machine learning researcher and engineer receiving his PhD in quantum machine learning, working at Google on quantum computing, and finally launching his own company called Extropic that seeks to build physics-based computing hardware for generative AI. Identity number two, Beff Jezos on X is the creator of the effective accelerationism movement, often abbreviated as e/acc, that advocates for propelling rapid technological progress as the ethically optimal course of action for humanity. For example, its proponents believe that progress in AI is a great social equalizer, which should be pushed forward. e/acc followers see themselves as a counterweight to the cautious view that AI is highly unpredictable, potentially dangerous, and needs to be regulated. They often give their opponents the labels of quote, \"doomers or decels\" short for deceleration, as Beff himself put it, \"e/acc is a mimetic optimism virus.\" The style of communication of this movement leans always toward the memes and the lols, but there is an intellectual foundation that we explore in this conversation. Now, speaking of the meme, I am to a kind of aspiring connoisseur of the absurd. It is not an accident that I spoke to Jeff Bezos and Beff Jezos back to back. As we talk about Beff admires Jeff as one of the most important humans alive, and I admire the beautiful absurdity and the humor of it all. This is the Lex Fridman Podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Guillaume Verdon. Let's get the facts of identity down first. Your name is Guillaume Verdon, Gill, but you're also behind the anonymous account on X called @BasedBeffJezos. So first, Guillaume Verdon, you're a quantum computing guy, physicist, applied mathematician, and then @BasedBeffJezos is basically a meme account that started a movement with a philosophy behind it. So maybe just can you linger on who these people are in terms of characters, in terms of communication styles, in terms of philosophies?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an anonymous account, it seemed like I had unclamped some variables in my brain and suddenly could explore a much wider parameter space of thoughts.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just a little on that, isn't that interesting that one of the things that people don't often talk about is that when there's pressure and constraints on speech, it somehow leads to constraints on thought even though it doesn't have to. We can think thoughts inside our head, but somehow it creates these walls around thought.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are exponentially more likely. So the universe is biased towards certain futures, and so there's a natural direction where the whole system wants to go.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the second law of thermodynamics says that the entropy is always increasing in the universe that's tending towards an equilibrium. And you're saying there's these pockets that have complexity and are out of equilibrium. You said that thermodynamics favors the creation of complex life that increases its capability to use energy to offload entropy. To offload entropy. So you have pockets of non-entropy that tend the opposite direction. Why is that intuitive to you that it's natural for such pockets to emerge?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the pie. And so I've balanced both identities so far, but I guess recently the two have been merged more or less without my consent.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You said a lot of really interesting things there. So first, representations of nature, that's something that first drew you in to try to understand from a quantum computing perspective, how do you understand nature? How do you represent nature in order to understand it, in order to simulate it, in order to do something with it? So it's a question of representations, and then there's that leap you take from the quantum mechanical representation to the what you're calling meso scale representation, where the thermodynamics comes into play, which is a way to represent nature in order to understand what? Life, human behavior, all this kind of stuff that's happening here on earth that seems interesting to us. Then there's the word hyperstition. So some ideas as suppose both pessimism and optimism of such ideas that if you internalize them, you in part make that idea reality. So both optimism, pessimism have that property. I would say that probably a lot of ideas have that property, which is one of the interesting things about humans. And you talked about one interesting difference also between the sort of the Guillaume, the Gill front end and the @BasedBeffJezos backend is the communication styles also that you are exploring different ways of communicating that can be more viral in the way that we communicate in the 21st century. Also, the movement that you mentioned that you started, it's not just a meme account, but there's also a name to it called effective accelerationism, e/acc, a play, a resistance to the effective altruism movement. Also, an interesting one that I'd love to talk to you about, the tensions there. And so then there was a merger, a get merge on the personalities recently without your consent, like you said. Some journalists figured out that you're one and the same. Maybe you could talk about that experience. First of all, what's the story of the merger of the two?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So I wrote the manifesto with my co-founder of e/acc, an account named @bayeslord, still anonymous, luckily and hopefully forever.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it was @BasedBeffJezos and bayes like bayesian, like @bayeslord, like bayesian lord, @bayeslord. Okay. And so we should say from now on, when you say e/acc, you mean E slash A-C-C, which stands for effective accelerationism.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And you're referring to a manifesto written on, I guess Substack.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are you also @bayeslord?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. It's a different person?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. All right. Well, there you go. Wouldn't it be funny if I'm @bayeslord?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>notes and now they had way too much. And that's when I got worried, because they said it was of public interest and in general-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I like how you said, sensor fused, like it's some giant neural network operating in a distributed way. We should also say that the journalists used, I guess at the end of the day, audio-based analysis of voice, comparing voice of what, talks you've given in the past and then voice on X spaces?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. And that's where primarily the match happened. Okay, continue.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>had just reached 50K followers. Allegedly, that's of public interest. And so where do we draw the line? When is it legal to dox someone?<|eot_id|><|start_header_id|>assistant<|end_header_id|>The word dox, maybe you can educate me. I thought doxxing generally refers to if somebody's physical location is found out, meaning where they live. So we're referring to the more general concept of revealing private information that you don't want revealed is what you mean by doxxing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>company, some of the leadership, and essentially, yeah, I told the world that I was Beff Jezos because they had me cornered at that point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So to you, it's fundamentally unethical. So one is unethical for them to do what they did, but also do you think not just your case, but in a general case, is it good for society? Is it bad for society to remove the cloak of anonymity or is it case by case?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to apply pressure on them to suppress their speech, I think that's a speech suppression mechanism, an idea suppression complex as Eric Weinstein would say.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the flip side of that, which is interesting, I'd love to ask you about it, is as we get better and better at large language models, you can imagine a world where there's anonymous accounts with very convincing large language models behind them, sophisticated bots essentially. And so if you protect that, it's possible then to have armies of bots. You could start a revolution from your basement, an army of bots and anonymous accounts. Is that something that is concerning to you?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>tech, moved back in with my parents, sold my car, let go of my apartment, bought about 100K of GPUs, and I just started building.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So I wasn't referring to the basement, because that's sort of the American or Canadian heroic story of one man in their basement with 100 GPUs. I was more referring to the unrestricted scaling of a Guillaume in the basement.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>how to steer systems in our civilization, and we should hear them out. And so why should we restrict free speech to biological intelligences only?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, but it feels like in the goal of maintaining variance and diversity of thought, it is a threat to that variance. If you can have swarms of non-biological beings, because they can be like the sheep in Animal Farm, you still within those swarms want to have variance.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>identify if a bot is a bot. And I think Elon is trying to converge on that on X, and hopefully other platforms follow suit.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it'd be interesting to also be able to sign where the bot came from like, who created the bot? What are the parameters, the full history of the creation of the bot, what was the original model? What was the fine tuning? All of it, the kind of unmodifiable history of the bot's creation. Because then you can know if there's a swarm of millions of bots that were created by a particular government, for example.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>one of our top priorities, so that we can maintain progress in AI and we can maintain a nice, stable, adversarial equilibrium of powers, right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think the, at least to me, attention between ideas here, so to me, deceleration can be both used to centralize power and to decentralize it and the same with acceleration. So sometimes using them a little bit synonymously or not synonymously, but that there's, one is going to lead to the other. And I just would like to ask you about, is there a place of creating a fault tolerant, diverse development of AI that also considers the dangers of AI? And AI, we can generalize to technology in general, is, should we just grow, build, unrestricted as quickly as possible, because that's what the universe really wants us to do? Or is there a place to where we can consider dangers and actually deliberate sort of a wise strategic optimism versus reckless optimism?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of reliability optimum than sort of heavy-handed regulations that are written by the incumbents and in a subversive fashion, serves them to achieve regulatory capture.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So to you, safe AI development will be achieved through market forces versus through, like you said, heavy-handed government regulation. There's a report from last month, I have a million questions here, from Yoshua Bengio, Geoff Hinton and many others, it's titled, \"Managing AI Risk in an Era of Rapid Progress.\" So there is a collection of folks who are very worried about too rapid development of AI without considering AI risk and they have a bunch of practical recommendations. Maybe I can give you four and you see if you like any of them.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, \"Give independent auditors access to AI labs,\" one. Two, \"Governments and companies allocate one third of their AI research and development funding to AI safety,\" sort of this general concept of AI safety. Three, \"AI companies are required to adopt safety measures if dangerous capabilities are found in their models.\" And then four, something you kind of mentioned, \"Making tech companies liable for foreseeable and preventable harms from their AI systems.\" So independent auditors, governments and companies are forced to spend a significant fraction of their funding on safety, you got to have safety measures if shit goes really wrong and liability-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Companies are liable. Any of that seem like something you would agree with?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>utility much faster than centralized control. And if we let go of that, we let go of our main advantage over our near peer competitors.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So if AGI turns out to be a really powerful technology or even the technologies that lead up to AGI, what's your view on the sort of natural centralization that happens when large companies dominate the market? Basically formation of monopolies like the takeoff, whichever company really takes a big leap in development and doesn't reveal intuitively, implicitly or explicitly, the secrets of the magic sauce, they can just run away with it. Is that a worry?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the more points we have in the search algorithm in this point cloud, the more we'll be able to explore new modes of thinking, right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, but it feels like a delicate balance, because we don't understand exactly what it takes to build AGI and what it will look like when we build it. And so far, like you said, it seems like a lot of different parties are able to make progress, so when OpenAI has a big leap, other companies are able to step up, big and small companies in different ways. But if you look at something like nuclear weapons, you've spoken about the Manhattan Project, there could be really like a technological and engineering barriers that prevent the guy or gal in her mom's basement to make progress. And it seems like the transition to that kind of world where only one player can develop AGI is possible, so it's not entirely impossible, even though the current state of things seems to be optimistic.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's what we're trying to avoid. To me, I think another point of failure is the centralization of the supply chains for the hardware.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we'll be able to explore the wider space of intelligences that can really be a massive benefit to our wellbeing and the advancement of civilization.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And still we're able to see the beauty and meaning in the human experience even though we're no longer in our best understanding of the world at the center of it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>matter we're in, we got to feel a responsibility to scale it in order to preserve it, because the options are to grow or die.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So if it turns out that the beauty that is consciousness in the universe is bigger than just humans, the AI can carry that same flame forward. Does it scare you, are you concerned that AI will replace humans?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>back in the forest, let go of technology and return to our primitive state. And those are the two paths forward, at least to me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But there's a philosophical question whether there's a limit to the human capacity to align. So let me bring it up as a form of argument, this guy named Dan Hendrycks and he wrote that he agrees with you that AI development could be viewed as an evolutionary process, but to him, to Dan, this is not a good thing, as he argues that natural selection favors AIs over humans and this could lead to human extinction. What do you think, if it is an evolutionary process and AI systems may have no need for humans?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>humans and that induces a selective pressure, if you consider a neural net being alive when there's an API running instances of it on GPUs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think that humans plus AI is a very powerful combination and it's not clear to me that pure AI would select out that combination.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the humans are creating the selection pressure right now to create AIs that are aligned to humans, but given how AI develops and how quickly it can grow and scale, to me, one of the concerns is unintended consequences, like humans are not able to anticipate all the consequences of this process. The scale of damage that could be done through unintended consequences with AI systems is very large.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The scale of the upside.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Guess it's-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we have a hard cutoff of like we don't birth these technologies at all and then we leave all the potential upside on the table.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>with higher carrying capacity by scaling up civilization. Out of responsibility to those humans, I think we have to make the greater grander future happen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there a middle ground between cutoff and all systems go? Is there some argument for caution?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>will exhibit caution. Every organism, company, consumer is acting out of self-interest and they won't assign capital to things that have negative utility to them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The problem is with the market is, there's not always perfect information, there's manipulation, there's bad faith actors that mess with the system. It's not always a rational and honest system.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>speech and freedom of thought in order to be able to converge on the subspace of technologies that have positive utility for us all, right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well let me ask you about p(doom), probability of doom. That's just fun to say, but not fun to experience. What is to you the probability that AI eventually kills all or most humans, also known as probability of doom?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>accurate estimate of the likelihood of certain futures. If they were that good, I think they would be very rich trading on the stock market.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But nevertheless, it's true that humans are biased, grounded in our evolutionary biology, scared of everything that can kill us, but we can still imagine different trajectories that can kill us. We don't know all the other ones that don't necessarily, but it's still I think, useful combined with some basic intuition grounded in human history, to reason about like what... Like looking at geopolitics, looking at basics of human nature, how can powerful technology hurt a lot of people? It just seems grounded in that, looking at nuclear weapons, you can start to estimate p(doom) maybe in a more philosophical sense, not a mathematical one. Philosophical meaning like is there a chance? Does human nature tend towards that or not?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>out of fear. And so I think that we shouldn't give into fear, fear is the mind killer, I think it's also the civilization killer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We can still think about the various ways things go wrong, for example, the founding fathers of the United States thought about human nature and that's why there's a discussion about the freedoms that are necessary. They really deeply deliberated about that and I think the same could possibly be done for AGI. It is true that human history shows that we tend towards centralization, or at least when we achieve centralization, a lot of bad stuff happens. When there's a dictator, a lot of dark, bad things happen. The question is, can AGI become that dictator? Can AGI when develop, become the centralizer, because of its power? Maybe because of the alignment of humans, perhaps, the same tendencies, the same Stalin like tendencies to centralize and manage centrally the allocation of resources? And you can even see that as a compelling argument on the surface level. \"Well, AGI is so much smarter, so much more efficient, so much better at allocating resources, why don't we outsource it to the AGI?\" And then eventually whatever forces that corrupt the human mind with power could do the same for AGI. It'll just say, \"Well, humans are dispensable, we'll get rid of them.\" Do the Jonathan Swift, Modest Proposal from a few centuries ago, I think the 1700s, when he satirically suggested that, I think it's in Ireland, that the children of poor people are fed as food to the rich people and that would be a good idea, because it decreases the amount of poor people and gives extra income to the poor people. So on several accounts decreases the amount of poor people, therefore more people become rich. Of course, it misses a fundamental piece here that's hard to put into a mathematical equation of the basic value of human life. So all of that to say, are you concerned about AGI being the very centralizer of power that you just talked about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>go closer to the perception. And we're going to break up AI into smaller subsystems that communicate with one another and form a meta system.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you look at the hierarchies that are in the world today, there's nations and those all hierarchical. But in relation to each other, nations are anarchic, so it's an anarchy.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you foresee a world like this, where there's not a over... What'd you call it? A centralized cybernetic control?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Centralized locus of control. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's suboptimal, you're saying?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, it would be always a state of competition at the very top level?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>old things that didn't work, it's what gives us adaptability and helps us converge on the technologies and things to do that are most good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I just hope there's not a failure mode that's unique to AGI versus humans, because you're describing human systems mostly right now.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I just hope when there's a monopoly on AGI in one company, that we'll see the same thing we see with humans, which is, another company will spring up and start competing effectively.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the research, avoids a neural tyranny where very few people have control over AI for the world and use it to oppress those around them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you were talking about intelligence, you mentioned multipartite quantum entanglement.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>High-level question first is, what do you think is intelligence? When you think about quantum mechanical systems and you observe some kind of computation happening in them, what do you think is intelligent about the kind of computation the universe is able to do; a small, small inkling of which is the kind of computation a human brain is able to do?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to develop. I spent several years doing that, but over time, I grew weary of the timelines that were starting to look like nuclear fusion.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One high-level question I can ask is maybe by way of definition, by way of explanation, what is a quantum computer and what is quantum machine learning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that's the core of my quest really, even to this day, after quantum computing. It's still in this journey to merge really physics and AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Quantum machine learning is a way to do machine learning on a representation of nature that stays true to the quantum mechanical aspect of nature?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>thinking about shipping something across a city using a rocket and going to orbit and back. It doesn't make sense. Just use a delivery truck.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What kind of stuff can you figure out, can you predict, can you understand with quantum deep learning that you can't with deep learning? So, incorporating quantum mechanical systems into the learning process?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>AlphaFold. AlphaFold understood the probability distribution over configurations of proteins. You can understand quantum distributions over configurations of electrons more efficiently with quantum machine learning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You co-authored a paper titled A Universal Training Algorithm for Quantum Deep Learning. That involves Baqprop, with a Q. Very well done, sir. Very well done. How does it work? Is there some interesting aspects you can just mention on how Baqprop and some of these things we know for classical machine learning transfer over to the quantum machine learning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>parameters, which is pretty funky. Now, you don't have just a point for parameters, you have a superposition over many potential parameters. Our goal is-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is using phase kick somehow to adjust the parameters?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>thing, and in practice with at least the current architectures for quantum computers that we have planned, such algorithms would be extremely expensive to run.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe this is a good place to ask the difference between the different fields that you've had a toe in. So, mathematics, physics, engineering, and also entrepreneurship, the different layers of the stack. I think a lot of the stuff you're talking about here is a little bit on the math side, maybe physics almost working in theory.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the difference between math, physics, engineering, and making a product for a quantum computing for quantum machine learning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>trying to hack the world. You're trying to find how to apply the physics that I know, my knowledge of the world, to do things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, in quantum computing in particular, I think there's just a lot of limits to engineering. It just seems to be extremely hard.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, there's a lot of value to be exploring quantum computing, quantum machine learning in theory with math. I guess one question is, why is it so hard to build a quantum computer? What's your view of timelines in bringing these ideas to life?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's a sort of exodus from quantum computing and we're going to broader physics-based AI that is not quantum. So, that gives you a hint.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We should say the name of your company is Extropic?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>10, 15-year timescale personally, but I think there's other quests that can be done in the meantime. I think it's in good hands right now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, let me just explore different beautiful ideas, large or small, in quantum computing that might jump out at you from memory when you co-authored a paper titled Asymptotically Limitless Quantum Energy Teleportation via Qudit Probes. Just out of curiosity, can you explain what a qudit is versus a qubit?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. It's a D-state qubit.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a multidimensional?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the limits allowable by the laws of physics. But there's all sorts of caveats there where you can't extract more than you've put in, obviously.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But you're saying it's possible to teleport the energy because you can extract information one place and then make, based on that, some kind of prediction about another place?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm not sure what to make of that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's allowable by the laws of physics. The reality though is that the correlations decay with distance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so, you're going to have to pay the price not too far away from where you extract it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The precision decreases in terms of your ability, but still. But since you mentioned UAPs, we talked about intelligence, and I forgot to ask, what's your view on the other possible intelligences that are out there at the Meso scale? Do you think there's other intelligent alien civilizations? Is that useful to think about? How often do you think about it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>way, increasing our capabilities and becoming formidable as humans, I think that's really important, so that we're robust against whatever the universe throws at us.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But to me, it's also an interesting challenge and thought experiment on how to perceive intelligence. This has to do with quantum mechanical systems. This has to do with any kind of system that's not like humans. To me, the thought experiment is, say, the aliens are here or they are directly observable. We're just too blind, too self-centered, don't have the right sensors, or don't have the right processing of the sensor data to see the obvious intelligence that's all around us.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, that's why we work on quantum sensors. They can sense gravity,<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. That's a good one, but there could be other stuff that's not even in the currently known forces of physics.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There could be some other stuff. The most entertaining thought experiment to me is that it's other stuff that's obvious. It's not like we lack the sensors. It's all around us, the consciousness being one possible one. But there could be stuff that's just obviously there. That once you know it, it's like, \"Oh, right. Right. The thing we thought is somehow emergent from the laws of physics, we understand them, is actually a fundamental part of the universe and can be incorporated in physics. Most understood.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that, I don't know what they're doing at the bottom of the ocean, allegedly, but maybe they're collecting minerals from the bottom of the ocean.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>am I certain that these systems are here? It'd be difficult for me to say so. I only have secondhand information about there being data.<|eot_id|><|start_header_id|>assistant<|end_header_id|>About the bottom of the ocean? Yeah. But could it be things like memes? Could it be thoughts and ideas? Could they be operating at that medium? Could aliens be the very thoughts that come into my head? What's the origin of ideas? In your mind, when an idea comes to your head, show me where it originates.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and I think the notion of intelligence and the notion of life needs to be expanded much more broadly to be less anthropocentric or biocentric.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just to linger a little longer on quantum mechanics, through all your explorations on quantum computing, what's the coolest, most beautiful idea that you've come across that has been solved or has not yet been solved?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>machine learning to help us understand quantum gravity, which is still a holy grail that I would like to hit before I leave this earth.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think is going on with black holes? As information-storing and processing units, what do you think is going on with black holes?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>information between the outgoing radiation and the infalling matter. But getting that calculation right, I think we're only just starting to put the pieces together.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a few pothead-like questions I want to ask you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One, does it terrify you that there's a giant black hole at the center of our galaxy?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>shop near it to fast-forward, meet a future civilization, if we have a limited lifetime, if you could go orbit a black hole and emerge.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If there's a special mission that could take you to a black hole, would you volunteer to go travel?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>To orbit and obviously not fall into it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's obvious. It's obvious to you that everything's destroyed inside a black hole? All the information that makes up Guillaume is destroyed? Maybe on the other side, Beff Jezos emerges and it's just all like it's tied together in some deeply memeful way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>so we don't know yet. We need to ascend the Kardashev Scale to answer these questions to peer into that regime of higher energy physics.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And maybe you can speak to the Kardashev Scale for people who don't know. So one of the sort of meme-like principles and goals of the e/acc movement is to ascend the Kardashev Scale. What is the Kardashev Scale and when do we want to ascend it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Type II would be harnessing all the energy that is output by the sun. And I think Type III is like the whole galaxy equivalent-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Galaxy, I think [inaudible 01:46:14] yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we don't even know what it has its store for us in the future. It could be something of beauty we can't even imagine today.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So this is probably a good place to talk a bit about the e/acc movement in a Substack blog post titled, What the Fuck is e/acc? Or actually, What the F* is e/acc?, you write, \"Strategically speaking, we need to work towards several overarching civilization goals that are all interdependent. And the four goals are, increase the amount of energy we can harness as a species, (climb the Kardashev gradient). In the short term, this almost certainly means nuclear fission. Increase human flourishing via pro-population growth policies and pro-economic growth policies. Create artificial general intelligence, the single greatest force multiplier in human history. And finally, develop interplanetary and interstellar transport so that humanity can spread beyond the earth. Could you build on top of that to maybe say, what to you is the e/acc movement? What are the goals? What are the principles?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The goal is for the human techno-capital memetic machine to become self-aware and to hyperstitiously engineer its own growth. So let's decompress that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Define each of those words.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>works, fundamentally it seeks to grow, and we simply want to lean into the natural tendencies of the system to adapt for its own growth.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So in that way, you're right, the e/acc is literally a memetic optimism virus that is constantly drifting, mutating, and propagating in a decentralized fashion. So memetic optimism virus. So you do want it to be a virus to maximize the spread, and it's hyperstitious, therefore the optimism will incentivize its growth.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Some are just memes, but some are more serious. Vitalik Buterin recently made a d/acc fork. He has his own sort of fine-tunings of e/acc.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Does anything jump out to memory of the unique characteristic of that fork from Vitalik?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>being conservative, suppressing thought, suppressing speech, adding constraints, adding too many regulations, slowing things down. And so, we're trying to bring balance to the force.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Balance to the force of human civilization.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>entropy. Systems want to equilibrate, balance these two things. I thought that the balance was lacking, and so we created this movement to bring balance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I like the visual of the landscape of ideas evolving through forks. So on the other part of history, thinking of Marxism as the original repository, and then Soviet Communism is a fork of that, and then the Maoism is a fork of Marxism and Communism. And so those are all forks. They're exploring different ideas.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of viral pessimism, decelerative movement, we needed to have not only one movement, but many, many variants, so it's very hard to pinpoint and stop.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the overarching thing is nevertheless a kind of mimetic optimism pandemic. Okay, let me ask you, do you think e/acc to some degree is a cult?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Define cult?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think a lot of human progress is made when you have independent thought, so you have individuals that are able to think freely. And very powerful mimetic systems can kind of lead to group think. There's something in human nature that leads to mass hypnosis, mass hysteria. We start to think alike whenever there's a sexy idea that captures our minds. And so it's actually hard to break us apart, pull us apart, diversify a thought. So to that degree, to which degree is everybody kind of chanting \"E/acc, e/acc\" like the sheep in Animal Farm?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>more fun. For example, we talk about thermodynamics being our god, and sometimes we do cult-like things, but there's no ceremony and robes and whatnot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Not yet.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>up in one cluster that's very cult-like. And so cults usually, they don't allow people to defect or start competing forks, whereas we encourage it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The pros and cons of humor in meme, in some sense there's like a wisdom to memes. What is it, the Magic Theater? What book is that from? Hermann Hesse. Steppenwolf, I think. But there's a kind of embracing of the absurdity that seems to get to the truth of things, but at the same time, it can also decrease the quality and the rigor of the discourse.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you feel the tension of that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>literally trying to polarize the tech ecosystem, at the end of the day so that we can have a conversation and find an optimum together.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, that's kind of what I try to do with this podcast given the landscape of things, to still have long-form conversations. But there is a degree to which absurdity is fully embraced. In fact, this very conversation is multi-level absurd. So first of all, I should say that just very recently I had a conversation with Jeff Bezos, and I would love to hear your, Beff Jezos, opinions of Jeff Bezos. Speaking of intergalactic Jeff Bezos. What do you think of that particular individual whom your name has inspired?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think we can all be grateful to Jeff, and he's one of the great entrepreneurs out there. one of the best of all time, unarguably.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And of course, the work at Blue Origin, similar to the work at SpaceX, is trying to make humans a multi-planetary species, which that seems almost like a bigger thing than the capitalist machine. Or it's the capitalist machine at a different timescale perhaps?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so that's why I'm for capitalism over, say, giving all our money to the government and letting them figure out how to allocate it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why do you think it's a viral and it's a popular meme to criticize billionaires? Since you mentioned billionaires. Why do you think there's quite a widespread criticism of people with wealth, especially those in the public eye, like Jeff and Elon and Mark Zuckerberg, and who else? Bill Gates.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you're far more in control of the future than you think. You have agency to change the world, go out and do it. Here's permission.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Each individual has agency. The motto, \"Keep building\" is often heard. What does that mean to you, and what does that have to do with Diet Coke? By the way, thank you so much for the Red Bull. It's working pretty well. I'm feeling pretty good.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to innovate our way out of this dilemma.\" And we're trying to get people to do the latter rather than the former, at all scales.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Elon is an interesting case. You are a proponent, you celebrate Elon, but he's also somebody who has for a long time warned about the dangers, the potential dangers, existential risks of artificial intelligence. How do you square the two? Is that a contradiction to you?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Neuralink. I think if he can't stop the progress of AI, he's building the technology to merge. Look at the actions, not just the words.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, there's some degree where being concerned... Maybe using human psychology, being concerned about threats all around us is a motivator. It's an encouraging thing. I operate much better when there's a deadline. The fear of the deadline. And I, for myself, create artificial things, like I want to create in myself this kind of anxiety as if something really horrible will happen if I miss the deadline. I think there's some degree of that here, because creating AI that's aligned with humans has a lot of potential benefits. And so a different way to reframe that is, \"If you don't, we're all going to die.\" It just seems to be a very powerful psychological formulation of the goal of creating human-aligned AI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to own a piece of the future. It can't just be legislated behind some wall where only a few players get to play the game.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The e/acc movement is often caricatured to mean progress and innovation at all costs. Doesn't matter how unsafe it is, doesn't matter if it causes a lot of damage. You just build cool shit as fast as possible, stay up all night with a Diet Coke, whatever it takes. I guess, I don't know if there's a question in there, but how important to you and what you've seen the different formulations of e/acc, is AI safety?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to the growth of this beautiful machine so that we can seek to the stars. That's really inspiring. That's also a sort of neuro hack.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you're saying AI safety is important to you, but right now the landscape of ideas you see is, AI safety as a topic is used more often to gain centralized control. So in that sense, you're resisting it, as a proxy for gaining centralized control?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>me, that just opens up the door wide open for a big brother, 1984-like scenario. And that's not a future I want to live in.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because we have some examples throughout history when that did not lead to a good outcome.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mentioned you founded a company, Extropic, that recently announced a 14.1 million seed round. What's the goal of the company? You're talking about a lot of interesting physics things, so what are you up to over there that you can talk about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>physics-based computing system and physics-based AI algorithms that are inspired by out-of-equilibrium thermodynamics, or harness it directly to do machine learning as a physical process.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what does that mean, machine learning as a physical process? Is that hardware? Is it software? Is it both? Is it trying to do the full stack in some kind of unique way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>computer architects, those that have designed IBM's and AWS's systems. They've left quantum computing to help us build what we call actually a thermodynamic computer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A thermodynamic computer. Well, actually let's linger around TensorFlow Quantum. What lessons have you learned from TensorFlow Quantum? Maybe you can speak to what it takes to create essentially, what, like a software API to a quantum computer?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right. That was a challenge to invent, to build, and then to get to run on the real devices.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you actually speak to what it is?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>optimize the system to accomplish a task, whatever that task may be. And that's a very universal meta framework for how to program physics-based computers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So try to parameterize everything, make those parameters differentiable, and then optimize?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Is there some more practical engineering lessons from TensorFlow Quantum? Just organizationally too, like the humans involved and how to get to a product, how to create good documentation? I don't know. All of these little subtle things that people might not think about.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>folks that have an AI background, folks that have a pure physics background, and somehow we have to make them talk to one another. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there a magic, is there some science and art to the hiring process, to building a team that can create magic together?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's really hard to pinpoint that je ne sais quoi, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I didn't know you speak French. That's very nice.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I'm actually French Canadian.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, you are a legitimately French Canadian.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I am.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I thought you were just doing that for the cred.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to just getting prescribed what to do at this stage, when you're at the pioneering stage, that's not necessarily who you want to hire. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you mentioned with Extropic you're trying to build the physical substrate for generative AI. What's the difference between that and the AGI AI itself? So, is it possible that in the halls of your company, AGI will be created? Or will AGI just be using this as a substrate?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think our goal is to both run human like AI, or anthropomorphic AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sorry for use of the term AGI. I know it's triggering for you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>scientist. And to us, that joint system of a physics-based and an anthropomorphic AI is the closest thing to a fully general, artificially intelligent system.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you can get closer to truth by grounding of the AI to physics, but you can also still have a anthropomorphic interface to us humans that like to talk to other humans, or human-like systems. So, on that topic, I suppose that is one of the big limitations of current large language models to you, is that they're good bullshitters, they're not really grounded to truth necessarily. Would that be fair to say?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>its priors or its uncertainties about the world very accurately. So, you need a different type of AI to compliment this text extrapolation AI. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mentioned singularity earlier. How far away are we from a singularity?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>computationally, and thermodynamically, to acquiring information about the world in order to be able to predict and control it. And that keeps things in check.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's funny you mentioned the other side of the aisle. So, in the poll I posted about p(doom) yesterday, what's the probability of doom? There seems to be a nice division between people think it's very likely, and very unlikely. I wonder if in the future there'll be the actual Republicans versus Democrats division, blue versus red? Is the AI doomers versus the e/accers, EAC? [inaudible 02:26:53].<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. So, this movement is not right wing or left wing fundamentally, it's more like up versus down, in terms of the scale of-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Which one is the up? Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... Civilization, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fundamental issue of our time. It's not just centralization versus decentralization. It's kind of do we go... It's like tech progressivism, versus techno conservatism. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So e/acc as a movement is often formulated in contrast to EA, effective altruism. What do you think are the pros and cons of effective altruism? What's interesting, insightful to you about them, and what is negative?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right. I think people trying to do good from first principles is good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We should actually say, and sorry to interrupt, we should probably say that, and you can correct me if I'm wrong, but effective altruism is the kind of movement that's trying to do good optimally, where good is probably measured something like the amount of suffering in the world. You want to minimize it. And there's ways that that can go wrong, as any optimization can. And so, it's interesting to explore how things can go wrong.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We're both trying to do good to some extent, and we're arguing for which loss function we should use, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to make progress, and ensure humanity flourishes, and gets to grow. We just have different loss functions, and different ways of going about doing it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there a degree, maybe you can educate me, correct me, I get a little bit skeptical when there's an equation involved trying to reduce all of the human civilization, human experience to an equation. Is there a degree that we should be skeptical of the tyranny of an equation of a loss function over wish to optimize? Like having a kind of intellectual humility about optimizing over loss functions?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or not, and we're just trying to point out what is, and try to orient ourselves, and chart a path forward given this fundamental truth.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But there's still some uncertainty, there's still a lack of information, and humans tend to fill the gap of the lack of information with narratives. And so, how they interpret... Even physics is up to interpretation when there's uncertainty involved. And humans tend to use that to further their own means. So, it's always, whenever there's an equation, it just seems like until we have really perfect understanding of the universe, humans will do what humans do, and they try to use the narrative of doing good to fool the populace into doing bad. I guess that this is something that we should be skeptical about in all movements.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right? So we invite skepticism. Right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you have an understanding of what might, to a degree that went wrong, what do you think may have gone wrong with effective altruism that might also go wrong with effective accelerationism?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think that yeah, ultimately, it was the perfect cover to acquire tons of power, and capital, and unfortunately sometimes that corrupts people over time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What does a perfectly productive day, since building is important, what is a perfectly productive day in the life of Guillaume Verdon look like? How much caffeine do you consume? What's a perfect day?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>CEO, I have to interface with the outside world, whether it's customers, or investors, or interviewing potential candidates. And usually I'll have ketones, exogenous ketones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, are you on a keto diet, or is this-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>football, and whatnot, but I like to have a meal after part of my day is done, and so I can just have extreme focus.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You do the social interactions earlier in the day without food.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>still technical. I'm contributing to most patents. And there, I'll just stay up late into the night, and work with engineers on very technical problems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it's like the 9:00 PM to 4:00 AM, whatever though, that range of time.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>was at Google, and had to manage a bit of the product during the day, and have meetings, and then do technical work at night.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exercise, sleep, those kinds of things. You said football, you used to play football?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>explore, and follow your nose, and try to acquire, and compress as much information as you can into your brain. Anything that you find interesting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And caring about a thing. Like you said, which is interesting, it works for me really well, is tricking yourself that you care about a thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then you start to really care about it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, it's funny, the motivation is a really good catalyst for learning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right. And so, at least part of my character, as Beff Jezos is kind of like...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, hype man.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of skill I've carried over to research, when it's go time, when the stakes are high, somehow I just reach another level of neural performance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So Beff Jezos is your sort of embodiment representation of your intellectual Hulk. It's your productivity Hulk that you just turn on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What have you learned about the nature of identity from having these two identities? I think it's interesting for people, to be able to put on those two hats so explicitly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>now it's fully merged. But it was already, before the docs, it was already starting mentally that I am this character. It's part of me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Would you recommend people have an alt?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Absolutely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Like young people. Would you recommend them to explore different identities by having alts? Alt accounts?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>aren't fully formed, and converge onto maybe hidden truths that were hard to converge upon if it was just through open conversation with real names.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I really believe in not radical, but rigorous empathy. It's like really considering what it's like to be a person of a certain viewpoint, and taking that, as a thought experiment, farther and farther and farther. And one way of doing that as an alt account. That's a fun, interesting way to really explore what it's like to be a person that believes a set of beliefs, and taking that across the span of several days, weeks, months. Of course there's always the danger of becoming that. That's the Nietzche, \"Gaze long into the abyss, the abyss gazes into you.\" You have to be careful.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Breaking Beff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, right. Breaking Beff. Yeah. You wake up with a shaved head one day, just like, \"Who am I? What have I become?\" So, you've mentioned quite a bit of advice already, but what advice would you give to young people of, in this interesting world we're in, how to have a career and how to have a life they can be proud of?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so, not everybody has to study mathematics, but I think it's really a huge cognitive unlock to learn math, and some physics, and engineering.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Get as close to the base of the stack as possible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>relevant in a few years. Of course there's a sort of transfer learning you can do, but then you have to always transfer learn, constantly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I guess the closer you are to the base of the stack, the easier the transfer learning, the shorter the jump.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>applying some of the thinking from out of [inaudible 02:45:36] thermodynamics to understanding the world around us, and it's led to e/acc, and this movement.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you look at you're one cog in the machine, in the capitalist machine, one human, and if you look at yourself, do you think mortality is a feature or a bug? Would you want to be immortal?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>dissipative adaptation, there's the word dissipation. Dissipation is important, death is important. We have a saying in physics, physics progresses one funeral at a time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I am for death, but I do think that longer lifespan, and longer time for neuroplasticity, bigger brains should be something we should strive for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, and that, Jeff Bezos, and Beff Jezos agree that all companies die. And for Jeff, the goal is to try to, he calls it day one thinking, try to constantly, for as long as possible, reinvent, sort of extend the life of the company. But eventually it too will die, because it's so difficult to keep reinventing. Are you afraid of your own death?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have ideas and things I'd like to achieve in this world before I have to go, but I don't think I'm necessarily afraid of death.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you're not attached to this particular body, and mind that you got?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, I'm sure there's going to be better versions of myself in the future, or...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Forks?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>so, I don't think I'm particularly an optimum that needs to stick around forever. I think there's going to be greater optima in many ways.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think is the meaning of it all? What's the why of the machine? The e/acc machine?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be very poetic. It's purely conjecture. But again, this is why figuring out quantum gravity would allow us to understand if we can do that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And above that, why does it all seems so beautiful and exciting? The quest to figuring out quantum gravity seems so exciting. Why? Why is that? Why are we drawn to that? Why are we pulled towards that? Just that puzzle solving creative force that underpins all of it, it seems like.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>engineering perception, prediction, and control to be done, to climb up the Carta shift scale. And to us, that's the great challenge of our times.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And when you're not sure where to go, let the meme pave the way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Guillaume, Beff, thank you for talking today. Thank you for the work you're doing. Thank you for the humor, and the wisdom you put into the world. This was awesome.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thank you so much for having me, Lex, It's a pleasure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thank you for listening to this conversation with Guillaume Verdon. To support this podcast. Please check out our sponsors in the description. And now, let me leave you with some words from Albert Einstein. \"If at first the idea is not absurd, then there is no hope for it.\" Thank you for listening. I hope to see you next time.<|eot_id|>"}
