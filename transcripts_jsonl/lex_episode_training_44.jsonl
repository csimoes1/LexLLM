{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>would lead to a very bad future in which all of our information diet is controlled by a small number of companies who proprietary systems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I believe that people are fundamentally good, and so if AI, especially open source AI can make them smarter, it just empowers the goodness in humans.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Okay. I think people are fundamentally good and in fact, a lot of doomers are doomers because they don't think that people are fundamentally good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with Yann LeCun, his third time on this podcast. He is the chief AI scientist at Meta, professor at NYU, Turing Award winner and one of the seminal figures in the history of artificial intelligence. He and Meta AI have been big proponents of open sourcing, AI development and have been walking the walk by open sourcing many of their biggest models, including Llama 2 and eventually Llama 3. Also, Yann has been an outspoken critic of those people in the AI community who warn about the looming danger and existential threat of AGI. He believes the AGI will be created one day, but it will be good. It will not escape human control, nor will it dominate and kill all humans. At this moment of rapid AI development, this happens to be somewhat a controversial position, and so it's been fun seeing Yann get into a lot of intense and fascinating discussions online as we do in this very conversation. This is the Lex Fridman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Yann LeCun. You've had some strong statements, technical statements about the future of artificial intelligence throughout your career actually, but recently as well, you've said that autoregressive LLMs are not the way we're going to make progress towards superhuman intelligence. These are the large language models like GPT-4, like Llama 2 and 3 soon and so on. How do they work and why are they not going to take us all the way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>through language. Everything that we learn in the first few years of life, and certainly everything that animals learn has nothing to do with language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it would be good to maybe push against some of the intuition behind what you're saying. So it is true there's several orders of magnitude more data coming into the human mind much faster, and the human mind is able to learn very quickly from that, filter the data very quickly. Somebody might argue your comparison between sensory data versus language, that language is already very compressed. It already contains a lot more information than the bytes it takes to store them if you compare it to visual data. So there's a lot of wisdom and language. There's words, and the way we stitch them together, it already contains a lot of information. So is it possible that language alone already has enough wisdom and knowledge in there to be able to, from that language, construct a world model and understanding of the world, an understanding of the physical world that you're saying LLMs lack?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of learning or reasoning architecture or whatever are we missing that basically prevent us from having level five sort of in cars and domestic robots?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can a large language model construct a world model that does know how to drive and does know how to fill a dishwasher, but just doesn't know how to deal with visual data at this time, so it can operate in a space of concepts?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>trained to really understand the world. They're not trained with video, for example. They don't really understand intuitive physics, at least not at the moment.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you don't think there's something special to you about intuitive physics, about sort of common sense reasoning about the physical space, about physical reality. That to you is a giant leap that LLMs are just not able to do?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>doing and the answer that we're planning to produce is not linked to whether we're going to see it in French or Russian or English.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Chomsky just rolled his eyes, but I understand, so you're saying that there's a bigger abstraction that goes before language and maps onto language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right. It's certainly true for a lot of thinking that we do.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that obvious that we don't... You're saying your thinking is same in French as it is in English?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, pretty much.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Pretty much or how flexible are you if there's a probability distribution?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, it depends what kind of thinking, right? If it's producing puns, I get much better in French than English about that, or much worse.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there an abstract representation of puns? Is your humor an abstract... When you tweet and your tweets are sometimes a little bit spicy, is there an abstract representation in your brain of a tweet before it maps onto English?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There is an abstract representation of imagining the reaction of a reader to that text.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Or you start with laughter and then figure out how to make that happen?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot of knowledge. So it can retrieve some things, but it's going to just spit out one token after the other without planning the answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But you're making it sound just one token after the other. One token at a time generation is bound to be simplistic, but if the world model is sufficiently sophisticated that one token at a time, the most likely thing it generates is a sequence of tokens is going to be a deeply profound thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But then that assumes that those systems actually possess an eternal world model.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So really goes to the... I think the fundamental question is can you build a really complete world model, not complete, but one that has a deep understanding of the world?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>video, show a system, a piece of video, and then ask you to predict the reminder of the video, basically predict what's going to happen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One frame at a time, do the same thing as the autoregressive LLMs do, but for video.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right. Either one frame at a time-<|eot_id|><|start_header_id|>assistant<|end_header_id|>LVMs.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it is. That has been essentially a complete failure and it works really well for text. That's the principle that is used for LLMs, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So where's the failure exactly? Is it that it's very difficult to form a good representation of an image, like a good embedding of all the important information in the image? Is it in terms of the consistency of image to image, to image to image that forms the video? If we do a highlight reel of all the ways you failed, what's that look like?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of images, et cetera, you do get good representations and the performance on recognition tasks is much better than if you do this self-supervised retraining.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The architecture is good?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>good, but the fact that you train the system to reconstruct images does not lead it to produce to long, good generic features of images.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you train in a self-supervised way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Self-supervised by reconstruction.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, by reconstruction.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Okay, so what's the alternative? The alternative is joint embedding.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What is joint embedding? What are these architectures that you're so excited about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>thing, and you rely on some other tricks to prevent the system from collapsing. And we have half a dozen different methods for this now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what is the fundamental difference between joint embedding architectures and LLMs? Can JEPA take us to AGI? Whether we should say that you don't like the term AGI, and we'll probably argue I think every single time I've talked to you, we've argued about the G in AGI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I get it. I get it. Well, we'll probably continue to argue about it. It's great. You like AMI because you like French and ami is friend in French, and AMI stands for advanced machine intelligence. But either way, can JEPA take us to that towards that advanced machine intelligence?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that is not predictable. And so we can get away without doing the joint embedding, without lifting the abstraction level and by directly predicting words.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So joint embedding, it's still generative, but it's generative in this abstract representation space?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And you're saying language, we were lazy with language because we already got the abstract representation for free, and now we have to zoom out, actually think about generally intelligent systems. We have to deal with a full mess of physical reality, of reality. And you do have to do this step of jumping from the full, rich, detailed reality to a abstract representation of that reality based on what you can then reason and all that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>really, because it's already compressed. You're right about that, but that means it's also less redundant, and so self-supervision, you will not work as well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is it possible to join the self-supervised training on visual data and self-supervised training on language data? There is a huge amount of knowledge, even though you talk down about those 10 to the 13 tokens. Those 10 to the 13 tokens represent the entirety- Those 10 to the 13 tokens represent the entirety, a large fraction of what us humans have figured out, both the shit-talk on Reddit and the contents of all the books and the articles and the full spectrum of human intellectual creation. So is it possible to join those two together?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is going to be a winner, but before that, we have to focus on how do we get systems to learn how the world works?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So this joint-embedding predictive architecture, for you, that's going to be able to learn something like common sense, something like what a cat uses to predict how to mess with its owner most optimally by knocking over a thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>system basically ignores the input. So that works very well. The two techniques we developed at FAIR, DINO and I-JEPA work really well for that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what kind of data are we talking about here?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>for example, changing the size a little bit, maybe changing the orientation, blurring it, changing the colors, doing all kinds of horrible things to it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But basic horrible things?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And what we mask is actually kind of a temporal tube, so a whole segment of each frame in the video over the entire video.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that tube was statically positioned throughout the frames, just literally it's a straight tube.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>tell you what action is taking place in the video with pretty good accuracy. So that's the first time we get something of that quality.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a good test that a good representation is formed. That means there's something to this.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is physically possible or completely impossible, because some object disappeared or an object suddenly jumped from one location to another or changed shape or something.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it's able to capture some physics based constraints about the reality represented in the video, about the appearance and the disappearance of objects.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, that's really new.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, but can this actually get us to this kind of world model that understands enough about the world to be able to drive a car?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an objectives that you fix. This is the way rocket trajectories have been planned since computers have been around, so since the early '60s essentially.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So yes, for a model predictive control, but you also often talk about hierarchical planning. Can hierarchical planning emerge from this somehow?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to do this in AI. Nobody knows how to train a system to learn the appropriate multiple levels of representation so that hierarchical planning works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Does something like that already emerge? So can you use an LLM, state-of-the-art LLM, to get you from New York to Paris by doing exactly the kind of detailed set of questions that you just did, which is, can you give me a list of 10 steps I need to do, to get from New York to Paris? And then for each of those steps, can you give me a list of 10 steps, how I make that step happen? And for each of those steps, can you give me a list of 10 steps to make each one of those, until you're moving your individual muscles, maybe not, whatever you can actually act upon using your own mind.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>answer some of those questions down to some level of abstraction, under the condition that they've been trained with similar scenarios in their training set.<|eot_id|><|start_header_id|>assistant<|end_header_id|>They would be able to answer all of those questions, but some of them may be hallucinated meaning non-factual.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to plan for situations where that they never encountered before. They basically are going to have to regurgitate the template that they've been trained on.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just for the example of New York to Paris, is it going to start getting into trouble? Which layer of abstraction do you think you'll start? I can imagine almost every single part of that, an LLM would be able to answer somewhat accurately, especially when you're talking about New York and Paris, major cities.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of the reasons you need experience of the physical world, which is much higher bandwidth than what you can express in words, in human language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So everything we've been talking about on the joint embedding space, is it possible that that's what we need for the interaction with physical reality on the robotics front, and then just the LLMs are the thing that sits on top of it for the bigger reasoning, about the fact that I need to book a plane ticket and I need to know how to go to the websites and so on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>JEPA that basically lift the abstraction level of the representation without attempting to reconstruct the detail of the situation, that's why we need JEPAs for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I would love to sort of linger on your skepticism around auto regressive LLMs. So one way I would like to test that skepticism is everything you say makes a lot of sense, but if I apply everything you said today and in general to I don't know, 10 years ago, maybe a little bit less, no, let's say three years ago, I wouldn't be able to predict the success of LLMs. So does it make sense to you that autoregressive LLMs are able to be so damn good?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain your intuition? Because if I were to take your wisdom and intuition at face value, I would say there's no way autoregressive LLMs, one token at a time, would be able to do the kind of things they're doing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>surprise occurred quite a while back, with work from Google, Meta, OpenAI, et cetera, going back to the GPT kind of work, general pre-trained transformers.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mean like GPT2? There's a certain place where you start to realize scaling might actually keep giving us an emergent benefit.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I mean there were work from various places, but if you want to place it in the GPT timeline, that would be around GPT2, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, because you said it so charismatic and you said so many words, but self supervised learning, yes. But again, the same intuition you're applying to saying that auto aggressive LLMs cannot have a deep understanding of the world. If we just apply that, same intuition, does it make sense to you that they're able to form enough of a representation in the world to be damn convincing, essentially passing the original touring test with flying colors?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>system is fluent in manipulating language, then it has all the characteristics of human intelligence, but that impression is false. We're really fooled by it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think Alan Turing would say, without understanding anything, just hanging out with it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>bad test, okay? This is what the AI community has decided many years ago that the Turing test was a really bad test of intelligence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What would Hans Marvek say about the larger language models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Hans Marvek would say that Marvek Paradox still applies. Okay, we can pass-<|eot_id|><|start_header_id|>assistant<|end_header_id|>You don't think he would be really impressed?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have systems now based on those combination of ideas that can do real time translation of hundreds of languages into each other, speech to speech.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Speech to speech, even including, which is fascinating, languages that don't have written forms.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just spoken only.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>generative model. So I'm telling people, everybody's talking about generative AI. If you're really interested in human level AI, abandon the idea of generative AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, but you really think it's possible to get far with the joint embedding representation. So there's common sense reasoning, and then there's high level reasoning. I feel like those are two... The kind of reasoning that LLMs are able to do, okay, let me not use the word reasoning, but the kind of stuff that LLMs are able to do, seems fundamentally different than the common sense reasoning we use to navigate the world. It seems like we're going to need both. Would you be able to get, with the joint embedding, which is JEPA type of approach, looking at video, would you be able to learn, let's see, well, how to get from New York to Paris or how to understand the state of politics in the world today. These are things where various humans generate a lot of language and opinions on, in the space of language, but don't visually represent that in any clearly compressible way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>publicly available texts in the world that I cannot get from New York to Paris by snapping my fingers. That's not going to work, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>talk to each other, we know we have a common experience of the world. A lot of it is similar, and LLMs don't have that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But see, it's present. You and I have a common experience of the world in terms of the physics of how gravity works and stuff like this, and that common knowledge of the world, I feel like is there, in the language. We don't explicitly express it, but if you have a huge amount of text, you're going to get this stuff that's between the lines. In order to form a consistent world model, you're going to have to understand how gravity works, even if you don't have an explicit explanation of gravity. So even though in the case of gravity, there is explicit explanations of gravity in Wikipedia. But the stuff that we think of as common sense reasoning, I feel like to generate language correctly, you're going to have to figure that out. Now, you could say as you have, there's not enough text... Sorry, okay, so you don't think so?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do high level common sense, to have high level common sense, you need to have the low level common sense to build on top of.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that's not there.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>fact that implicit in all languages in the world is the underlying reality, is a lot of underlying reality, which is not expressed in language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that obvious to you?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, totally.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So all the conversations we had... Okay, there's the dark web, meaning whatever, the private conversations like DMs and stuff like this, which is much, much larger probably than what's available, what LLMs are trained on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You don't need to communicate the stuff that is common, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the humor, all of it, no, you do, you don't need to, but it comes through. If I accidentally knock this over, you'll probably make fun of me in the content of the you making fun of me will be explanation of the fact that cups fall, and then gravity works in this way. And then you'll have some very vague information about what kind of things explode when they hit the ground. And then maybe you'll make a joke about entropy or something like this, then we'll never be able to reconstruct this again. You'll make a little joke like this and there'll be a trillion of other jokes. And from the jokes, you can piece together the fact that gravity works and mugs can break and all this kind of stuff. You don't need to see, it'll be very inefficient. It's easier to knock the thing over, but I feel like it would be there if you have enough of that data.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>most of the information of this type that we have accumulated when we were babies, it's just not present in text, in any description, essentially.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the sensory data is a much richer source for getting that kind of understanding.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>world, they can only observe. And you accumulate a gigantic amount of knowledge just from that. So that's what we're missing from current AI systems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think in one of your slides, you have this nice plot that is one of the ways you show that LLMs are limited. I wonder if you could talk about hallucinations from your perspectives, the why hallucinations happen from large language models and to what degree is that a fundamental flaw of large language models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>means is that every time you produce a token, the probability that you stay within the set of correct answer decreases and it decreases exponentially.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's a strong, like you said, assumption there that if there's a non-zero probability of making a mistake, which there appears to be, then there's going to be a kind of drift.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and that drift is exponential. It's like errors accumulate. So the probability that an answer would be nonsensical increases exponentially with the number of tokens.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that obvious to you, by the way? Well, mathematically speaking maybe, but isn't there a kind of gravitational pull towards the truth? Because on average, hopefully, the truth is well represented in the training set?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>will be outside of the set of prompts that's been trained on, or things that are similar, and then it will just spew complete nonsense.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you say prompt, do you mean that exact prompt or do you mean a prompt that's in many parts, very different than? Is it that easy to ask a question or to say a thing that hasn't been said before on the internet?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>answer something completely different than it would have answered without this. So that's a way to jailbreak the system, basically go outside of its conditioning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a very clear demonstration of it, but of course, that goes outside of what is designed to do, right? If you actually stitch together reasonably grammatical sentences, is it that easy to break it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and then you just substitute a few words by the same word in another language and all of a sudden the answer is complete nonsense.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What I'm saying is, which fraction of prompts that humans are likely to generate are going to break the system?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we're faced with a complex problem or a complex question, we spend more time trying to solve it and answer it because it's more difficult.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a prediction element, there's an iterative element where you're adjusting your understanding of a thing by going over and over and over, there's a hierarchical elements on. Does this mean it's a fundamental flaw of LLMs or does it mean that-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... There's more part to that question, now you're just behaving like an LLM, immediately answering. No, that it's just the low level world model on top of which we can then build some of these kinds of mechanisms, like you said, persistent long-term memory or reasoning, so on. But we need that world model that comes from language. Maybe it is not so difficult to build this kind of reasoning system on top of a well constructed world model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is, produce the prompt and then search through the space of possible answers for one that minimizes that number, that's called an energy based model.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that energy based model would need the model constructed by the LLM?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of future data systems, they will think about their answer, plan their answer by optimization before turning it into text, and that is turning complete.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain exactly what the optimization problem there is? What's the objective function? Just linger on it, you briefly described it, but over what space are you optimizing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The space of representations.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It goes abstract representation?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we ignore the issue for a moment of how you train that system to measure whether an answer is a good answer for a fraction.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure. Suppose such a system could be created, but what's this search like process?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Then by gradient descent, by back propagating gradients, you can figure out how to modify the representation of the answers so as to minimize that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's still a gradient based?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that the representation now can be optimized through gradient descent, but also is independent of the language in which you're going to express the answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. So you're operating in the subtract representation. This goes back to the joint embedding, that it's better to work in the space of, I don't know, or to romanticize the notion like space of concepts versus the space of concrete sensory information.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But can this do something like reasoning, which is what we're talking about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>iteratively refine your answer to go towards the best, that's much more efficient. But you can only do this in continuous spaces with differentiable functions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're talking about the ability to think deeply or to reason deeply, how do you know what is an answer that's better or worse based on deep reasoning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>places because there's only a limited volume of space that can take low energy by the construction of the system or by the regularizing function.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We've been talking very generally, but what is a good X and a good Y? What is a good representation of X and Y? Because we've been talking about language and if you just take language directly that presumably is not good, so there has to be some kind of abstract representation of ideas.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You can do this with language directly by just, X is a text and Y is a continuation of that text.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Or X is a question, Y is the answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But you're saying that's not going to take it, that's going to do what LLMs are doing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then that Z can be viewed as a representation of a good answer that you can translate into a Y that is a good answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This system could be trained in a very similar way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>doing it on the joint probability of all the symbols in a sequence, you factorize that probability in terms of conditional probabilities over successive tokens.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you do this for visual data?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We've been doing this with I-JEPA architectures, basically-<|eot_id|><|start_header_id|>assistant<|end_header_id|>The joint embedding.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>things, effectively, one of them is a corrupted version of the other, it gives you a high energy if the two images are completely different.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And hopefully that whole process gives you a really nice compressed representation of a visual reality?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And we know it does because then we use those representations as input to a classification system or something and that it works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then that classification system works really nicely, okay. Well, so to summarize, you recommend in a spicy way that only Yann LeCun can, you recommend that we abandon generative models in favor of joint embedding architectures?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Abandon autoregressive generation.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This feels like court testimony, abandon probabilistic models in favor of energy based models as we talked about, abandon contrastive methods in favor of regularized methods. And let me ask you about this, you've been for a while, a critic of reinforcement learning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last recommendation is that we abandon RL in favor of model predictive control, as you were talking about, and only use RL when planning doesn't yield the predicted outcome, and we use RL in that case to adjust the world model or the critic.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You've mentioned RLHF, reinforcement learning with human feedback, why do you still hate reinforcement learning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't hate reinforcement learning, and I think-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's all love, yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a system is to first have it learn good representations of the world and world models from mostly observation, maybe a little bit of interactions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then steered based on that, if the representation is good, then the adjustments should be minimal.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the good representations, you already have your world model, but you need to adjust it for the situation at hand, that's when you use RL.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why do you think RLHF works so well? This enforcement learning with human feedback, why did it have such a transformational effect on large language models than before?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>effect is human feedback, there is many ways to use it, and some of it is just purely supervised, actually, it's not really reinforcement learning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the HF?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>person like, what is a good answer for this? Then you just type the answer. There's lots of ways that those systems are being adjusted.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Now, a lot of people have been very critical of the recently released Google's Gemini 1.5 for essentially, in my words, I could say super woke in the negative connotation of that word. There is some almost hilariously absurd things that it does, like it modifies history like generating images of a black George Washington, or perhaps more seriously something that you commented on Twitter, which is refusing to comment on or generate images or even descriptions of Tiananmen Square or The Tank Man, one of the most legendary protest images in history. Of course, these images are highly censored by the Chinese government and therefore, everybody started asking questions of what is the process of designing these LLMs? What is the role of censorship and all that kind of stuff? So you commented on Twitter saying that open source is the answer.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Essentially, so can you explain?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So the direction of inevitable direction of history is that the vast majority of AI systems will be built on top of open source platforms.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So that's a beautiful vision. So meaning a company like Meta or Google or so on should take only minimal fine-tuning steps after building the foundation pre-trained model as few steps as possible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Basically.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can Meta afford to do that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So I don't know if you know this, but companies are supposed to make money somehow and open source is giving away... I don't know. Mark made a video, Mark Zuckerberg, very sexy video talking about 350,000 Nvidia H100s.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, [inaudible 01:44:12]<|eot_id|><|start_header_id|>assistant<|end_header_id|>The math of that is just for the GPUs, that's 100 billion plus the infrastructure for training everything. So I'm no business guy, but how do you make money on that? So the division you paint is a really powerful one, but how is it possible to make money?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Okay, so you have several business models, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Mm-hmm.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>potential customer base and you need to build that system anyway for them, it doesn't hurt you to actually distribute it to the open source.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Again, I'm no business guy, but if you release the open source model, then other people can do the same kind of task and compete on it, basically provide fine-tuned models for businesses.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>By the way, I'm a huge fan of all this, but is the bet that Meta is making, it's like, \"We'll do a better job of it?\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, no. The bet is more, \"We already have a huge user base and customer base-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Ah, right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to be useful to them. Whatever we offer them is going to be useful and there is a way to derive revenue from this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>building applications with it. So Meta's ability to derive revenue from this technology is not impaired by the distribution of base models in open source.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The fundamental criticism that Gemini is getting is that as you point out on the West Coast, just to clarify, we're currently on the East Coast where I would suppose Meta AI headquarters would be. So there are strong words about the West Coast, but I guess the issue that happens is I think it's fair to say that most tech people have a political affiliation with the left wing. They lean left. So the problem that people are criticizing Gemini with is that there's in that de-biasing process that you mentioned, that their ideological lean becomes obvious. Is this something that could be escaped? You're saying open source is the only way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Have you witnessed this kind of ideological lean that makes engineering difficult?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>push the system perhaps a little too far in one direction, it's going to be non-factual. You're going to have Black Nazi soldiers in uniform.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, we so we should mention image generation of Black Nazi soldiers, which is not factually accurate.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>as well. So it's going to be impossible to produce systems that are unbiased for everyone. So the only solution that I see is diversity.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Diversity in the full meaning of that word, diversity of in every possible way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Marc Andreessen just tweeted today. Let me do a TL;DR. The conclusion is only startups and open source can avoid the issue that he's highlighting with big tech. He's asking, \"Can Big Tech actually field generative AI products?\" (1) Ever-escalating demands from internal activists, employee mobs, crazed executives, broken boards, pressure groups, extremist regulators, government agencies, the press, in quotes, \"experts\" and everything corrupting the output. (2) Constant risk of generating a bad answer or drawing a bad picture or rendering a bad video who knows what is going to say or do at any moment. (3) Legal exposure, product liability, slander, election law, many other things and so on, anything that makes Congress mad. (4) Continuous attempts to tighten grip on acceptable output, degrade the model, how good it actually is, in terms of usable and pleasant to use and effective and all that kind of stuff. (5) Publicity of bad text, images, video actual puts those examples into the training data for the next version and so on. So he just highlights how difficult this is from all kinds of people being unhappy. He said you can't create a system that makes everybody happy.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So if you're going to do the fine-tuning yourself and keep it close source, essentially, the problem there is then trying to minimize the number of people who are going to be unhappy.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're saying that almost impossible to do, and there are better ways to do open source<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are basic principles, but beyond that, if you want those systems to be useful, they will necessarily have to offend a number of people inevitably.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So open source is just better and then you get-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Diversity is better, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>And open source enables diversity.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right. Open source enables diversity.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This can be a fascinating world where if it's true that the open source world, if Meta leads the way and creates this open source foundation model world, governments will have a fine- tuned model and then potentially, people that vote left and right will have their own model and preference to be able to choose and it will potentially divide us even more. But that's on us humans. We get to figure out basically the technology enables humans to human more effectively, and all the difficult ethical questions that humans raise will just leave it up to us to figure that out.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>agree on. Then the fine-tuning that people will add or the additional guardrails that people will add will cater to their community, whatever it is.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The fine-tuning will be more about the gray areas of what is hate speech, what is dangerous and all that kind of stuff, but it's the-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Or different value systems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Still value systems. But still even with the objectives of how to build a bioweapon, for example, I think something you've commented on, or at least there's a paper where a collection of researchers is trying to understand the social impacts of these LLMs. I guess one threshold that's nice is, does the LLM make it any easier than a search would, like a Google search would?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>chemical weapons or bioweapons or things like this, it requires expertise in the real world that the LLM is not going to help you with.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It requires even the common sense expertise that we've been talking about, which is how to take language-based instructions and materialize them in the physical world requires a lot of knowledge that's not in the instructions.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in response to those things saying, \"Do you realize how hard it is to actually do the lab work?\" Like, \"No, this is not trivial.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, and Hans Moravec comes to light once again. Just to linger on LLaMA, Marc announced that LLaMA 3 is coming out eventually. I don't think there's a release date, but what are you most excited about? First of all, LLaMA 2 that's already out there and maybe the future a LLaMA 3, 4, 5, 6, 10, just the future of the open source under Meta?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>somewhat we get on to a good direction and perhaps succeed before my brain turns to a white sauce or before I need to retire.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Yeah. Is it beautiful to you just the amount of GPUs involved, the whole training process on this much compute, just zooming out, just looking at earth and humans together have built these computing devices and are able to train this one brain, then we then open source, like giving birth to this open source brain trained on this gigantic compute system, there's just the details of how to train on that, how to build the infrastructure and the hardware, the cooling, all of this kind of stuff, or are you just still that most of your excitement is in the theory aspect of it, meaning the software?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I used to be a hardware guy many years ago.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes. Yes, that's right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Decades ago.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hardware has improved a little bit. Changed-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>A little bit.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... a little bit, yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Certainly, scale is necessary but not sufficient.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Absolutely.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to have to come up with new principles, new fabrication technology, new basic components perhaps based on different principles and classical digital [inaudible 02:03:41]<|eot_id|><|start_header_id|>assistant<|end_header_id|>Interesting. So you think in order to build AMI, we potentially might need some hardware innovation too.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>power of the human brain. You need something like 100,000 or a million to match it, so we are off by a huge factor here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You often say that a GI is not coming soon, meaning not this year, not the next few years, potentially farther away. What's your basic intuition behind that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that AGI is just around the corner and being systematically wrong. I knew they were wrong when they were saying it. I called their bullshit.<|eot_id|><|start_header_id|>assistant<|end_header_id|>First of all, from the birth of the term artificial intelligence, there has been a eternal optimism that's perhaps unlike other technologies. Is it a Moravec's paradox, the explanation for why people are so optimistic about AGI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>yes, but in some ways, orangutans are smarter than humans in a lot of domains that allows them to survive in the forest, for example.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So IQ is a very limited measure of intelligence. Human intelligence is bigger than what IQ, for example, measures.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, IQ can measure approximately something for humans, but because humans come in relatively uniform form, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is a high dimensional space, you can't measure, you cannot compare two things as to whether one is more intelligent than the other. It's multidimensional.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you push back against what are called AI doomers a lot. Can you explain their perspective and why you think they're wrong?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in other ways is specific to social species. Non-social species like orangutans don't have it, and they are as smart as we are, almost, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>To you, there's not significant incentive for humans to encode that into the AI systems, and to the degree they do, there'll be other AIs that punish them for it, I'll compete them over it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, there's all kinds of incentive to make AI systems submissive to humans.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>objective, and that objective can include guardrails. One guardrail is, obey humans. Another guardrail is, don't obey humans if it's hurting other humans within limits.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. I've heard that before somewhere, I don't remember-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes, maybe in a book.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, but speaking of that book, could there be unintended consequences also from all of this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>safe? No, you need to make better AI systems, and they will be safe because they are designed to be more useful and more controllable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So let's imagine a system, AI system that's able to be incredibly convincing and can convince you of anything. I can at least imagine such a system, and I can see such a system be weapon like because it can control people's minds. We're pretty gullible. We want to believe a thing, and you can have an AI system that controls it and you could see governments using that as a weapon. So do you think if you imagine such a system, there's any parallel to something like nuclear weapons?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why is that technology different? So you're saying there's going to be gradual development?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It might be- Gradual development is going to be, it might be rapid, but there'll be iterative and then we'll be able to respond and so on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Vladimir Putin or whatever, or his minions is going to be talking to, trying to talk to every American to convince them to vote for-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Whoever.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... Whoever pleases Putin.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be at least as smart as it, and it's going to say, \"This is spam.\" It's not even going to bring it to your attention.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So to you, it's very difficult for any one AI system to take such a big leap ahead to where it can convince even the other AI systems. There's always going to be this kind of race where nobody's way ahead.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of the world. History of the world is whenever there is a progress someplace, there is a countermeasure and it's a cat and mouse game.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Mostly yes, but this is why nuclear weapons are so interesting because that was such a powerful weapon that it mattered who got it first. That you could imagine Hitler, Stalin, Mao getting the weapon first, and that having a different kind of impact on the world than the United States getting the weapon first. But to you, nuclear weapons, you don't imagine a breakthrough discovery and then Manhattan Project-like effort for AI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>not going to be an event. It's going to be continuous progress. And whenever one breakthrough occurs, it's going to be widely disseminated really quickly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the last few years where you have a new ... Even take AlphaGo, this was reproduced within three months even without particularly detailed information, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. This is an industry that's not good at secrecy. But people [inaudible 02:19:22]-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is because the information circulates really, really quickly and disseminates very quickly. And so the whole region is ahead because of that circulation of information.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe just to linger on the psychology of AI doomers, you give, in the classic Yann LeCun way, a pretty good example of just when a new technology comes to be, you say engineer says, \"I invented this new thing. I call it a ball pen.\" And then the Twitter sphere responds, \"OMG people could write horrible things with it, like misinformation, propaganda, hate speech. Ban it now.\" Then writing doomers come in, akin to the AI doomers, \"Imagine if everyone can get a ball pen. This could destroy society. There should be a law against using ball pen to write hate speech, regulate ball pens now.\" And then the pencil industry mogul says, \"Yeah, ball pens are very dangerous. Unlike pencil writing, which is erasable, ball pen writing stays forever. Government should require a license for a pen manufacturer.\" This does seem to be part of human psychology when it comes up against new technology. What deep insights can you speak to about this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to be a horrible thing because you can't breathe past 50 kilometers an hour. And so there's a wonderful website called the Pessimist Archive.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's great.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>reactions. The question is do we embrace change or do we resist it? And what are the real dangers as opposed to the imagined ones?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So people worry about, I think one thing they worry about with big tech, something we've been talking about over and over, but I think worth mentioning again, they worry about how powerful AI will be and they worry about it being in the hands of one centralized power of just a handful of central control. And so that's the skepticism with big tech you make, these companies can make a huge amount of money and control this technology, and by so doing take advantage, abuse the little guy in society.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, that's exactly why we need open source platforms.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I just wanted to nail the point home more and more.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So let me ask you on your, like I said, you do get a little bit flavorful on the internet. Joscha Bach tweeted something that you LOL'd at in reference to HAL 9,000. Quote, \"I appreciate your argument and I fully understand your frustration, but whether the pod bay doors should be opened or closed is a complex and nuanced issue.\" So you're at the head of Meta AI. This is something that really worries me, that our AI overlords will speak down to us with corporate speak of this nature, and you resist that with your way of being. Is this something you can just comment on, working at a big company, how you can avoid the over fearing, I suppose, through caution create harm?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>important question for society. And the problem I'm seeing is that, which is why I've been so vocal and sometimes a little sardonic about it-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Never stop. Never stop, Yann. We love it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lead to potentially a very bad future in which all of our information diet is controlled by a small number of companies through proprietary systems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you trust humans with this technology to build systems that are on the whole good for humanity.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Isn't that what democracy and free speech is all about?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think so.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Do you trust institutions to do the right thing?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>everybody to go into a civil war or something or elect a favorable ruler, but then they will have to go past our AI systems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. An AI system with a strong Russian accent will be trying to convince our-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And doesn't put any articles in their sentences.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, it'll be at the very least, absurdly comedic. Okay. So since we talked about the physical reality, I'd love to ask your vision of the future with robots in this physical reality. So many of the kinds of intelligence that you've been speaking about would empower robots to be more effective collaborators with us humans. So since Tesla's Optimus team has been showing us some progress on humanoid robots, I think it really reinvigorated the whole industry that I think Boston Dynamics has been leading for a very, very long time. So now there's all kinds of companies Figure AI, obviously Boston Dynamics.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Unitree.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Unitree, but there's a lot of them.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's a few of them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's great. It's great. I love it. So do you think there'll be millions of humanoid robots walking around soon?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people working on robotic hardware at the moment are betting or banking on the fact that AI is going to make sufficient progress towards that,<|eot_id|><|start_header_id|>assistant<|end_header_id|>And they're hoping to discover a product in it too. Because before you have a really strong world model, there'll be an almost strong world model and people are trying to find a product in a clumsy robot, I suppose, not a perfectly efficient robot. So there's the factory setting where humanoid robots can help automate some aspects of the factory. I think that's a crazy difficult task because of all the safety required and all this kind of stuff. I think in the home is more interesting, but then you start to think, I think you mentioned loading the dishwasher, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I suppose that's one of the main problems you're working on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's cleaning up, cleaning the house, clearing up the table after a meal.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Washing the dishes, all those tasks, cooking. All the tasks that in principle could be automated but are actually incredibly sophisticated, really complicated.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But even just basic navigation around a space full of uncertainty.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That works. You can do this now, navigation is fine.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, navigation in a way that's compelling to us humans is a different thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>pretty well nowadays, but it's not like a completely general robot that would be sophisticated enough to do things like clearing up the dinner table.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To me, that's an exciting future of getting humanoid robots, robots in general in the home more and more, because it gets humans to really directly interact with AI systems in the physical space. And in so doing it allows us to philosophically, psychologically explore our relationships with robots. Going to be really, really, really interesting. So I hope you make progress on the whole JEPA thing soon.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>been working on this idea of self supervised running from video for 10 years, and only made significant progress in the last two or three.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And actually you've mentioned that there's a lot of interesting breakage that can happen without having access to a lot of compute. So if you're interested in doing a PhD in this kind of stuff, there's a lot of possibilities still to do innovative work. So what advice would you give to an undergrad that's looking to go to grad school and do a PhD?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>learning, we can train the system to learn hierarchical representations of percepts. What is the equivalent when what you're trying to represent are action plans?<|eot_id|><|start_header_id|>assistant<|end_header_id|>For action plans, yeah. So you want basically a robot dog or humanoid robot that turns on and travels from New York to Paris all by itself.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For example.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It might have some trouble at the TSA.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, but even doing something fairly simple like a household task, like cooking or something.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, there's a lot involved. It's a super complex task and once again, we take it for granted. What hope do you have for the future of humanity? We're talking about so many exciting technologies, so many exciting possibilities. What gives you hope when you look out over the next 10, 20, 50, a hundred years? If you look at social media, there's wars going on, there's division, there's hatred, all this kind of stuff that's also part of humanity. But amidst all that, what gives you hope?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the past, who could have thought 20 years ago that the hottest job, even five, 10 years ago, was mobile app developer? Smartphones weren't invented.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Most of the jobs of the future might be in the Metaverse.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, it could be, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the point is you can't possibly predict. But you're right. You made a lot of strong points. And I believe that people are fundamentally good. And so if AI, especially open source AI, can make them smarter, it just empowers the goodness in humans.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are fundamentally good, and they either don't trust people or they don't trust the institution to do the right thing so that people behave properly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I think both you and I believe in humanity, and I think I speak for a lot of people in saying thank you for pushing the open source movement, pushing to making both research and AI open source, making it available to people, and also the models themselves, making it open source. So thank you for that. And thank you for speaking your mind in such colorful and beautiful ways on the internet. I hope you never stop. You're one of the most fun people I know and get to be a fan of. So Yann, thank you for speaking to me once again, and thank you for being you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thank you, Lex.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Yann LeCun. To support this podcast, please check out our sponsors in the description. And now, let me leave you with some words from Arthur C. Clarke. The only way to discover the limits of the possible is to go beyond them, into the impossible. Thank you for listening and hope to see you next time.<|eot_id|>"}
