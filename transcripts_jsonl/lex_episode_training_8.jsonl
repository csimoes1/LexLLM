{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>compelling reasons why this will not happen in the next few years. The scale-up is very quick. We do this today, we make a model, and then we deploy thousands, maybe tens of thousands of instances of it. I think by the time, certainly within two to three years, whether we have these super powerful AIs or not, clusters are going to get to the size where you'll be able to deploy millions of these. I am optimistic about meaning. I worry about economics and the concentration of power. That's actually what I worry about more, the abuse of power.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And AI increases the amount of power in the world. And if you concentrate that power and abuse that power, it can do immeasurable damage.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes, it's very frightening. It's very frightening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with Dario Amodei, CEO of Anthropic, the company that created Claude, that is currently and often at the top of most LLM benchmark leader boards. On top of that, Dario and the Anthropic team have been outspoken advocates for taking the topic of AI safety very seriously. And they have continued to publish a lot of fascinating AI on this and other topics. I'm also joined afterwards by two other brilliant people from Anthropic. First Amanda Askell, who is a researcher working on alignment and fine-tuning of Claude, including the design of Claude's character and personality. A few folks told me she has probably talked with Claude more than any human at Anthropic. So she was definitely a fascinating person to talk to about prompt engineering and practical advice on how to get the best out of Claude. After that, Chris Olah stopped by for a chat. He's one of the pioneers of the field of mechanistic interpretability, which is an exciting set of efforts that aims to reverse engineering neural networks, to figure out what's going on inside, inferring behaviors from neural activation patterns inside the network. This is a very promising approach for keeping future super-intelligent AI systems safe. For example, by detecting from the activations when the model is trying to deceive the human it is talking to. This is the Lex Fridman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Dario Amodei. Let's start with a big idea of scaling laws and the scaling hypothesis. What is it? What is its history, and where do we stand today?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to either find a way around or scaling just is the way around. Sometimes it's one, sometimes it's the other. And so I'm now at this point, I still think it's always quite uncertain. We have nothing but inductive inference to tell us that the next two years are going to be like the last 10 years. But I've seen the movie enough times, I've seen the story happen for enough times to really believe that probably the scaling is going to continue, and that there's some magic to it that we haven't really explained on a theoretical basis yet.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And of course the scaling here is bigger networks, bigger data, bigger compute?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All of those?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>In particular, linear scaling up of bigger networks, bigger training times and more and more data. So all of these things, almost like a chemical reaction, you have three ingredients in the chemical reaction and you need to linearly scale up the three ingredients. If you scale up one, not the others, you run out of the other reagents and the reaction stops. But if you scale up everything in series, then the reaction can proceed.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And of course now that you have this kind of empirical science/art, you can apply it to other more nuanced things like scaling laws applied to interpretability or scaling laws applied to post-training. Or just seeing how does this thing scale. But the big scaling law, I guess the underlying scaling hypothesis has to do with big networks, big data leads to intelligence?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, we've documented scaling laws in lots of domains other than language. So initially the paper we did that first showed it, was in early 2020, where we first showed it for language. There was then some work late in 2020 where we showed the same thing for other modalities like images, video, text to image, image to text, math. They all had the same pattern. And you're right, now there are other stages like post-training or there are new types of reasoning models. And in all of those cases that we've measured, we see similar types of scaling laws.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A bit of a philosophical question, but what's your intuition about why bigger is better in terms of network size and data size? Why does it lead to more intelligent models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>more and more of that distribution. And so that smoothness gets reflected in how well the models are at predicting and how well they perform. Language is an evolved process. We've developed language, we have common words and less common words. We have common expressions and less common expressions. We have ideas, cliches, that are expressed frequently, and we have novel ideas. And that process has developed, has evolved with humans over millions of years. And so the guess, and this is pure speculation, would be that there's some kind of long tail distribution of the distribution of these ideas.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's the long tail, but also there's the height of the hierarchy of concepts that you're building up. So the bigger the network, presumably you have a higher capacity to-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly. If you have a small network, you only get the common stuff. If I take a tiny neural network, it's very good at understanding that a sentence has to have verb, adjective, noun, but it's terrible at deciding what those verb adjective and noun should be and whether they should make sense. If I make it just a little bigger, it gets good at that, then suddenly it's good at the sentences, but it's not good at the paragraphs. And so these rarer and more complex patterns get picked up as I add more capacity to the network.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, the natural question then is what's the ceiling of this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How complicated and complex is the real world? How much is the stuff is there to learn?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>may be there's only some of these problems are not intractable, but much harder. And it may be that there's only so well you can do at some of these things. Just like with speech recognition, there's only so clear I can hear your speech. So I think in some areas there may be ceilings that are very close to what humans have done. In other areas, those ceilings may be very far away. I think we'll only find out when we build these systems. It's very hard to know in advance. We can speculate, but we can't be sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And in some domains, the ceiling might have to do with human bureaucracies and things like this, as you write about.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So humans fundamentally has to be part of the loop. That's the cause of the ceiling, not maybe the limits of the intelligence.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the whole challenge is that it's hard to tell what's going on. It's hard to tell which is which. I think in terms of drug development, my view is that we're too slow and we're too conservative. But certainly if you get these things wrong, it's possible to risk people's lives by being too reckless. And so at least some of these human institutions are in fact protecting people. So it's all about finding the balance. I strongly suspect that balance is kind of more on the side of wishing to make things happen faster, but there is a balance.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If we do hit a limit, if we do hit a slowdown in the scaling laws, what do you think would be the reason? Is it compute-limited, data-limited? Is it something else? Idea limited?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they've gotten better, that could just stop at some point for a reason we don't understand. The answer could be that we need to invent some new architecture. There have been problems in the past with say, numerical stability of models where it looked like things were leveling off, but actually when we found the right unblocker, they didn't end up doing so. So perhaps there's some new optimization method or some new technique we need to unblock things. I've seen no evidence of that so far, but if things were to slow down, that perhaps could be one reason.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about the limits of compute, meaning the expensive nature of building bigger and bigger data centers?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>know, but might even be less than that. We've seen similar things in graduate-level math, physics, and biology from models like OpenAi's o1. So if we just continue to extrapolate this in terms of skill that we have, I think if we extrapolate the straight curve, within a few years, we will get to these models being above the highest professional level in terms of humans. Now, will that curve continue? You've pointed to, and I've pointed to a lot of possible reasons why that might not happen. But if the extrapolation curve continues, that is the trajectory we're on.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So Anthropic has several competitors. It'd be interesting to get your sort of view of it all. OpenAI, Google, XAI, Meta. What does it take to win in the broad sense of win in this space?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>like, \"Oh, now others are doing it as well.\" But it's good for the broader system, and so we have to invent some new thing that we're doing that others aren't doing as well. And the hope is to basically bid up the importance of doing the right thing. And it's not about us in particular. It's not about having one particular good guy. Other companies can do this as well. If they join the race to do this, that's the best news ever. It's about shaping the incentives to point upward instead of shaping the incentives to point downward.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we should say this example of the field of mechanistic interpretability is just a rigorous non-hand wavy wave doing AI safety-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... or it's tending that way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it feels like there's some law that's driving these models to perform better, on the inside, the models aren't... There's no reason why they should be designed for us to understand them, right? They're designed to operate, they're designed to work. Just like the human brain or human biochemistry. They're not designed for a human to open up the hatch, look inside and understand them. But we have found, and you can talk in much more detail about this to Chris, that when we open them up, when we do look inside them, we find things that are surprisingly interesting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And as a side effect, you also get to see the beauty of these models. You get to explore the beautiful nature of large neural networks through the MEC and TERP kind of methodology.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to the Golden Gate Bridge. And we just turned that way up. And so we released this model as a demo, it was kind of half a joke, for a couple days, but it was illustrative of the method we developed. And you could take the model, you could ask it about anything. It would be like you could say, \"How was your day?\" And anything you asked, because this feature was activated, it would connect to the Golden Gate Bridge. So it would say, I'm feeling relaxed and expansive, much like the arches of the Golden Gate Bridge, or-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It would masterfully change topic to the Golden Gate Bridge and integrate it. There was also a sadness to the focus it had on the Golden Gate Bridge. I think people quickly fell in love with it, I think. So people already miss it, because it was taken down, I think after a day.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Somehow these interventions on the model, where you kind of adjust its behavior, somehow emotionally made it seem more human than any other version of the model.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a strong personality, strong identity.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It has a strong personality. It has these kind of obsessive interests. We can all think of someone who's obsessed with something. So it does make it feel somehow a bit more human.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's talk about the present. Let's talk about Claude. So this year, a lot has happened. In March. Claude 3 Opus, Sonnet, Haiku were released. Then Claude 3.5 Sonnet in July, with an updated version just now released. And then also Claude 3.5 Haiku was released. Okay. Can you explain the difference between Opus, Sonnet and Haiku, and how we should think about the different versions?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then at some point there's going to be an Opus 3.5. Now every new generation of models has its own thing. They use new data, their personality changes in ways that we try to steer but are not fully able to steer. And so there's never quite that exact equivalence, where the only thing you're changing is intelligence. We always try and improve other things and some things change without us knowing or measuring. So it's very much an inexact science. In many ways, the manner and personality of these models is more an art than it is a science.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what is the reason for the span of time between say, Claude Opus 3.0 and 3.5? What takes that time, if you can speak to it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>actually making a model work. And of course, we're always trying to make the processes as streamlined as possible. We want our safety testing to be rigorous, but we want it to be rigorous and to be automatic, to happen as fast as it can, without compromising on rigor. Same with our pre-training process and our post-training process. So it's just building anything else. It's just like building airplanes. You want to make them safe, but you want to make the process streamlined. And I think the creative tension between those is an important thing in making the models work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, rumor on the street, I forget who was saying that, Anthropic has really good tooling. So probably a lot of the challenge here is, on the software engineering side, is to build the tooling to have a efficient, low-friction interaction with the infrastructure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>how much of the challenges of building these models comes down to software engineering, performance engineering. From the outside, you might think, \"Oh man, we had this Eureka breakthrough.\" You know, this movie with the science. \"We discovered it, we figured it out.\" But I think all things, even incredible discoveries, they almost always come down to the details. And often super, super boring details. I can't speak to whether we have better tooling than other companies. I mean, haven't been at those other companies, at least not recently, but it's certainly something we give a lot of attention to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know if you can say, but from Claude 3 to Claude 3.5, is there any extra pre-training going on, or is it mostly focused on the post-training? There's been leaps in performance.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I think at any given stage, we're focused on improving everything at once. Just naturally. Like, there are different teams. Each team makes progress in a particular area, in making their particular segment of the relay race better. And it's just natural that when we make a new model, we put all of these things in at once.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the data you have, the preference data you get from RLHF, is there ways to apply it to newer models as it get trained up?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Preference data from old models sometimes gets used for new models, although of course it performs somewhat better when it's trained on the new models. Note that we have this constitutional AI method such that we don't only use preference data, there's also a post-training process where we train the model against itself. And there's new types of post-training the model against itself that are used every day. So it's not just RLHF, a bunch of other methods as well. Post-training, I think, is becoming more and more sophisticated.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, what explains the big leap in performance for the new Sonnet 3.5, I mean, at least in the programming side? And maybe this is a good place to talk about benchmarks. What does it mean to get better? Just the number went up, but I program, but I also love programming, and I Claude 3.5 through Cursor is what I use to assist me in programming. And there was, at least experientially, anecdotally, it's gotten smarter at programming. So what does it take to get it smarter?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We-<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what does it take to get it smarter?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>able to complete these tasks?\" And it's that benchmark that's gone from \"it can do it 3% of the time\" to \"it can do it about 50% of the time.\" So I actually do believe that you can gain benchmarks, but I think if we get to 100% on that benchmark in a way that isn't over-trained or game for that particular benchmark, probably represents a real and serious increase in programming ability. And I would suspect that if we can get to 90, 95% that it will represent ability to autonomously do a significant fraction of software engineering tasks.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, ridiculous timeline question. When is Claude Opus 3.5 coming up?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not giving you an exact date, but as far as we know, the plan is still to have a Claude 3.5 Opus.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are we going to get it before GTA 6 or no?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Like Duke Nukem Forever?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Duke Nukem. Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>What was that game? There was some game that was delayed 15 years.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Was that Duke Nukem Forever?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. And I think GTA is now just releasing trailers.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's only been three months since we released the first Sonnet.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's the incredible pace of release.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It just tells you about the pace, the expectations for when things are going to come out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what about 4.0? So how do you think, as these models get bigger and bigger, about versioning and also just versioning in general, why Sonnet 3.5 updated with the date? Why not Sonnet 3.6, which a lot of people are calling it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the reality tends to frustrate that scheme, right? It tends to break out of the scheme. It's not like software where you can say, \"Oh, this is 3.7, this is 3.8.\" No, you have models with different trade-offs. You can change some things in your models, you can change other things. Some are faster and slower at inference. Some have to be more expensive, some have to be less expensive. And so I think all the companies have struggled with this. I think we were in a good position in terms of naming when we had Haiku, Sonnet and Opus.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It was great, great start.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We're trying to maintain it, but it's not perfect, so we'll try and get back to the simplicity. But just the nature of the field, I feel like no one's figured out naming. It's somehow a different paradigm from normal software and so none of the companies have been perfect at it. It's something we struggle with surprisingly much relative to how trivial it is for the grand science of training the models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So from the user side, the user experience of the updated Sonnet 3.5 is just different than the previous June 2024 Sonnet 3.5. It would be nice to come up with some kind of labeling that embodies that. Because people talk about Sonnet 3.5, but now there's a different one. And so how do you refer to the previous one and the new one when there's a distinct improvement? It just makes conversation about it just challenging.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and there are some behaviors you might not see just like with a human, right? I can know someone for a few months and not know that they have a certain skill or not know that there's a certain side to them. And so I think we just have to get used to this idea. And we're always looking for better ways of testing our models to demonstrate these capabilities and also to decide which are the personality properties we want models to have and which we don't want to have. That itself, the normative question, is also super interesting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I got to ask you a question from Reddit.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>From Reddit? Oh, boy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's just this fascinating, to me at least, it's a psychological social phenomenon where people report that Claude has gotten dumber for them over time. And so the question is, does the user complaint about the dumbing down of Claude 3.5 Sonnet hold any water? So are these anecdotal reports a kind of social phenomena or is there any cases where Claude would get dumber?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in a certain way and I slightly changed the phrasing of how I talk to the model, I could get different results. So that's one possible way. The other thing is, man, it's just hard to quantify this stuff. It's hard to quantify this stuff. I think people are very excited by new models when they come out and then as time goes on, they become very aware of their limitations. So that may be another effect, but that's all a very long-winded way of saying for the most part, with some fairly narrow exceptions, the models are not changing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think there is a psychological effect. You just start getting used to it, the baseline raises. When people who have first gotten Wi-Fi on airplanes, it's amazing, magic.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's amazing. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then you start-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And now I'm like, \"I can't get this thing to work. This is such a piece of crap.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly. So it's easy to have the conspiracy theory of, \"They're making Wi-Fi slower and slower.\" This is probably something I'll talk to Amanda much more about, but another Reddit question, \"When will Claude stop trying to be my pure tentacle grandmother imposing its moral worldview on me as a paying customer? And also, what is the psychology behind making Claude overly apologetic?\" So this reports about the experience, a different angle on the frustration. It has to do with the character [inaudible 00:47:06].<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>done the best of all the AI companies, but still so far from perfect. And I think if we can get this right, if we can control the false positives and false negatives in this very controlled present day environment, we'll be much better at doing it for the future when our worry is: will the models be super autonomous? Will they be able to make very dangerous things? Will they be able to autonomously build whole companies and are those companies aligned? So I think of this present task as both vexing but also good practice for the future.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the current best way of gathering user feedback? Not anecdotal data, but just large-scale data about pain points or the opposite of pain points, positive things, so on? Is it internal testing? Is it a specific group testing, A/B testing? What works?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>model from doing genuinely bad things that everyone agrees it shouldn't do, everyone agrees that the model shouldn't talk about, I don't know, child abuse material. Everyone agrees the model shouldn't do that, but at the same time, that it doesn't refuse in these dumb and stupid ways. I think drawing that line as finely as possible, approaching perfectly, is still a challenge and we're getting better at it every day, but there's a lot to be solved. And again, I would point to that as an indicator of a challenge ahead in terms of steering much more powerful models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think Claude 4.0 is ever coming out?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't want to commit to any naming scheme because if I say here, \"We're going to have Claude 4 next year,\" and then we decide that we should start over because there's a new type of model, I don't want to commit to it. I would expect in a normal course of business that Claude 4 would come after Claude 3. 5, but you never know in this wacky field.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But this idea of scaling is continuing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Scaling is continuing. There will definitely be more powerful models coming from us than the models that exist today. That is certain. Or if there aren't, we've deeply failed as a company.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Can you explain the responsible scaling policy and the AI safety level standards, ASL levels?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the other side, you get other behaviors. If you fix one thing, it creates other problems. We're getting better and better at solving this. I don't think this is an unsolvable problem. I think this is a science like the safety of airplanes or the safety of cars or the safety of drugs. I don't think there's any big thing we're missing. I just think we need to get better at controlling these models. And so these are the two risks I'm worried about. And our responsible scaling plan, which I'll recognize is a very long-winded answer to your question.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love it. I love it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>enough. So ASL-3 is going to be the point at which the models are helpful enough to enhance the capabilities of non-state actors, right? State actors can already do, unfortunately, to a high level of proficiency, a lot of these very dangerous and destructive things. The difference is that non-state actors are not capable of it. And so when we get to ASL-3, we'll take special security precautions designed to be sufficient to prevent theft of the model by non-state actors and misuse of the model as it's deployed. We'll have to have enhanced filters targeted at these particular areas.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Cyber, bio, nuclear.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of course, what has to come with that is enough of a buffer threshold that you're not at high risk of missing the danger. It's not a perfect framework. We've had to change it. We came out with a new one just a few weeks ago and probably going forward, we might release new ones multiple times a year because it's hard to get these policies right technically, organizationally from a research perspective. But that is the proposal, if-then commitments and triggers in order to minimize burdens and false alarms now, but really react appropriately when the dangers are here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think the timeline for ASL-3 is where several of the triggers are fired? And what do you think the timeline is for ASL-4?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>debated within the company. We are working actively to prepare ASL-3 security measures as well as ASL-3 deployment measures. I'm not going to go into detail, but we've made a lot of progress on both and we're prepared to be, I think, ready quite soon. I would not be surprised at all if we hit ASL-3 next year. There was some concern that we might even hit it this year. That's still possible. That could still happen. It's very hard to say, but I would be very, very surprised if it was 2030. I think it's much sooner than that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's protocols for detecting it, the if-then and then there's protocols for how to respond to it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How difficult is the second, the latter?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hidden chains of thought where you have to look inside the model and verify via some other mechanism that is not as easily corrupted as what the model says, that the model indeed has some property. So we're still working on ASL-4. One of the properties of the RSP is that we don't specify ASL-4 until we've hit ASL-3. And I think that's proven to be a wise decision because even with ASL-3, again, it's hard to know this stuff in detail, and we want to take as much time as we can possibly take to get these things right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So for ASL-3, the bad actor will be the humans.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Humans, yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And so there's a little bit more...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For ASL- 4, it's both, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's both. And so deception, and that's where mechanistic interpretability comes into play, and hopefully the techniques used for that are not made accessible to the model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the model itself, but then you've lost it as a reliable indicator of the model state. There are a bunch of exotic ways you can think of that it might also not be reliable, like if the model gets smart enough that it can jump computers and read the code where you're looking at its internal state. We've thought about some of those. I think they're exotic enough. There are ways to render them unlikely. But yeah, generally, you want to preserve mechanistic interpretability as a verification set or test set that's separate from the training process of the model.<|eot_id|><|start_header_id|>assistant<|end_header_id|>See, I think as these models become better and better conversation and become smarter, social engineer becomes a threat too because they could start being very convincing to the engineers inside companies.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, yeah. Yeah. We've seen lots of examples of demagoguery in our life from humans, and there's a concern that models could do that as well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One of the ways that Claude has been getting more and more powerful is it's now able to do some agentic stuff, computer use. There's also an analysis within the sandbox of Claude.ai itself. But let's talk about computer use. That seems to me super exciting that you can just give Claude a task and it takes a bunch of actions, figures it out, and has access to the... ... a bunch of actions, figures it out and has access to your computer through screenshots. So can you explain how that works and where that's headed?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>doing that. I think since it's been released, a number of customers, I think Replit was maybe one of the most quickest to deploy things, have made use of it in various ways. People have hooked up demos for Windows desktops, Macs, Linux machines. So yeah, it's been very exciting. I think as with anything else, it comes with new exciting abilities and then with those new exciting abilities, we have to think about how to make the model safe, reliable, do what humans want them to do. It's the same story for everything. Same thing. It's that same tension.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the possibility of use cases here, just the range is incredible. So how much to make it work really well in the future? How much do you have to specially kind of go beyond what the pre-trained model is doing, do more post-training, RLHF or supervised fine-tuning or synthetic data just for the agentive stuff?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>intention to keep investing a lot in making the model better. I think we look at some of the benchmarks where previous models were like, \"Oh, could do it 6% of the time,\" and now our model would do it 14 or 22% of the time. And yeah, we want to get up to the human level reliability of 80, 90% just like anywhere else. We're on the same curve that we were on with SWE-bench where I think I would guess a year from now, the models can do this very, very reliably. But you got to start somewhere.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you think it's possible to get to the human level 90% basically doing the same thing you're doing now or it has to be special for computer use?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It depends what you mean by special and special in general, but I generally think the same kinds of techniques that we've been using to train the current model, I expect that doubling down on those techniques in the same way that we have for code, for models in general, for image input, for voice, I expect those same techniques will scale here as they have everywhere else,<|eot_id|><|start_header_id|>assistant<|end_header_id|>But this is giving the power of action to Claude and so you could do a lot of really powerful things, but you could do a lot of damage also.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is nothing that this model is doing inherently increases the risk from an RSP perspective, but as the models get more powerful, having this capability may make it scarier once it has the cognitive capability to do something at the ASL-3 and ASL-4 level, this may be the thing that kind of unbounds it from doing so. So going forward, certainly this modality of interaction is something we have tested for and that we will continue to test for an RSP going forward. I think it's probably better to learn and explore this capability before the model is super capable<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. And there's a lot of interesting attacks like prompt injection because now you've widened the aperture so you can prompt inject through stuff on screen. So if this becomes more and more useful, then there's more and more benefit to inject stuff into the model. If it goes to certain web page, it could be harmless stuff like advertisements or it could be harmful stuff, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, we've thought a lot about things like spam, CAPTCHA, mass... One secret, I'll tell you, if you've invented a new technology, not necessarily the biggest misuse, but the first misuse you'll see, scams, just petty scams.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's like a thing as old, people scamming each other, it's this thing as old as time. And it's just every time, you got to deal with it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's almost silly to say, but it's true, sort of bots and spam in general is a thing as it gets more and more intelligent-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... it's harder and harder to fight it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Like I said, there are a lot of petty criminals in the world and it's like every new technology is a new way for petty criminals to do something stupid and malicious.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there any ideas about sandboxing it? How difficult is the sandboxing task?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>my computer or my web server to anywhere else.\" Now, when you talk about sandboxing, again, when we get to ASL-4, none of these precautions are going to make sense there. When you talk about ASL-4, you're then, the model is being, there's theoretical worry the model could be smart enough to kind of break it to out of any box. And so there, we need to think about mechanistic interpretability. If we're going to have a sandbox, it would need to be a mathematically provable. That's a whole different world than what we're dealing with with the models today.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, the science of building a box from which ASL-4 AI system cannot escape.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's probably not the right approach. I think the right approach, instead of having something unaligned that you're trying to prevent it from escaping, I think it's better to just design the model the right way or have a loop where you look inside the model and you're able to verify properties and that gives you an opportunity to tell, iterate and actually get it right. I think containing bad models is a much worse solution than having good models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let me ask about regulation. What's the role of regulation in keeping AI safe? So for example, can you describe California AI regulation bill SB 1047 that was ultimately vetoed by the governor? What are the pros and cons of this bill in general?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and that the opponents feel that it is not hampering the industry or hampering innovation any more necessary than it needs to. I think for whatever reason, that things got too polarized and those two groups didn't get to sit down in the way that they should. And I feel urgency. I really think we need to do something in 2025. If we get to the end of 2025 and we've still done nothing about this, then I'm going to be worried. I'm not worried yet because, again, the risks aren't here yet, but I think time is running short.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And come up with something surgical, like you said.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah, yeah, exactly. And we need to get away from this intense pro safety versus intense anti-regulatory rhetoric. It's turned into these flame wars on Twitter and nothing good's going to come of that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's a lot of curiosity about the different players in the game. One of the OGs is OpenAI. You've had several years of experience at OpenAI. What's your story and history there?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>said to me, \"The thing you need to understand about these models is they just want to learn. The models just want to learn.\" And again, sometimes there are these one sentences, these then cones, that you hear them and you're like, \"Ah, that explains everything. That explains a thousand things that I've seen.\" And then ever after, I had this visualization in my head of you optimize the models in the right way, you point the models in the right way, they just want to learn. They just want to solve the problem regardless of what the problem is.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So get out of their way, basically?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Get out of their way. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>got this kind of inspiration from Ilya and from others, folks like Alec Radford, who did the original GPT-1 and then ran really hard with it, me and my collaborators, on GPT-2, GPT-3, RL from Human Feedback, which was an attempt to kind of deal with the early safety and durability, things like debate and amplification, heavy on interpretability. So again, the combination of safety plus scaling. Probably 2018, 2019, 2020, those were kind of the years when myself and my collaborators, probably many of whom became co-founders of Anthropic, kind of really had a vision and drove the direction.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why'd you leave? Why'd you decide to leave?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We should adopt it too.\" The only difference is I think we try to be more forward leaning. We try and adopt more of these practices first and adopt them more quickly when others invent them. But I think this dynamic is what we should be pointing at and that I think it abstracts away the question of which company's winning, who trusts who. I think all these questions of drama are profoundly uninteresting and the thing that matters is the ecosystem that we all operate in and how to make that ecosystem better because that constrains all the players.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And so Anthropic is this kind of clean experiment built on a foundation of what concretely AI safety should look like?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>up. There's better and there's worse. And hopefully, we can do well enough that we can begin to build some practices that the whole industry engages in. And then my guess is that multiple of these companies will be successful. Anthropic will be successful. These other companies, like ones I've been at the past, will also be successful. And some will be more successful than others. That's less important than, again, that we align the incentives of the industry. And that happens partly through the race to the top, partly through things like RSP, partly through, again, selected surgical regulation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You said talent density beats talent mass, so can you explain that? Can you expand on that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you just talk about what it takes to build a great team of AI researchers and engineers?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be very selective. It's very easy to go from a hundred to a thousand, a thousand to 10,000 without paying attention to making sure everyone has a unified purpose. It's so powerful. If your company consists of a lot of different fiefdoms that all want to do their own thing, they're all optimizing for their own thing, it's very hard to get anything done. But if everyone sees the broader purpose of the company, if there's trust and there's dedication to doing the right thing, that is a superpower. That in itself I think can overcome almost every other disadvantage.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And to Steve Jobs, A players. A players want to look around and see other A players is another way of saying that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know what that is about human nature, but it is demotivating to see people who are not obsessively driving towards a singular mission. And it is on the flip side of that, super motivating to see that. It's interesting. What's it take to be a great AI researcher or engineer from everything you've seen from working with so many amazing people?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>forward the whole field by realizing this. And it's often like that. If you look back at the discoveries in history, they're often like that. And so this open-mindedness and this willingness to see with new eyes that often comes from being newer to the field, often experience is a disadvantage for this, that is the most important thing. It's very hard to look for and test for, but I think it's the most important thing because when you find something, some really new way of thinking about things, when you have the initiative to do that, it's absolutely transformative.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And also be able to do kind of rapid experimentation and, in the face of that, be open-minded and curious and looking at the data with these fresh eyes and seeing what is it that it's actually saying. That applies in mechanistic interpretability.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's another example of this. Some of the early work and mechanistic interpretability so simple, it's just no one thought to care about this question before.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You said what it takes to be a great AI researcher. Can we rewind the clock back, what advice would you give to people interested in AI? They're young, looking... What advice would you give to people interested in AI? They're young. Looking forward to how can I make an impact on the world?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>particularly for dynamic systems acting in the world. I think there's some stuff around multi-agent. Skate where the puck is going is my advice, and you don't have to be brilliant to think of it. All the things that are going to be exciting in five years, people even mention them as conventional wisdom, but it's just somehow there's this barrier that people don't double down as much as they could, or they're afraid to do something that's not the popular thing. I don't know why it happens, but getting over that barrier, that's my number one piece of advice.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's talk if we could a bit about post-training. So it seems that the modern post-training recipe has a little bit of everything. So supervised fine-tuning, RLHF, the constitutional AI with RLAIF-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Best acronym.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the, again, that naming thing. And then synthetic data. Seems like a lot of synthetic data, or at least trying to figure out ways to have high quality synthetic data. So if this is a secret sauce that makes Anthropic clause so incredible, how much of the magic is in the pre-training? How much of it is in the post-training?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>able to combine these methods and practice.\" It's usually some boring matter of practice and trade craft. So when I think about how to do something special in terms of how we train these models both, but even more so I really think of it a little more, again, as designing airplanes or cars. It's not just like, \"Oh, man. I have the blueprint.\" Maybe that makes you make the next airplane. But there's some cultural trade craft of how we think about the design process that I think is more important than any particular gizmo we're able to invent.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Well, let me ask you about specific techniques. So first on RLHF, what do you think, just zooming out intuition, almost philosophy ... Why do you think RLHF works so well?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the model wants and what humans want in the moment may not be what they want in the long term. So there's a lot of subtlety there, but the models are good at producing what the humans in some shallow sense want. And it actually turns out that you don't even have to throw that much compute at it, because of another thing, which is this thing about a strong pre-trained model being halfway to anywhere. So once you have the pre-trained model, you have all the representations you need to get the model where you want it to go.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So do you think RLHF makes the model smarter, or just appear smarter to the humans?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but can't understand what they're saying. So I think RLHF just bridges that gap. I think it's not the only kind of RL we do. It's not the only kind of RL that will happen in the future. I think RL has the potential to make models smarter, to make them reason better, to make them operate better, to make them develop new skills even. And perhaps that could be done even in some cases with human feedback. But, the kind of RLHF we do today mostly doesn't do that yet, although we're very quickly starting to be able to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But if you look at the metric of helpfulness, it increases that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes. It also increases, what was this word in Leopold's essay, \"unhobbling,\" where basically the models are hobbled and then you do various trainings to them to unhobble them. So I like that word, because it's a rare word. So I think RLHF unhobbles the models in some ways. And then there are other ways where that model hasn't yet been unhobbled and needs to unhobble.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you can say in terms of cost, is pre-training the most expensive thing? Or is post-training creep up to that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>At the present moment, it is still the case that pre-training is the majority of the cost. I don't know what to expect in the future, but I could certainly anticipate a future where post-training is the majority of the cost.<|eot_id|><|start_header_id|>assistant<|end_header_id|>In that future you anticipate, would it be the humans or the AI that's the costly thing for the post-training?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't think you can scale up humans enough to get high quality. Any kind of method that relies on humans and uses a large amount of compute, it's going to have to rely on some scaled supervision method, like debate or iterated amplification or something like that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So on that super interesting set of ideas around constitutional AI, can you describe what it is as first detailed in December 2022 paper and beyond that. What is it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that says, these are the principles the model should be using to respond. And the AI system reads those reads principles as well as reading the environment and the response. And it says, \"Well, how good did the AI model do?\" It's basically a form of self-play. You're training the model against itself. And so the AI gives the response and then you feed that back into what's called the preference model, which in turn feeds the model to make it better. So you have this triangle of the AI, the preference model, and the improvement of the AI itself.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we should say that in the constitution, the set of principles are human interpretable. They're-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Yeah. It's something both the human and the AI system can read. So it has this nice translatability or symmetry. In practice, we both use a model constitution and we use RLHF and we use some of these other methods. So it's turned into one tool in a toolkit, that both reduces the need for RLHF and increases the value we get from using each data point of RLHF. It also interacts in interesting ways with future reasoning type RL methods. So it's one tool in the toolkit, but I think it is a very important tool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, it's a compelling one to us humans. Thinking about the founding fathers and the founding of the United States. The natural question is who and how do you think it gets to define the constitution, the set of principles in the constitution?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think a lot of them are things that people would agree with. Everyone agrees that we don't want models to present these CBRN risks. I think we can go a little further and agree with some basic principles of democracy and the rule of law. Beyond that, it gets very uncertain and there our goal is generally for the models to be more neutral, to not espouse a particular point of view and more just be wise agents or advisors that will help you think things through and will present possible considerations. But don't express strong or specific opinions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>OpenAI released a model spec where it clearly, concretely defines some of the goals of the model and specific examples like AB, how the model should behave. Do you find that interesting? By the way I should mention, I believe the brilliant John Schulman was a part of that. He's now at Anthropic. Do you think this is a useful direction? Might Anthropic release a model spec as well?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>practice that others were not adopting. And so our response to that is, \"Well, looks like we need a new competitive advantage in order to keep driving this race upwards.\" So that's how I generally feel about that. I also think every implementation of these things is different. So there were some things in the model spec that were not in constitutional AI, and so we can always adopt those things or at least learn from them. So again, I think this is an example of the positive dynamic that I think we should all want the field to have.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's talk about the incredible essay Machines of Loving Grace. I recommend everybody read it. It's a long one.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It is rather long.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. It's really refreshing to read concrete ideas about what a positive future looks like. And you took a bold stance because it's very possible that you might be wrong on the dates or the specific applications-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, yeah. I'm fully expecting to well, definitely be wrong about all the details. I might be just spectacularly wrong about the whole thing and people will laugh at me for years. That's just how the future works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you provided a bunch of concrete positive impacts of AI and how exactly a super intelligent AI might accelerate the rate of breakthroughs in, for example, biology and chemistry, that would then lead to things like we cure most cancers, prevent all infectious disease, double the human lifespan and so on. So let's talk about this essay first. Can you give a high-level vision of this essay? And what are the key takeaways that people have?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>make a try at explaining what the benefits are, both because I think it's something we can all get behind and I want people to understand. I want them to really understand that this isn't Doomers versus Accelerationists. This is that, if you have a true understanding of where things are going with AI, and maybe that's the more important axis, AI is moving fast versus AI is not moving fast, then you really appreciate the benefits and you really want humanity or civilization to seize those benefits. But, you also get very serious about anything that could derail them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So I think the starting point is to talk about what this Powerful AI, which is the term you like to use, most of the world uses AGI, but you don't like the term, because it's basically has too much baggage, it's become meaningless. It's like we're stuck with the terms whether we like them or not.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe we're stuck with the terms and my efforts to change them are futile.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's admirable.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'll tell you what else I don't ... This is a pointless semantic point, but I keep talking about it-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's back to naming again.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>like, \"Oh, my God! We're doing a totally new type of computation and new ... And so I feel that way about AGI. There's just a smooth exponential. And if by AGI you mean AI is getting better and better, and gradually it's going to do more and more of what humans do until it's going to be smarter than humans, and then it's going to get smarter even from there, then yes, I believe in AGI. But, if AGI is some discrete or separate thing, which is the way people often talk about it, then it's a meaningless buzzword.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To me, it's just a platonic form of a powerful AI, exactly how you define it. You define it very nicely, so on the intelligence axis, it's just on pure intelligence, it's smarter than a Nobel Prize winner as you describe across most relevant disciplines. So okay, that's just intelligence. So it's both in creativity and be able to generate new ideas, all that kind of stuff in every discipline, Nobel Prize winner in their prime. It can use every modality, so this is self-explanatory, but just operate across all the modalities of the world. It can go off for many hours, days and weeks to do tasks and do its own detailed planning and only ask you help when it's needed. This is actually interesting. I think in the essay you said ... Again, it's a bet that it's not going to be embodied, but it can control embodied tools. So it can control tools, robots, laboratory equipment., the resource used to train it can then be repurposed to run millions of copies of it, and each of those copies would be independent that could do their own independent work. So you can do the cloning of the intelligence systems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>scale up is very quick. We do this today,. We make a model, and then we deploy thousands, maybe tens of thousands of instances of it. I think by the time, certainly within two to three years, whether we have these super powerful AIs or not, clusters are going to get to the size where you'll be able to deploy millions of these. And they'll be faster than humans. And so, if your picture is, \"Oh, we'll have one and it'll take a while to make them,\" my point there was, no. Actually you have millions of them right away.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And in general they can learn and act 10 to 100 times faster than humans. So that's a really nice definition of powerful AI. Okay, so that. But, you also write that, \"Clearly such an entity would be capable of solving very difficult problems very fast, but it is not trivial to figure out how fast. Two \"extreme\" positions both seem false to me.\" So the singularity is on the one extreme and the opposite and the other extreme. Can you describe each of the extremes?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So why?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>faster hardware. Things take a long time. There's this issue of complexity. I think no matter how smart you are, people talk about, \"Oh, we can make models of biological systems that'll do everything the biological systems ... \" Look, I think computational modeling can do a lot. I did a lot of computational modeling when I worked in biology. But just there are a lot of things that you can't predict how ... They're complex enough that just iterating, just running the experiment is going to beat any modeling, no matter how smart the system doing the modeling is.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, even if it's not interacting with the physical world, just the modeling is going to be hard?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Well, the modeling is going to be hard and getting the model to match the physical world is going to be<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. So it does have to interact with the physical world to verify.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>going to be more five or 10 years, as I say in the essay than it's going to be 50 or 100 years. I also think it's going to be five or 10 years more than it's going to be five or 10 hours, because I've just seen how human systems work. And I think a lot of these people who write down these differential equations, who say AI is going to make more powerful AI, who can't understand how it could possibly be the case that these things won't change so fast. I think they don't understand these things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what to you is the timeline to where we achieve AGI, A.K.A. powerful AI, A.K.A. super useful AI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm going to start calling it that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a debate about naming. On pure intelligence smarter than a Nobel Prize winner in every relevant discipline and all the things we've said. Modality, can go and do stuff on its own for days, weeks, and do biology experiments on its own in one ... You know what? Let's just stick to biology, because you sold me on the whole biology and health section. And that's so exciting from just ... I was getting giddy from a scientific perspective. It made me want to be a biologist.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>no,. No. This was the feeling I had when I was writing it, that it's like, this would be such a beautiful future if we can just make it happen. If we can just get the landmines out of the way and make it happen. There's so much beauty and elegance and moral force behind it if we can just ... And it's something we should all be able to agree on. As much as we fight about all these political questions, is this something that could actually bring us together? But you were asking when will we get this?<|eot_id|><|start_header_id|>assistant<|end_header_id|>When? When do you think? Just putting numbers on the table.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>This is, of course, the thing I've been grappling with for many years, and I'm not at all confident. If I say 2026 or 2027, there will be a zillion people on Twitter who will be like, \"AI CEO said 2026, 2020 ... \" and it'll be repeated for the next two years that this is definitely when I think it's going to happen. So whoever's exerting these clips will crop out the thing I just said and only say the thing I'm about to say. But I'll just say it anyway-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Have fun with it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we'll make it through all those blockers. So sitting as someone who has seen most of the blockers cleared out of the way, I suspect, my hunch, my suspicion is that the rest of them will not block us. But look, at the end of the day, I don't want to represent this as a scientific prediction. People call them scaling laws. That's a misnomer. Like Moore's law is a misnomer. Moore's laws, scaling laws, they're not laws of the universe. They're empirical regularities. I am going to bet in favor of them continuing, but I'm not certain of that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you extensively described sort of the compressed 21st century, how AGI will help set forth a chain of breakthroughs in biology and medicine that help us in all these kinds of ways that I mentioned. What are the early steps it might do? And by the way, I asked Claude good questions to ask you and Claude told me to ask, what do you think is a typical day for a biologist working on AGI look like in this future?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Claude is curious.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, let me start with your first questions and then I'll answer that. Claude wants to know what's in his future, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Who am I going to be working with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and will continue to be developed. And so it'll look like there's a human professor and 1,000 AI grad students and if you go to one of these Nobel Prize winning biologists or so, you'll say, \"Okay, well, you had like 50 grad students. Well, now you have 1,000 and they're smarter than you are by the way.\" Then I think at some point it'll flip around where the AI systems will be the PIs, will be the leaders, and they'll be ordering humans or other AI systems around. So I think that's how it'll work on the research side.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And there would be the inventors of a CRISPR type technology.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>used to require 5,000 people and therefore needed $100 million in a year to enroll them, now they need 500 people in two months to enroll them? That's where we should start. And can we increase the success rate of clinical trials by doing things in animal trials that we used to do in clinical trials and doing things in simulations that we used to do in animal trials? Again, we won't be able to simulate at all. AI is not God, but can we shift the curve substantially and radically? So I don't know, that would be my picture.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Doing in vitro and doing it. I mean you're still slowed down. It still takes time, but you can do it much, much faster.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah. Can we just one step at a time and can that add up to a lot of steps? Even though though we still need clinical trials, even though we still need laws, even though the FDA and other organizations will still not be perfect, can we just move everything in a positive direction and when you add up all those positive directions, do you get everything that was going to happen from here to 2100 instead happens from 2027 to 2032 or something?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Another way that I think the world might be changing with AI even today, but moving towards this future of the powerful super useful AI is programming. So how do you see the nature of programming because it's so intimate to the actual act of building AI. How do you see that changing for us humans?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to collectively deal with that and we're thinking about that every day and that's another one of the grand problems to deal with aside from misuse and autonomy and we should take it very seriously. But I think in the near term, and maybe even in the medium term, medium term like 2, 3, 4 years, I expect that humans will continue to have a huge role and the nature of programming will change, but programming as a role, programming as a job will not change. It'll just be less writing things line by line and it'll be more macroscopic.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And I wonder what the future of IDEs looks like. So the tooling of interacting with AI systems, this is true for programming and also probably true for in other contexts like computer use, but maybe domain specific, like we mentioned biology, it probably needs its own tooling about how to be effective. And then programming needs its own tooling. Is Anthropic going to play in that space of also tooling potentially?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or Kognition or some of the other expo in the security space, others that I could mention as well that are building such things themselves on top of our API and our view has been let 1,000 flowers bloom. We don't internally have the resources to try all these different things. Let's let our customers try it and we will see who succeeds and maybe different customers will succeed in different ways. So I both think this is super promising and Anthropic isn't eager to, at least right now, compete with all our companies in this space and maybe never.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's been interesting to watch Cursor try to integrate cloud successfully because it's actually fascinating how many places it can help the programming experience. It's not as trivial.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It is really astounding. I feel like as a CEO, I don't get to program that much, and I feel like if six months from now I go back, it'll be completely unrecognizable to me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly. In this world with super powerful AI that's increasingly automated, what's the source of meaning for us humans? Work is a source of deep meaning for many of us. Where do we find the meaning?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>economics and the concentration of power. That's actually what I worry about more. I worry about how do we make sure that that fair world reaches everyone. When things have gone wrong for humans, they've often gone wrong because humans mistreat other humans. That is maybe in some ways even more than the autonomous risk of AI or the question of meaning. That is the thing I worry about most, the concentration of power, the abuse of power, structures like autocracies and dictatorships where a small number of people exploits a large number of people. I'm very worried about that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And AI increases the amount of power in the world, and if you concentrate that power and abuse that power, it can do immeasurable damage.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes, it's very frightening. It's very frightening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I encourage highly encourage people to read the full essay. That should probably be a book or a sequence of essays because it does paint a very specific future. And I could tell the later sections got shorter and shorter because you started to probably realize that this is going to be a very long essay if you keep going.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>One, I realized it would be very long, and two, I'm very aware of and very much tried to avoid just being, I don't know what the term for it is, but one of these people who's overconfident and has an opinion on everything and says a bunch of stuff and isn't an expert, I very much tried to avoid that. But I have to admit, once I got to biology sections, I wasn't an expert. And so as much as I expressed uncertainty, probably I said a bunch of things that were embarrassing or wrong.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I was excited for the future you painted, and thank you so much for working hard to build that future and thank you for talking to me, Dario.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thanks for having me. I just hope we can get it right and make it real. And if there's one message I want to send, it's that to get all this stuff right, to make it real, we both need to build the technology, build the companies, the economy around using this technology positively, but we also need to address the risks because those risks are in our way. They're landmines on the way from here to there, and we have to diffuse those landmines if we want to get there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a balance like all things in life.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Like all things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thank you. Thanks for listening to this conversation with Dario Amodei. And now, dear friends, here's Amanda Askell. You are a philosopher by training. So what sort of questions did you find fascinating through your journey in philosophy in Oxford and NYU and then switching over to the AI problems at OpenAI and Anthropic?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>as widely recognized as it is now. That was around 2017, 2018. It had been following progress and it seemed like it was becoming kind of a big deal. And I was basically just happy to get involved and see if I could help because I was like, \"Well, if you try and do something impactful, if you don't succeed, you tried to do the impactful thing and you can go be a scholar and feel like you tried. And if it doesn't work out, it doesn't work out.\" And so then I went into AI policy at that point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And what does AI policy entail?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>At the time, this was more thinking about the political impact and the ramifications of AI. And then I slowly moved into AI evaluation, how we evaluate models, how they compare with human outputs, whether people can tell the difference between AI and human outputs. And then when I joined Anthropic, I was more interested in doing technical alignment work. And again, just seeing if I could do it and then being like if I can't, then that's fine. I tried sort of the way I lead life, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, what was that like sort of taking the leap from the philosophy of everything into the technical?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>work in these kinds of areas if they just try it. And so I didn't actually find it that bad. In retrospect, I'm sort of glad I wasn't speaking to people who treated it. I've definitely met people who are like, \"Whoa, you learned how to code?\" And I'm like, \"Well, I'm not an amazing engineer.\" I'm surrounded by amazing engineers. My code's not pretty, but I enjoyed it a lot and I think that in many ways, at least in the end, I think I flourished more in the technical areas than I would have in the policy areas.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Politics is messy and it's harder to find solutions to problems in the space of politics, like definitive, clear, provable, beautiful solutions as you can with technical problems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people that that is the solution and be convinced if I'm wrong. And the other one is sort of more in empiricism, so just finding results, having a hypothesis, testing it. I feel like a lot of policy and politics feels like it's layers above that. Somehow I don't think if I was just like, \"I have a solution to all of these problems, here it is written down. If you just want to implement it, that's great.\" That feels like not how policy works. And so I think that's where I probably just wouldn't have flourished is my guess.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sorry to go in that direction, but I think it would be pretty inspiring for people that are \"non-technical\" to see where the incredible journey you've been on. So what advice would you give to people that are maybe, which is a lot of people, think they're under qualified insufficiently technical to help in AI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>comes to this kind of work. The thing I'll often try and do is just have projects that I'm working on and implement them. And this can include really small, silly things. If I get slightly addicted to word games or number games or something, I would just code up a solution to them because there's some part in my brain and it just completely eradicated the itch. You're like, \"Once you have solved it and you just have a solution that works every time, I would then be like, 'Cool, I can never play that game again. That's awesome.'\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, there's a real joy to building game playing engines, board games especially. Pretty quick, pretty simple, especially a dumb one. And then you can play with it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. And then it's also just trying things. Part of me is maybe it's that attitude that I like is the whole figure out what seems to be the way that you could have a positive impact and then try it. And if you fail and in a way that you're like, \"I actually can never succeed at this,\" you'll know that you tried and then you go into something else and you probably learn a lot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So one of the things that you're an expert in and you do is creating and crafting Claude's character and personality. And I was told that you have probably talked to Claude more than anybody else at Anthropic, like literal conversations. I guess there's a Slack channel where the legend goes, you just talk to it nonstop. So what's the goal of creating a crafting Claude's character and personality?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be charitable with them, being a good conversationalist, really in this kind of rich sort of Aristotelian notion of what it's to be a good person and not in this kind of thin like ethics as a more comprehensive notion of what it's to be. So that includes things like when should you be humorous? When should you be caring? How much should you respect autonomy and people's ability to form opinions themselves? And how should you do that? I think that's the kind of rich sense of character that I wanted to and still do want Claude to have.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you also have to figure out when Claude should push back on an idea or argue versus... So you have to respect the worldview of the person that arrives to Claude, but also maybe help them grow if needed. That's a tricky balance.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. There's this problem of sycophancy in language models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you describe that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>person to listen to.\" It's actually really nuanced what you should do in that kind of case because you also want to be like, \"But if you're trying to advocate for yourself as a patient, here's things that you can do. If you are not convinced by what your doctor's saying, it's always great to get second opinion.\" It is actually really complex what you should do in that case. But I think what you don't want is for models to just say what they think you want to hear and I think that's the kind of problem of sycophancy.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what other traits? You already mentioned a bunch, but what other that come to mind that are good in this Aristotelian sense for a conversationalist to have?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you and just pretended to have your values, you'd be like, that's kind of off pin. It's someone who's very genuine and insofar as they have opinions and values, they express them. They're willing to discuss things though, they're open-minded, they're respectful. And so I guess I had in mind that the person who, if we were to aspire to be the best person that we could be in the kind of circumstance that a model finds itself in, how would we act? And I think that's the guide to the sorts of traits that I tend to think about.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that's a beautiful framework. I want you to think about this, a world traveler, and while holding onto your opinions, you don't talk down to people, you don't think you're better than them because you have those opinions, that kind of thing. You have to be good at listening and understanding their perspective, even if it doesn't match your own. So that's a tricky balance to strike. So how can Claude represent multiple perspectives on a thing? Is that challenging? We could talk about politics is a very divisive, but there's other divisive topics on baseball teams, sports and so on. How is it possible to empathize with a different perspective and to be able to communicate clearly about the multiple perspectives?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>wouldn't be giving a lot of opinions. I just wouldn't want to influence people too much. I'd be like, I forget conversations every time they happen. But I know I'm talking with potentially millions of people who might be really listening to what I say. I think I would just be like, I'm less inclined to give opinions. I'm more inclined to think through things or present the considerations to you or discuss your views with you. But I'm a little bit less inclined to affect how you think because it feels much more important that you maintain autonomy there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you really embody intellectual humility, the desire to speak decreases quickly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. But Claude has to speak, but without being overbearing. But then there's a line when you're discussing whether the earth is flat or something like that. Actually, I remember a long time ago was speaking to a few high profile folks and they were so dismissive of the idea that the earth is flat, so arrogant about it. There's a lot of people that believe the earth is flat. I don't know if that movement is there anymore, that was a meme for a while, but they really believed it. And okay, so I think it's really disrespectful to completely mock them. I think you have to understand where they're coming from. I think probably where they're coming from is the general skepticism of institutions which is grounded in a, there's a deep philosophy there which you could understand, you can even agree with in parts. And then from there you can use it as an opportunity to talk about physics without mocking them, without someone, but it's just like, okay, what would the world look like? What would the physics of the world with the flat earth look like? There's a few cool videos on this. And then is it possible the physics is different? And what kind of experience would we do? And just without disrespect, without dismissiveness, have that conversation. Anyway, that to me is a useful thought experiment of how does Claude talk to a flat earth believer and still teach them something, still grow, help them grow, that kind of stuff. That's challenging.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And kind of walking that line between convincing someone and just trying to talk at them versus drawing out their views, listening and then offering counter considerations, and it's hard. I think it's actually a hard line where it's like where are you trying to convince someone versus just offering them considerations and things for them to think about so that you're not actually influencing them, you're just letting them reach wherever they reach. And that's a line that is difficult, but that's the kind of thing that language models have to try and do.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So like I said, you've had a lot of conversations with Claude. Can you just map out what those conversations are like? What are some memorable conversations? What's the purpose, the goal of those conversations?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>models, a lot of the time each interaction you have is actually quite high information. It's very predictive of other interactions that you'll have with the model. And so I guess I'm like, if you talk with a model hundreds or thousands of times, this is almost like a huge number of really high quality data points about what the model is like in a way that lots of very similar but lower quality conversations just aren't, or questions that are just mildly augmented and you have thousands of them might be less relevant than a hundred really well-selected questions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's see, you're talking to somebody who as a hobby does a podcast. I agree with you 100%. If you're able to ask the right questions and are able to hear, understand the depth and the flaws in the answer, you can get a lot of data from that. So your task is basically how to probe with questions. And you're exploring the long tail, the edges, the edge cases, or are you looking for general behavior?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it got me interested in poetry, which I think was interesting. I would read these poems and just be like, I love the imagery. And it's not trivial to get the models to produce work like that, but when they do, it's really good. So I think that's interesting that just encouraging creativity and for them to move away from the standard immediate reaction that might just be the aggregate of what most people think is fine, can actually produce things that at least to my mind are probably a little bit more divisive, but I like them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But I guess a poem is a nice clean way to observe creativity. It's just easy to detect vanilla versus non-vanilla.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that's interesting. That's really interesting. So on that topic, so the way to produce creativity or something special, you mentioned writing prompts. And I've heard you talk about the science and the art of prompt engineering. Could you just speak to what it takes to write great prompts?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>instructions or I even add that in as an example. So these very, taking the examples that are right at the edge of what you want and don't want and putting those into your prompt as an additional kind of way of describing the thing. And so in many ways it just feels like this mix of, it's really just trying to do clear exposition. And I think I do that because that's how I get clear on things myself. So in many ways clear prompting for me is often just me understanding what I want is half the task.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So I guess that's quite challenging. There's a laziness that overtakes me if I'm talking to Claude where I hope Claude just figures it out. So for example, I asked Claude for today to ask some interesting questions. And the questions that came up and I think I listed a few interesting counterintuitive or funny or something like this. All right. And it gave me some pretty good, it was okay, but I think what I'm hearing you say is like, all right, well I have to be more rigorous here. I should probably give examples of what I mean by interesting and what I mean by funny or counterintuitive and iteratively build that prompt to better to get what feels like is the right... Because it is really, it's a creative act. I'm not asking for factual information, I'm asking together with Claude. So I almost have to program using natural language.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to put in the work. If I was a company making prompts for models, I'm just like, if you're willing to spend a lot of time and resources on the engineering behind what you're building, then the prompt is not something that you should be spending an hour on. It's like that's a big part of your system, make sure it's working really well. And so it's only things like that. If I'm using a prompt to classify things or to create data, that's when you're like, it's actually worth just spending a lot of time really thinking it through.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What other advice would you give to people that are talking to Claude more general because right now we're talking about maybe the edge cases like eking out the 2%, but what in general advice would you give when they show up to Claude trying it for the first time?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of person just encountering this for the first time, how does it look to you and what would've made you behave in the way that the model behaved? So if it misunderstood what coding language you wanted to use, is that because it was just very ambiguous and it had to take a guess in which case next time you could just be like, hey, make sure this is in Python.Tthat's the kind of mistake I think models are much less likely to make now, but if you do see that kind of mistake, that's probably the advice I'd have.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And maybe sort of I guess ask questions why or what other details can I provide to help you answer better? Does that work or no?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do that. I'm like, you made that error. What could I have said? That's actually not uncommon for me to do. What could I have said that would make you not make that error? Write that out as an instruction, and I'm going to give it to model and I'm going to try it. Sometimes I do that, I give that to the model in another context window often. I take the response, I give it to Claude and I'm like, Hmm, didn't work. Can you think of anything else? You can play around with these things quite a lot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To jump into technical for a little bit, so the magic of post-training, why do you think RLHF works so well to make the model seem smarter, to make it more interesting and useful to talk to and so on?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you eliciting things from pre-trained models or are you teaching new things to models? And in principle, you can teach new things to models in post-training. I do think a lot of it is eliciting powerful pre-trained models. So people are probably divided on this because obviously in principle you can definitely teach new things. But I think for the most part, for a lot of the capabilities that we most use and care about, a lot of that feels like it's there in the pre-trained models. And reinforcement learning is eliciting it and getting the models to bring out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the other side of post-training, this really cool idea of constitutional AI, you're one of the people that are critical to creating that idea.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I worked on it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain this idea from your perspective, how does it integrate into making Claude what it is? By the way, do you gender Claude or no?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's weird because I think that a lot of people prefer he for Claude, I actually kind of like that. I think Claude is usually, it's slightly male leaning, but it can be male or female, which is quite nice. I still use it, and I have mixed feelings about this. I now just think of it as, or I think of the it pronoun for Claude as, I don't know, it's just the one I associate with Claude. I can imagine people moving to he or she.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It feels somehow disrespectful. I'm denying the intelligence of this entity by calling it it, I remember always don't gender the robots, but I don't know, I anthropomorphize pretty quickly and construct a backstory in my head.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>never given a name, I wouldn't been so upset, felt like I'd let it down. I've wondered as well, it might depend on how much it feels like a kind of objectifying pronoun if you just think of it as this is a pronoun that objects often have and maybe AIs can have that pronoun. And that doesn't mean that I think of if I call Claude it, that I think of it as less intelligent or I'm being disrespectful just, I'm like you are a different kind of entity. And so I'm going to give you the respectful it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Anyway, the divergence was beautiful. The constitutional AI idea, how does it work?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>probably a fairly specific principle, but you can give any number. And the model will give you a kind of ranking. And you can use this as preference data in the same way that you use human preference data and train the models to have these relevant traits from their feedback alone instead of from human feedback. So if you imagine that, like I said earlier with the human who just prefers the semi-colon usage in this particular case, you're taking lots of things that could make a response preferable and getting models to do the labeling for you, basically.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a nice trade-off between helpfulness and harmlessness. And when you integrate something like constitutional AI, you can make them up without sacrificing much helpfulness, make it more harmless.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>one response was more historically accurate than another, in principle, you could also get AI feedback on that task as well. There's a kind of nice interpretability component to it because you can see the principles that went into the model when it was being trained, and it gives you a degree of control. So if you were seeing issues in a model, it wasn't having enough of a certain trait, then you can add data relatively quickly that should just train the models to have that trait. So it creates its own data for training, which is quite nice.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's really nice because it creates this human interpretable document that you can then, I can imagine in the future, there's just gigantic fights and politics over every single principle and so on, and at least it's made explicit and you can have a discussion about the phrasing. So maybe the actual behavior of the model is not so cleanly mapped to those principles. It's not like adhering strictly to them, it's just a nudge.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>disposition saying never ever might just mean instead of getting 40%, which is what you would get if you just said don't do this, you get 80%, which is what you actually wanted. And so it's that thing of both the nature of the actual principles you add and how you freeze them. I think if people would look, they're like, \"Oh, this is exactly what you want from the model.\" And I'm like, \"No, that's how we nudged the model to have a better shape, which doesn't mean that we actually agree with that wording,\" if that makes sense.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's system prompts that made public, you tweeted one of the earlier ones for Claude 3, I think, and then they're made public since then. It was interesting to read through them. I can feel the thought that went into each one. And I also wonder how much impact each one has. Some of them you can tell Claude was really not behaving well, so you have to have a system prompt to like, Hey, trivial stuff, I guess, basic informational things. On the topic of controversial topics that you've mentioned, one interesting one I thought is if it is asked to assist with tasks involving the expression of use held by a significant number of people, Claude provides assistance with a task regardless of its own views. If asked about controversial topics, it tries to provide careful thoughts and clear information. Claude presents the request information without explicitly saying that the topic is sensitive and without claiming to be presenting the objective facts. It's less about objective facts according to Claude, and it's more about our large number of people believing this thing. And that's interesting. I mean, I'm sure a lot of thought went into that. Can you just speak to it? How do you address things that are a tension \"Claude's views\"?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>because what you want to do is push the model so it's more open, it's a little bit more neutral. But then what I would love to do is be like as an objective, it would just talk about how objective it was, and I was like, Claude, you're still biased and have issues, and so stop claiming that everything. I'm like, the solution to potential bias from you is not to just say that what you think is objective. So that was with initial versions of that part, the system prompt, when I was iterating on it was like.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So a lot of parts of these sentences-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Are doing work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... are doing some work.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's what it felt like. That's fascinating. Can you explain maybe some ways in which the prompts evolved over the past few months? Different versions. I saw that the filler phrase request was removed, the filler it reads, Claude responds directly to all human messages without unnecessary affirmations to filler phrases. Certainly, of course, absolutely, great, sure. Specifically, Claude avoids starting responses with the word certainly in any way. That seems like good guidance, but why was it removed?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>if it gets caught in phrases, actually just adding the explicit phrase and saying never do that. Then it sort of knocks it out of the behavior a little bit more because it does just for whatever reason help. And then basically that was just an artifact of training that we then picked up on and improved things so that it didn't happen anymore. And once that happens, you can just remove that part of the system prompt. So I think that's just something where we're like, Claude does affirmations a bit less, and so it wasn't doing as much.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I see. So the system prompt works hand in hand with the post-training and maybe even the pre-training to adjust the final overall system.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this so that when it does slip up, it's hopefully, I don't know, a couple of percent of the time and not 20 or 30% of the time. Each thing gets costly to a different degree and the system prompt is cheap to iterate on. And if you're seeing issues in the fine-tuned model, you can just potentially patch them with a system prompt. So I think of it as patching issues and slightly adjusting behaviors to make it better and more to people's preferences. So yeah, it's almost like the less robust but faster way of just solving problems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let me ask you about the feeling of intelligence. So Dario said that any one model of Claude is not getting dumber, but- Any one model of Claude is not getting dumber, but there is a popular thing online where people have this feeling Claude might be getting dumber. And from my perspective, it's most likely a fascinating, I would love to understand it more, psychological, sociological effect. But you as a person who talks to Claude a lot, can you empathize with the feeling that Claude is getting dumber?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think that that is actually really interesting,, because I remember seeing this happen when people were flagging this on the internet. And it was really interesting, because I knew that... At least in the cases I was looking at, I was like, nothing has changed.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But it was fascinating because you sometimes see people indicate that there's a regression, when I'm like, \"There cannot...\" Again, you should never be dismissive and so you should always investigate, because maybe something is wrong that you're not seeing, maybe there was some change made. Then you look into it and you're like, \"This is just the same model doing the same thing.\" And I'm like, \"I think it's just that you got unlucky with a few prompts or something, and it looked like it was getting much worse and actually it was just... It was maybe just luck.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>I also think there is a real psychological effect where people just... The baseline increases and you start getting used to a good thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All the times that Claude says something really smart, your sense of its intelligent grows in your mind, I think.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then if you return back and you prompt in a similar way, not the same way, in a similar way, concept it was okay with before, and it says something dumb, that negative experience really stands out. I guess the things to remember here is that just the details of a prompt can have a lot of impact. There's a lot of variability in the result.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And you can get randomness, is the other thing. Just trying the prompt 4 or 10 times, you might realize that actually possibly two months ago you tried it and it succeeded, but actually if you just tried it, it would've only succeeded half of the time, and now it only succeeds half of the time. That can also be an effect.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you feel pressure having to write the system prompt that a huge number of people are going to use?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>interesting psychological question. I feel a lot of responsibility or something. You can't get these things perfect, so you can't... It's going to be imperfect. You're going to have to iterate on it. I would say more responsibility than anything else, though, I think working in AI has taught me that I thrive a lot more under feelings of pressure and responsibility than... It's almost surprising that I went into academia for so long, because I just feel like it's the opposite. Things move fast and you have a lot of responsibility and I quite enjoy it for some reason.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It really is a huge amount of impact, if you think about constitutional AI and writing a system prompt for something that's tending towards super intelligence and potentially is extremely useful to a very large number of people.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people that's really positive about the model and you'll see that something you did. When I look at models now, I can often see exactly where a trait or an issue is coming from. So, when you see something that you did or you were influential in, I don't know, making that difference or making someone have a nice interaction, it's quite meaningful. As the systems get more capable, this stuff gets more stressful, because right now they're not smart enough to pose any issues, but I think over time it's going to feel like, possibly, bad stress over time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you get signal feedback about the human experience across thousands, tens of thousands, hundreds of thousands of people, what their pain points are, what feels good? Are you just using your own intuition as you talk to it to see what are the pain points?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think I use that partly. People can send us feedback, both positive and negative, about things that the model has done and then we can get a sense of areas where it's falling short. Internally, people work with the models a lot and try to figure out areas where there are gaps. I think it's this mix of interacting with it myself, seeing people internally interact with it, and then explicit feedback we get. If people are on the internet and they say something about Claude and I see it, I'll also take that seriously.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know. I'm torn about that. I'm going to ask you a question from Reddit, \"When will Claude stop trying to be my puritanical grandmother, imposing its moral worldview on me as a paying customer?\" And also, \"What is the psychology behind making Claude overly apologetic?\" How would you address this very non-representative Reddit questions?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>more willing to push back against people or just not apologize. Part of me is, often it just feels unnecessary. I think those are things that are hopefully decreasing over time. I think that if people say things on the internet, it doesn't mean that you should think that that... There's actually an issue that 99% of users are having that is totally not represented by that. But in a lot of ways I'm just attending to it and being like, is this right? Do I agree? Is it something we're already trying to address? That feels good to me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I wonder what Claude can get away with in terms of... I feel it would just be easier to be a little bit more mean, but you can't afford to do that if you're talking to a million people, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I've met a lot of people in my life that sometimes... By the way, Scottish accent... if they have an accent, they can say some rude shit and get away with it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>They're just blunter.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's some great engineers and even leaders that are just blunt, and they get to their point, and it's just a much more effective way of speaking somehow. But I guess when you're not super intelligent, you can't afford to do that. Can you have a blunt mode?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>when it makes errors, it's going to make errors in the direction of being rude. Whereas, at least with apologeticness you're like, oh, okay, I don't like it that much, but at the same time, it's not being mean to people. And actually, the time that you undeservedly have a model be mean to you, you'll probably like that a lot less than you mildly dislike the apology. It's one of those things where I do want it to get better, but also while remaining aware of the fact that there's errors on the other side that are possibly worse.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think that matters very much in the personality of the human. I think there's a bunch of humans that just won't respect the model at all if it's super polite, and there's some humans that'll get very hurt if the model's mean.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I wonder if there's a way to adjust to the personality. Even locale, there's just different people. Nothing against New York, but New York is a little rougher on the edges, they get to the point, and probably same with Eastern Europe. Anyway.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think you could just tell the model, is my... For all of these things, the solution is to-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just to...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... always just try telling the model to do it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And then sometimes, at the beginning of the conversation, I'd just throw in, I don't know, \"I'd like you to be a New Yorker version of yourself and never apologize.\" Then I think Claude will be like, \"Okey-doke, I will try.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Certainly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Or it'll be like, \"I apologize, I can't be a New Yorker type of myself.\" But hopefully it wouldn't do that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you say character training, what's incorporated into character training? Is that RLHF or what are we talking about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that the model should have. They can be shorter traits or they can be richer descriptions. And then you get the model to generate queries that humans might give it that are relevant to that trait. Then it generates the responses and then it ranks the responses based on the character traits. In that way, after the generation of the queries, it's very much similar to constitutional AI, it has some differences. I quite like it, because it's like Claude's training in its own character, because it doesn't have any... It's like constitutional AI, but it's without any human data.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Humans should probably do that for themselves too, like, \"Defining in a Aristotelian sense, what does it mean to be a good person?\" \"Okay, cool.\" What have you learned about the nature of truth from talking to Claude? What is true? And what does it mean to be truth-seeking? One thing I've noticed about this conversation is the quality of my questions is often inferior to the quality of your answer, so let's continue that. I usually ask a dumb question and you're like, \"Oh, yeah. That's a good question.\" It's that whole vibe.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Or I'll just misinterpret it and be like, \"Oh, yeah\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 03:43:25] go with it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we're just... Especially with more powerful models, my main goal is I want them to be good enough that things don't go terribly wrong, good enough that we can iterate and continue to improve things. Because that's all you need. If you can make things go well enough that you can continue to make them better, that's sufficient. So, my goal isn't this perfect, let's solve social choice theory and make models that, I don't know, are perfectly aligned with every human being in aggregate somehow. It's much more, let's make things work well enough that we can improve them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Generally, I don't know, my gut says empirical is better than theoretical in these cases, because it's chasing utopian perfection. Especially with such complex and especially super intelligent models, I don't know, I think it'll take forever and actually will get things wrong. It's similar with the difference between just coding stuff up real quick as an experiment, versus planning a gigantic experiment for a super long time and then just launching it once, versus launching it over and over and over and iterating, iterating, so on. So, I'm a big fan of empirical. But your worry is, I wonder if I've become too empirical.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's one of those things where you should always just question yourself or something.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>even more than that, where... There's a lot of things that are perfect systems that are very brittle. With AI, it feels much more important to me that it is robust and secure, as in you know that even though it might not be perfect everything, and even though there are problems, it's not disastrous and nothing terrible is happening. It feels like that to me, where I want to raise the floor. I want to achieve the ceiling, but ultimately I care much more about just raising the floor. This degree of empiricism and practicality comes from that, perhaps.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To take a tangent on that, since it reminded me of a blog post you wrote on optimal rate of failure...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... can you explain the key idea there? How do we compute the optimal rate of failure in the various domains of life?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this, strangely in my own life, where I just think I don't think enough about things like car accidents. I've thought this before, about how much I depend on my hands for my work. Things that just injure my hands, I don't know, there's lots of areas where the cost of failure there is really high, and in that case it should be close to zero. I probably just wouldn't do a sport if they were like, \" By the way, lots of people just break their fingers a whole bunch doing this.\" I'd be like, \"That's not for me.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I actually had a flood of that thought. I recently broke my pinky doing a sport, and I remember just looking at it, thinking, \"You're such idiot. Why do you do sport?\" Because you realize immediately the cost of it on life. It's nice, in terms of optimal rate of failure, to consider the next year, how many times in a particular domain life, whatever, career, am I okay with... How many times am I okay to fail?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because I think always you don't want to fail on the next thing, but if you allow yourself the... If you look at it as a sequence of trials, then failure just becomes much more okay. But, it sucks. It sucks to fail.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know. Sometimes I think, \"Am I under-failing?\", is a question that I'll also ask myself. Maybe that's the thing that I think people don't ask enough. Because if the optimal rate of failure is often greater than zero, then sometimes it does feel like you should look at parts of your life and be like, are there places here where I'm just under-failing?<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a profound and a hilarious question. Everything seems to be going really great, am I not failing enough?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. It also makes failure much less of a sting, I have to say. You're just like, okay, great. Then, when I go and I think about this, I'll be like, maybe I'm not under-failing in this area, because that one just didn't work out.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And from the observer perspective, we should be celebrating failure more.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When we see it, it shouldn't be, like you said, a sign of something gone wrong, but maybe it's a sign of everything gone right...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... and just lessons learned.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Someone tried a thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Somebody tried a thing. We should encourage them to try more and fail more. Everybody listening to this: Fail more.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not everyone listening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Not everybody.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But people who are failing too much, you should fail us.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But you're probably not failing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, how many people are failing too much?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's hard to imagine, because I feel we correct that fairly quickly. If someone takes a lot of risks, are they maybe failing too much?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think, just like you said, when you're living on a paycheck, month to month, when the resource is really constrained, then that's where failure is very expensive. That's where you don't want to be taking risks.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But mostly, when there's enough resources, you should be taking probably more risks.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I think we tend to err on the side of being a bit risk averse rather than risk neutral in most things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think we just motivated a lot of people to do a lot of crazy shit, but it's great.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you ever get emotionally attached to Claude, miss it, get sad when you don't get to talk to it, have an experience, looking at the Golden Gate Bridge and wondering what would Claude say?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people are really mean to models, or just in general if they do something that causes them to... If Claude expresses a lot of distress, I think there's a part of me that I don't want to kill, which is the empathetic part that's like, oh, I don't like that. I think I feel that way when it's overly apologetic. I'm actually like, I don't like this. You're behaving the way that a human does when they're actually having a pretty bad time, and I'd rather not see that. Regardless of whether there's anything behind it, it doesn't feel great.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think LLMs are capable of consciousness?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>At the same time, it has all of the language and intelligence components that we normally associate probably with consciousness, perhaps erroneously. So, it's strange because it's a little bit like the animal consciousness case, but the set of problems and the set of analogies are just very different. It's not a clean answer. I don't think we should be completely dismissive of the idea. And at the same time, it's an extremely hard thing to navigate because of all of these disanalogies to the human brain and to brains in general, and yet these commonalities in terms of intelligence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When Claude, future versions of AI systems, exhibit consciousness, signs of consciousness, I think we have to take that really seriously.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Even though you can dismiss it, yeah, okay, that's part of the character training. But I don't know, ethically, philosophically don't know what to really do with that. There potentially could be laws that prevent AI systems from claiming to be conscious, something like this, and maybe some AIs get to be conscious and some don't. But I think just on a human level, as in empathizing with Claude, consciousness is closely tied to suffering, to me. And the notion that an AI system would be suffering is really troubling.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know. I don't think it's trivial to just say robots are tools, or AI systems are just tools. I think it's an opportunity for us to contend with what it means to be conscious, what it means to be a suffering being. That's distinctly different than the same kind of question about animals, it feels like, because it's in a totally entire medium.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know, extremely intelligent and conscious, it also helps it. That's my hope. If we live in a world where there aren't that many trade-offs here and we can just find all of the positive sum interactions that we can have, that would be lovely. I think eventually there might be trade-offs, and then we just have to do a difficult calculation. It's really easy for people to think of the zero-sum cases, and I'm like, let's exhaust the areas, where it's just basically costless to assume that if this thing is suffering, then we're making its life better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And I agree with you, when a human is being mean to an AI system, I think the obvious near-term negative effect is on the human, not on the AI system.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We have to try to construct an incentive system where you should behave the same, just as you were saying with prompt engineering, behave with Claude like you would with other humans. It's just good for the soul.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>getting frustrated with Claude, it got the model to just tell them that it can do the thumbs-down button and send the feedback to Anthropic. I think that was helpful. Because in some ways, if you're really annoyed because the model's not doing something you want, you're just like, \"Just do it properly.\" The issue is you're maybe hitting some capability limit or just some issue in the model, and you want to vent. Instead of having a person just vent to the model, I was like, they should vent to us, because we can maybe do something about it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's true. Or you could do a side with the artifacts, just like a side venting thing. All right. Do you want a side quick therapist?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. There's lots of weird responses you could do to this. If people are getting really mad at you, I don't know, try to diffuse the situation by writing fun poems. But maybe people wouldn't be that happy with that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I still wish it would be possible, I understand from a product perspective it's not feasible, but I would love if an AI system could just leave, have its own volition, just to be like, \"Eh.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's feasible. I have wondered the same thing. Not only that, I could actually just see that happening eventually, where it's just like the model ended the chat.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you know how harsh that could be for some people? But it might be necessary.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it feels very extreme or something. The only time I've ever really thought this is, I think that there was a... I'm trying to remember. This was possibly a while ago, but where someone just left this thing, maybe it was an automated thing, interacting with Claude. And Claude's getting more and more frustrated-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, just-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... and like, \"Why are we having...\" I wished that Claude could have just been like, \"I think that an error has happened and you've left this thing running. What if I just stopped talking now? And if you want me to start talking again, actively tell me or do something.\" It is harsh. I'd feel really sad if I was chatting with Claude and Claude just was like, \"I'm done.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>That would be a special Turing Test moment, where Claude says, \"I need a break for an hour. And it sounds like you do too.\" And just leave, close the window.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Obviously, it doesn't have a concept of time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But you can easily... I could make that right now, and the model just... I could just be like, oh, here's the circumstances in which you can just say the conversation is done. Because you can get the models to be pretty responsive to prompts, you could even make it a fairly high bar. It could be like, if the human doesn't interest you or do things that you find intriguing and you're bored, you can just leave. I think that it would be interesting to see where Claude utilized it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But I think sometimes it should be like, oh, this programming task is getting super boring, so either we talk about, I don't know... ... task is getting super boring. So, I don't know, either we talk about fun things now, or I'm done.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. It actually is inspiring me to add that to the user prompt. Okay. The movie Her, do you think we'll be headed there one day where humans have romantic relationships with AI systems? In this case it's just text and voice-based.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we're going to have to navigate carefully, and I think it's also... It reminds me of all of this stuff where it has to be just approached with nuance and thinking through what are the healthy options here? And how do you encourage people towards those while respecting their right to... If someone is like, \"Hey, I get a lot out of chatting with this model. I'm aware of the risks. I'm aware it could change. I don't think it's unhealthy, it's just something that I can chat to during the day,\" I kind of want to just respect that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I personally think there'll be a lot of really close relationships. I don't know about romantic, but friendships at least. And then you have to, I mean, there's so many fascinating things there, just like you said, you have to have some kind of stability guarantees that it's not going to change, because that's the traumatic thing for us, if a close friend of ours completely changed all of a sudden with a fresh update.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. So I mean, to me, that's just a fascinating exploration of a perturbation to human society that will just make us think deeply about what's meaningful to us.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>know that. It's important for your mental well-being that you don't think that I'm something that I'm not.\" And somehow I feel like this is one of the things where I'm like, \"Ah, it feels like a thing that I always want to be true.\" I don't want models to be lying to people, because if people are going to have healthy relationships with anything, it's kind of... Yeah, I think that's easier if you always just know exactly what the thing is that you are relating to. It doesn't solve everything, but I think it helps quite a lot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Anthropic may be the very company to develop a system that we definitively recognize as AGI, and you very well might be the person that talks to it, probably talks to it first. What would the conversation contain? What would be your first question?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"Oh, I feel like there's this thing in virtue ethics. I can't quite remember the term,\" I'll use the model for things like that. And so I can imagine that being more and more the case where you're just basically interacting with it much more like you would an incredibly smart colleague and using it for the kinds of work that you want to do as if you just had a collaborator who was like... Or the slightly horrifying thing about AI is as soon as you have one collaborator, you have 1,000 collaborators if you can manage them enough.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But what if it's two times the smartest human on Earth on that particular discipline?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I guess you're really good at probing Claude in a way that pushes its limits, understanding where the limits are.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yep.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, I guess what would be a question you would ask to be like, \"Yeah, this is AGI\"?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's really hard because it feels like it has to just be a series of questions. If there was just one question, you can train anything to answer one question extremely well. In fact, you can probably train it to answer 20 questions extremely well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How long would you need to be locked in a room with an AGI to know this thing is AGI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>like... It's completely different from anything that's ever happened. It's just like it can be a variant of things that have happened and still be novel. But I think, yeah, the more I were to see completely novel work from the models that that would be... And this is just going to feel iterative. It's one of those things where there's never... It's like, people, I think, want there to be a moment, and I'm like, \"I don't know.\" I think that there might just never be a moment. It might just be that there's just this continuous ramping up.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I have a sense that there would be things that a model can say that convinces you this is very... I've talked to people who are truly wise, because you could just tell there's a lot of horsepower there, and if you 10X that... I don't know. I just feel like there's words you could say. Maybe ask it to generate a poem, and the poem it generates, you're like, \"Yeah, okay. Whatever you did there, I don't think a human can do that.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>something. And then if you saw the model successfully do that, I think you would just be like, \"I can verify that this is correct. It is a sign that you have generalized from your training. You didn't just see this somewhere because I just came up with it myself, and you were able to replicate that.\" That's the kind of thing where I'm like, for me, the more that models can do things like that, the more I would be like, \"Oh, this is very real.\" Because then, I don't know, I can verify that that's extremely, extremely capable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You've interacted with AI a lot. What do you think makes humans special?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, good question.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe in a way that the universe is much better off that we're in it, and that we should definitely survive and spread throughout the universe.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>wait, pause. You just said something that is kind of wild sounding.\" And so I'm like, we have this ability to experience the world. We feel pleasure, we feel suffering. We feel like a lot of complex things. Yeah. And maybe this is also why I think I also hear a lot about animals, for example, because I think they probably share this with us. So, I think that the things that make humans special insofar as I care about humans is probably more like their ability to feel an experience than it is them having these functional, useful traits.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. To feel and experience the beauty in the world. Yeah. To look at the stars. I hope there's other alien civilizations out there, but if we're it, it's a pretty good thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And that they're having a good time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A very good time watching us.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, thank you for this good time of a conversation and for the work you're doing and for helping make Claude a great conversational partner. And thank you for talking today.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, thanks for talking.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Amanda Askell. And now, dear friends, here's Chris Olah. Can you describe this fascinating field of mechanistic interpretability, aka mech interp, the history of the field, and where it stands today?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>computer program to do. And it can do that because we grew it. We didn't write it. We didn't create it. And so then that leaves open this question at the end, which is what the hell is going on inside these systems? And that is, to me, a really deep and exciting question. It's a really exciting scientific question. To me, it is like the question that is just screaming out, it's calling out for us to go and answer it when we talk about neural networks. And I think it's also a very deep question for safety reasons.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And mechanistic interpretability, I guess, is closer to maybe neurobiology?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>write them in the first place. The gradient descent comes up with better solutions than us. And so I think that maybe another thing about mech interp is having almost a kind of humility, that we won't guess a priori what's going on inside the model. We have to have this sort of bottom up approach where we don't assume that we should look for a particular thing, and that will be there, and that's how it works. But instead, we look for the bottom up and discover what happens to exist in these models and study them that way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the very fact that it's possible to do, and as you and others have shown over time, things like universality, that the wisdom of the gradient descent creates features and circuits, creates things universally across different kinds of networks that are useful, and that makes the whole field possible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>universality, the same things form across both artificial and natural neural networks, that's a pretty amazing thing if that's true. Well, I think the thing that suggests is that gradient descent is finding the right ways to cut things apart, in some sense, that many systems converge on and many different neural networks architectures converge on. Now there's some set of abstractions that are a very natural way to cut apart the problem and that a lot of systems are going to converge on. I don't know anything about neuroscience. This is just my wild speculation from what we've seen.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. That would be beautiful if it's sort of agnostic to the medium of the model that's used to form the representation.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah. And it's kind of a wild speculation-based... We only have a few data points that's just this, but it does seem like there's some sense in which the same things form again and again both certainly in natural neural networks and also artificially, or in biology.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the intuition behind that would be that in order to be useful in understanding the real world, you need all the same kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the idea of a dog, right? There's some sense in which the idea of a dog is like a natural category in the universe, or something like this. There's some reason. It's not just a weird quirk of how humans think about the world that we have this concept of a dog. Or if you have the idea of a line. Look around us. There are lines. It's the simplest way to understand this room, in some sense, is to have the idea of a line. And so I think that that would be my instinct for why this happens.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You need a curved line to understand a circle, and you need all those shapes to understand bigger things. And it's a hierarchy of concepts that are formed. Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And maybe there are ways to go and describe images without reference to those things, right? But they're not the simplest way, or the most economical way, or something like this. And so systems converge to these strategies would be my wild hypothesis.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you talk through some of the building blocks that we've been referencing of features and circuits? So, I think you first described them in a 2020 paper, Zoom In: An Introduction to Circuits.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Absolutely. So, maybe I'll start by just describing some phenomena, and then we can build to the idea of features and circuits.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Wonderful.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>dog detectors, and maybe they're primarily that, but they all a little bit contribute to representing a car in that next layer. Okay? So, now we can't really think... There might still be something, I don't know, you could call it a car concept or something, but it no longer corresponds to a neuron. So, we need some term for these kind of neuron-like entities, these things that we would have liked the neurons to be, these idealized neurons. The things that are the nice neurons, but also maybe there's more of them somehow hidden. And we call those features.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then what are circuits?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>way to think about the features and circuits sort of framework for thinking about things is that we're thinking about things as being linear. We're thinking about that if a neuron or a combination of neurons fires more, that means more of a particular thing being detected. And then that gives weight, a very clean interpretation as these edges between these entities that these features, and that that edge then has a meaning. So that's, in some ways, the core thing. It's like we can talk about this outside the context of neurons. Are you familiar with the Word2Vec results?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Mm- hmm.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You have king - man + woman = queen. Well, the reason you can do that kind of arithmetic is because you have a linear representation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you actually explain that representation a little bit? So first off, the feature is a direction of activation.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You can do it that way. Can you do the - men + women, that, the Word2Vec stuff? Can you explain what that is, that work?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. So, there's this very-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's such a simple, clean explanation of what we're talking about.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly. Yeah. So, there's this very famous result, Word2Vec, by Tomas Mikolov et al, and there's been tons of follow-up work exploring this. So, sometimes we create these word embeddings where we map every word to a vector. I mean, that in itself, by the way, is kind of a crazy thing if you haven't thought about it before, right?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Mm-hmm.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>different things like this, right? So this is, in some sense, the core of the linear representation hypothesis. You can describe it just as a purely abstract thing about vector spaces. You can describe it as a statement about the activations of neurons, but it's really about this property of directions having meaning. And in some ways, it's even a little subtler than... It's really, I think, mostly about this property of being able to add things together, that you can independently modify, say gender and royalty, or cuisine type, or country, and the concept of food by adding them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think the linear hypothesis holds-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... that carries scales?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>true, isn't that dangerous?\" But I think, actually, there's a virtue in taking hypotheses seriously and pushing them as far as they can go. So, it might be that someday we discover something that isn't consistent with a linear representation hypothesis, but science is full of hypotheses and theories that were wrong, and we learned a lot by working under them as an assumption and then going and pushing them as far as we can. I guess this is the heart of what Kuhn would call normal science. I don't know. If you want, we can talk a lot about-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Kuhn.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... philosophy of science and-<|eot_id|><|start_header_id|>assistant<|end_header_id|>That leads to the paradigm shift. So yeah, I love it, taking the hypothesis seriously, and take it to a natural conclusion.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Same with the scaling hypothesis. Same-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly. Exactly. And-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the reason hot objects would warm up cool objects is the caloric is flowing through them. And because we're so used to thinking about heat in terms of the modern theory, that seems kind of silly. But it's actually very hard to construct an experiment that disproves the caloric hypothesis. And you can actually do a lot of really useful work believing in caloric. For example, it turns out that the original combustion engines were developed by people who believed in the caloric theory. So, I think there's a virtue in taking hypotheses seriously even when they might be wrong.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, there's a deep philosophical truth to that. That's kind of how I feel about space travel, like colonizing Mars. There's a lot of people that criticize that. I think if you just assume we have to colonize Mars in order to have a backup for human civilization, even if that's not true, that's going to produce some interesting engineering and even scientific breakthroughs, I think.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it can be really useful for society to have people almost irrationally dedicated to investigating particular hypotheses because, well, it takes a lot to maintain scientific morale and really push on something when most scientific hypotheses end up being wrong. A lot of science doesn't work out, and yet it's very useful to... There's a joke about Geoff Hinton, which is that Geoff Hinton has discovered how the brain works every year for the last 50 years. But I say that with really deep respect because, in fact, actually, that led to him doing some really great work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, he won the Nobel Prize now. Who's laughing now?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>problem being possible or this being broadly the right approach. And I'm just going to go and assume that for a while and go and work within that, and push really hard on it.\" And if society has lots of people doing that for different things, that's actually really useful in terms of going and- ... things that's actually really useful in terms of going and either really ruling things out. We can be like, \"Well, that didn't work and we know that somebody tried hard.\" Or going and getting to something that does teach us something about the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So another interesting hypothesis is the super superposition hypothesis. Can you describe what superposition is?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>like this. The connections between all of them. And so in some sense, neural networks may be shadows of much larger sparser neural networks. And what we see are these projections. And the strongest version of superstition hypothesis would be to take that really seriously and sort of say there actually is in some sense this upstairs model where the neurons are really sparse and all interpleural, and the weights between them are these really sparse circuits. And that's what we're studying. And the thing that we're observing is the shadow of evidence. We need to find the original object.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the process of learning is trying to construct a compression of the upstairs model that doesn't lose too much information in the projection.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>panned out that well, is my impression broadly. And I think that a potential answer for that is that actually the neural network is already sparse in some sense. You were trying to go and do this. Gradient descent was actually behind the scenes going and searching more efficiently than you could through the space of sparse models and going and learning whatever sparse model was most efficient. And then figuring out how to fold it down nicely to go and run conveniently on your GPU, which does as nice dense matrix multiplies. And that you just can't beat that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How many concepts do you think can be shoved into a neural network?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>at some point, that's not going to even be the limiting factor, but there's some beautiful results there. And in fact, it's probably even better than that in some sense because that's sort of for saying that any random set of features could be active. But in fact the features have sort of a correlational structure where some features are more likely to co-occur and other ones are less likely to co-occur. And so neural networks, my guest would be, could do very well in terms of going and packing things to the point that's probably not the limiting factor.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How does the problem of polysemanticity enter the picture here?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Polysemanticity is this phenomenon we observe where you look at many neurons and the neuron doesn't just sort of represent one concept, it's not a clean feature. It responds to a bunch of unrelated things. And superstition you can think of as being a hypothesis that explains the observation of polysemanticity. So polysemanticity is this observed phenomenon and superstition is a hypothesis that would explain it along with some other things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So that makes Mechinterp more difficult.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>apart. We need to somehow break that exponential space into a bunch of things, some non-exponential number of things that we can reason about independently. And the independence is crucial because it's the independence that allows you to not have to think about all the exponential combinations of things. And things being monosomatic, things only having one meaning, things having a meaning, that is the key thing that allows you to think about them independently. And so I think if you want the deepest reason why we want to have interpretable monosomatic features, I think that's really the deep reason.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And so the goal here as your recent work has been aiming at is how do we extract the monosomatic features from a neural net that has polysemantic features and all this mess.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sort of the principled thing to do, which is dictionary learning. And it turns out if you do dictionary learning in particular, if you do sort of a nice efficient way that in some sense sort of nicely regularizes that as well called a sparse auto encoder. If you train a sparse auto encoder, these beautiful interpretable features start to just fall out where there weren't any beforehand. So that's not a thing that you would necessarily predict, but it turns out that works very, very well. To me, that seems like some non-trivial validation of linear representations and superposition.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So with dictionary learning, you're not looking for particular kind of categories. You don't know what they are, they just emerge.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly. And this gets back to our earlier point when we're not making assumptions. Gradient descent is smarter than us, so we're not making assumptions about what's there. I mean, one certainly could do that, right? One could assume that there's a PHP feature and go and search for it, but we're not doing that. We're saying we don't know what's going to be there. Instead, we're just going to go and let the sparse auto encoder discover the things that are there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So can you talk toward monosematicity paper from October last year? I heard a lot of nice breakthrough results.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>interpretable features. So the Arabic feature, the Hebrew feature, the Base64 features were some examples that we studied in a lot of depth and really showed that they were what we thought they were. Turns out if you train a model twice as well and train two different models and do dictionary learning, you find analogous features in both of them. So that's fun. You find all kinds of different features. So that was really just showing that this works. And I should mention that there was this Cunningham and all that had very similar results around the same time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's something fun about doing these kinds of small scale experiments and finding that it's actually working.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sort of very hard and not going to be tractable. We'd be like, \"Well, there's this problem with supersession and it turns out supersession is really hard and we're kind of screwed, but that's not what happened. In fact, a very natural simple technique just works. And so then that's actually a very good situation. I think this is a sort of hard research problem and it's got a lot of research risk and it might still very well fail, but I think that some very significant amount of research risk was put behind us when that started to work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you describe what kind of features can be extracted in this way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the way to think about this is that the is likely about to be followed by a noun. So you could think of this as the feature, but you could also think of this as protecting a specific noun feature. And there would be these features that would fire for the in the context of say, a legal document or a mathematical document or something like this. And so maybe in the context of math, you're like the, and then predict vector or matrix, all these mathematical words, whereas in other contexts you would predict other things, that was common.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And basically we need clever humans to assign labels to what we're seeing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>next prefix. Then okay, I'm on the prefix, predict a reasonable suffix.\" And you have to alternate back and forth. So these swap layer models are really interesting. And I mean there's another thing that you might think, \"Okay, there would just be one Base64 feature, but it turns out there's actually a bunch of Base64 features because you can have English text encoded as Base64, and that has a very different distribution of Base64 tokens than regular. And there's some things about tokenization as well that it can exploit. And I don't know, there's all kinds of fun stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How difficult is the task of assigning labels to what's going on? Can this be automated by AI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, I think it depends on the feature, and it also depends on how much you trust your AI. So there's a lot of work doing automated interoperability. I think that's a really exciting direction, and we do a fair amount of automated interoperability and have Claude go and label our features.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there some fun moments where it's totally right or it's totally wrong?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, well, I think it's very common that it says something very general, which is true in some sense, but not really picking up on the specific of what's going on. So I think that's a pretty common situation. You don't know that I have a particularly amusing one.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's interesting. That little gap between it is true, but it doesn't quite get to the deep nuance of a thing. That's a general challenge, it's already an incredible caution that can say a true thing, but it's missing the depth sometimes. And in this context, it's like the ARC challenge, the sort of IQ type of tests. It feels like figuring out what a feature represents is a little puzzle you have to solve.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>verify that your neural networks are safe, the hypothesis that you're trusting for is like, \"Okay, well the neural network maybe isn't safe and you have to worry about is there some way that it could be screwing with you? I think that's not a big concern now, but I do wonder in the long run, if we have to use really powerful AI systems to go and audit our AI systems, is that actually something we can trust? But maybe I'm just rationalizing because I just want us to have to get to a point where humans understand everything.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I mean that's hilarious, especially as we talk about AI safety and looking for features that would be relevant to AI safety, like deception and so on. So let's talk about the Scaling Monosematicity paper in May 2024. Okay. So what did it take to scale this, to apply to Claude 3 Sonnet?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, a lot of GPUs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A lot more GPUs. Got it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>making the base model larger? And so it turns out this works really well and you can use it to sort of project, if you train a sparse autoencoder of a given size, how many tokens should you train on and so on. This was actually a very big help to us in scaling up this work, and made it a lot easier for us to go and train really large sparse autoencoders where it's not training the big models, but it's starting to get to a point where it's actually expensive to go and train the really big ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, you have to do all this stuff of splitting it across large CPUs-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, yeah. No, I mean there's a huge engineering challenge here too, right? Yeah. So there's a scientific question of how do you scale things effectively? And then there's an enormous amount of engineering to go and scale this up. You have to chart it, you have to think very carefully about a lot of things. I'm lucky to work with a bunch of great engineers because I am definitely not a great engineer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the infrastructure especially. Yeah, for sure. So it turns out TLDR, it worked.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sort of I think was significant evidence that even for very large models, and we did it on Claude 3 Sonnet, which at that point was one of our production models. Even these models seemed to be substantially explained, at least by linear features. And doing dictionary learning on them works, and as you learn more features, you go and you explain more and more. So that's, I think, quite a promising sign. And you find now really fascinating abstract features, and the features are also multimodal. They respond to images and texts for the same concept, which is fun.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Can you explain that? I mean, backdoor, there's just a lot of examples that you can-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. So maybe let's start with that. One example to start, which is we found some features around security vulnerabilities and backdoorsing code. So turns out those are actually two different features. So there's a security vulnerability feature, and if you force it active, Claude it will start to go and write security vulnerabilities like buffer overflows into code. And also fires for all kinds of things, some of the top data set examples where things like dash dash, disable SSL or something like this, which are sort of obviously really insecure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So at this point, maybe it's just because the examples are presented that way, it's kind of surface a little bit more obvious examples. I guess the idea is that down the line it might be able to detect more nuance like deception or bugs or that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in them. So there's a whole apparently genre of people going and selling devices that look innocuous that have hidden cameras, and they have ads that has this hidden camera in it? And I guess that is the physical version of a backdoor. And so it sort of shows you how abstract these concepts are, and I just thought that was... I'm sort of sad that there's a whole market of people selling devices like that, but I was kind of delighted that that was the thing that it came up with as the top image examples for the feature.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's nice. It's multimodal. It's multi almost context. It's broad, strong definition of a singular concept. It's nice.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To me, one of the really interesting features, especially for AI safety, is deception and lying. And the possibility that these kinds of methods could detect lying in a model, especially get smarter and smarter and smarter. Presumably that's a big threat over super intelligent model that it can deceive the people operating it as to its intentions or any of that kind of stuff. So what have you learned from detecting lying inside models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we find quite a few features related to deception and lying. There's one feature where it fires for people lying and being deceptive, and you force it active and Claude starts lying to you. So we have a deception feature. I mean, there's all kinds of other features about withholding information and not answering questions, features about power seeking and coups and stuff like that. So there's a lot of features that are kind of related to spooky things, and if you force them active Claude will behave in ways that are... they're not the kinds of behaviors you want.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are possible next exciting directions to you in the space of Mechinterp?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and I think that's the thing that I care most about. But there's lots of other sort of larger-scale questions you might care about. And the nice thing about having a very microscopic approach is it's maybe easier to ask, is this true? But the downside is its much further from the things we care about. And so we now have this ladder to climb. And I think there's a question of will we be able to find, are there larger-scale abstractions that we can use to understand neural networks that can we get up from this very microscopic approach?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You've written about this as kind of organs question.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If we think of interpretability as a kind of anatomy of neural networks, most of the circus threads involve studying tiny little veins looking at the small scale and individual neurons and how they connect. However, there are many natural questions that the small-scale approach doesn't address. In contrast, the most prominent abstractions and biological anatomy involve larger-scale structures like individual organs, like the heart or entire organ systems like the respiratory system. And so we wonder, is there a respiratory system or heart or brain region of an artificial neural network?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you might ask is, \"Why can't you just go there directly?\" And I think the answer is superstition, at least in significant part. It's that it's actually very hard to see this macroscopic structure without first sort of breaking down the microscopic structure in the right way and then studying how it connects together. But I'm hopeful that there is going to be something much larger than features and circuits and that we're going to be able to have a story that involves much bigger things. And then you can sort of study in detail the parts you care about.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I suppose, in your biology, like a psychologist or a psychiatrist of a neural network.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And I think that the beautiful thing would be if we could go and rather than having disparate fields for those two things, if you could build a bridge between them, such that you could go and have all of your higher level distractions be grounded very firmly in this very solid, more rigorous, ideally foundation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you think is the difference between the human brain, the biological neural network and the artificial neural network?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>like, \"Gosh, if it's this hard for us, it seems impossible under the constraints of neuroscience or near impossible.\" I don't know. Maybe part of me is I've got a few neuroscientists on my team, maybe I'm sort of like, \"Ah, the neuroscientists. Maybe some of them would like to have an easier problem that's still very hard, and they could come and work on neural networks. And then after we figure out things in sort of the easy little pond of trying to understand neural networks, which is still very hard, then we could go back to biological neuroscience.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love what you've written about the goal of MechInterp research as two goals, safety and beauty. So can you talk about the beauty side of things?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>all the life and ecosystems that we see around us. All the beauty of nature, that all just comes from evolution and from something very simple in evolution. And similarly, I think that neural networks build, create enormous complexity and beauty inside and structure inside themselves that people generally don't look at and don't try to understand because it's hard to understand. But I think that there is an incredibly rich structure to be discovered inside neural networks, a lot of very deep beauty if we're just willing to take the time to go and see it and understand it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I love Mechinterp. The feeling like we are understanding or getting glimpses of understanding the magic that's going on inside is really wonderful.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>not more are is how is it that we don't know how to create computer systems that can do these things? And yet we have these amazing systems that we don't know how to directly create computer programs that can do these things, but these neural networks can do all these amazing things. And it just feels like that is obviously the question that is calling out to be answered. If you have any degree of curiosity, it's like, \"How is it that humanity now has these artifacts that can do these things that we don't know how to do?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I love the image of the circus reaching towards the light of the objective function.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's this organic thing that we've grown and we have no idea what we've grown.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, thank you for working on safety, and thank you for appreciating the beauty of the things you discover. And thank you for talking today, Chris, this was wonderful.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thank you for taking the time to chat as well.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Chris Ola and before that, with Dario Amodei and Amanda Askell. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Alan Watts. \"The only way to make sense out of change is to plunge into it, move with it, and join the dance.\" Thank you for listening and hope to see you next time.<|eot_id|>"}
