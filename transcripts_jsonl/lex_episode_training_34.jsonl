{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Naively I certainly thought that all humans would have words for exact counting, and the Piraha don't. Okay, so they don't have any words for even one. There's not a word for one in their language. And so there's certainly not a word for two, three or four. And so that blows people's minds often.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that's blowing my mind.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's pretty weird, isn't it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>How are you going to ask, \"I want two of those.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You just don't. And so that's just not a thing you can possibly ask in Piraha. It's not possible, there's no words for that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with Edward Gibson, or Ted, as everybody calls him. He's a psycholinguistics professor at MIT. He heads the MIT language lab that investigates why human languages look the way they do, the relationship between cultural language and how people represent, process and learn language. Also, you should have a book titled Syntax: A Cognitive Approach, published by MIT Press coming out this fall so look out for that. This is the Lex Fridman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Edward Gibson. When did you first become fascinated with human language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>As a kid in school, when we had to structure sentences and English grammar, I found that process interesting. I found it confusing as to what it was I was told to do. I didn't understand what the theory was behind it, but I found it very interesting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you look at grammar, you're almost thinking about it like a puzzle, almost a mathematical puzzle.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>my undergraduate degree, which was computer science and math in Canada in Queen's University, I decided to go to grad school, as that's what I always thought I would do. And I went to Cambridge where they had a master's program in computational linguistics. And I hadn't taken a single language class before. All I'd taken was CS, computer science, math classes pretty much mostly as an undergrad. And I just thought this was an interesting thing to do for a year, because it was a single year program. And then I ended up spending my whole life doing it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Fundamentally, your journey through life was one of a mathematician and a computer scientist. And then you discovered the puzzle, the problem of language, and approached it from that angle to try to understand it from that angle, almost like a mathematician or maybe even an engineer.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>As an engineer, I'd say ... To be frank, I had taken an AI class, I guess it was '83 or '84, '85, somewhere in there a long time ago. And there was a natural language section in there. And it didn't impress me. I thought, \"There must be more interesting things we can do.\" It seemed just a bunch of hacks to me, it didn't seem like a real theory of things in any way. And so I just thought this seemed like an interesting area where there wasn't enough good work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Did you ever come across the philosophy angle of logic? If you think about the 80s with AI, the expert systems where you try to maybe sidestep the poetry of language and some of the syntax and the grammar and all that stuff and go to the underlying meaning that language is trying to communicate and try to somehow compress that in a computer representable way? Did you ever come across that in your studies?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I probably did but I wasn't as interested in it. I was trying to do the easier problems first, the ones I thought maybe were handleable, which seems like the syntax is easier, which is just the forms as opposed to the meaning. When you're starting talking about the meaning, that's a very hard problem and it still is a really, really hard problem. But the forms is easier. And so I thought at least figuring out the forms of human language, which sounds really hard but is actually maybe more attractable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's interesting. You think there is a big divide, there's a gap, there's a distance between form and meaning, because that's a question you have discussed a lot with LLMs because they're damn good at form.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I think that's what they're good at, is form. And that's why they're good, because they can do form, meanings are ...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think there's ... Oh, wow. It's an open question.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How close form and meaning are. We'll discuss it but to me studying form, maybe it's a romantic notion it gives you. Form is the shadow of the bigger meaning thing underlying language. Language is how we communicate ideas. We communicate with each other using language. In understanding the structure of that communication, I think you start to understand the structure of thought and the structure of meaning behind those thoughts and communication, to me. But to you, big gap.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you find most beautiful about human language? Maybe the form of human language, the expression of human language.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>so it's one or the other, it's either verb initial or verb final. But then you'll have prepositions, prepositions or post positions. And that's across the languages that we can look at. We've got around a thousand languages for ... There's around 7,000 languages around on the earth right now. But we have information about say, word order on around a thousand of those pretty. Decent amount of information. And for those thousand which we know about, about 95% fit that pattern. It's about half and half, half are verb initial like English and half are verb final like Japanese.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just to clarify, verb initial is subject-verb-object.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Verb final is still subject-object-verb.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's correct. Yeah, the subject is generally first<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's so fascinating. I ate an apple, or I apple ate.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. And it's fascinating that there's a pretty even division in the world amongst those, 45%.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>words that are connecting for their meaning are close together. It's easier for you in understanding if that's also true. If they're far away, it's hard to produce that and it's hard for you to understand. And the languages of the world, within a language and across languages fit that generalization. It turns out that having verbs initial and then having prepositions ends up making dependencies shorter. And having verbs final and having post positions ends up making dependencies shorter than if you cross them. If you cross them, you just end up ... It's possible, you can do it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mean within a language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Within a language you can do it. It just ends up with longer dependencies than if you didn't. And so languages tend to go that way, they call it harmonic. It was observed a long time ago without the explanation, by a guy called Joseph Greenberg, who's a famous typologist from Stanford. He observed a lot of generalizations about how word order works, and these are some of the harmonic generalizations that he observed.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Harmonic generalizations about word order. There's so many things I want to ask you. Okay, let me just some basics. You mentioned dependencies a few times. What do you mean by dependencies?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sentence or utterance, whatever, I'm talking to you, you're talking to me, we have a bunch of words and we're putting them together in a sequence, it turns out they are connected, so that every word is connected to just one other word in that sentence. And so you end up with what's called technically a tree, it's a tree structure, where there's a root of that utterance, of that sentence. And then there's a bunch of dependents, like branches from that root that go down to the words. The words are the leaves in this metaphor for a tree.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A tree is also a mathematical construct.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. It's graph theoretical thing, exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>A graph theory thing. It's fascinating that you can break down a sentence into a tree and then every word is hanging onto another, is depending on it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right. All linguists will agree with that, no one ...<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is not a controversial ...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That is not controversial.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's nobody sitting here listening mad at you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I do not think so, I don't think so.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. There's no linguist sitting there mad at this.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No. In every language, I think everyone agrees that all sentences are trees at some level.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can I pause on that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because to me just as a layman, it is surprising that you can break down sentences in mostly all languages.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>All languages, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... into a tree.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think so. I've never heard of anyone disagreeing with that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's weird.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The details of the trees are what people disagree about.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, okay. What's at the root of a tree? How do you construct ... How hard is it? What is the process of constructing a tree from a sentence?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you want to call that word. There's a bunch of categories of words here we're noticing. There are verbs. Those are these things that typically mark ... They refer to events and states in the world. An d they're nouns, which typically refer to people, places and things is what people say. But they can refer to events themselves as well. They're marked by the category, the part of speech of a word is how it gets used in language. That's how you decide what the category of a word is, not by the meaning but how it gets used.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How it's used. What's usually the root? Is it going to be the verb that defines the event?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Usually, usually. Yes, yes. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If I don't say a verb, then there won't be a verb. And so it'll be something else.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are we talking about language that's correct language? What if you're doing poetry and messing with stuff, then rules go out the window.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>order of the articles and the nouns. You just can't do that. In some languages, you can mess around with the order of words much more. You speak Russian, Russian has a much freer word order than English. And so in fact, you can move around words in ... I told you that English has this subject-verb-object word order, so does Russian but Russian is much freer than English. And so you can actually mess around with a word order. Probably Russian poetry is going to be quite different from English poetry because the word order is much less constrained.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. There's a much more extensive culture of poetry throughout the history of the last hundred years in Russia. And I always wondered why that is but it seems that there's more flexibility in the way the language is used. You're morphing the language easier by altering the words, altering the order of the words and messing with it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a Russian name in each of those names, you'll also say ... It'll be the nominative, which is marking the subject, or an accusative will mark the object. And you could put them in the reverse order. You could put accusative first. You could put the patient first and then the verb and then the subject. And that would be a perfectly good Russian sentence. And it would still ... I could say John kissed Mary, meaning Mary kissed John, as long as I use the case markers in the right way, you can't do that in English. And so<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love the terminology of agent and patient and the other ones you used. Those are linguistic terms, correct?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Those are for meaning, those are meaning. And subject and object are generally used for position. Subject is just the thing that comes before the verb and the object is the one that comes after the verb. The agent is the thing doing, that's what that means. The subject is often the person doing the action, the thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, this is fascinating. How hard is it to form a tree in general? Is there a procedure to it? If you look at different languages, is it supposed to be a very natural ... Is it automatable or is there some human genius involved in construction ...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's pretty automatable at this point. People can figure out the words are. They can figure out the morphemes, technically morphemes are the minimal meaning units within a language, okay. And so when you say eats or drinks, it actually has two morphemes in English. There's the root, which is the verb. And then there's some ending on it which tells you that's the third person singular.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you say what morphemes are?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Morphemes are just the minimal meaning units within a language. And then a word is just the things we put spaces between in English and they have a little bit more, they have the morphology as well. They have the endings, this inflectional morphology on the endings on the roots.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It modifies something about the word that adds additional meaning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>English we say drinks. He drinks, but everyone else is, \"I drink, you drink, we drink.\" It's unmarked in a way. But in the past tense, it's just drank for everyone. There's no morphology at all for past tense. There is morphology that's marking past tense but it's an irregular now. Drink to drank, it's not even a regular word. In many verbs there's an ED we add. Walk to walked, we add that to say it's the past tense. I just happen to choose an irregular because the high frequency words tend to have irregulars in English for ...<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's an irregular?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Irregular is just there isn't a rule. Drink to drank is an irregular.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Drink, drank. Okay, okay. Versus walked.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>As opposed to walk, walked, talk, talked.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. And there's a lot of irregulars in English.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's a lot of irregulars in English. The frequent ones, the common words tend to be irregular. There's many, many more low frequency words and those are regular ones.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The evolution of the irregulars are fascinating because it's essentially slang that's sticky because breaking the rules. And then everybody uses it and doesn't follow the rules and they say screw it to the rules. It's fascinating. You said morphemes, lots of questions. Morphology is what, the study of morphemes?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Morphology is the connections between the morphemes onto the roots. In English, we mostly have suffixes. We have endings on the words, not very much but a little bit, as opposed to prefixes. Some words depending on your language can have mostly prefixes, mostly suffixes or both. And then several languages have things called infixes, where you have some general form for the root and you put stuff in the middle, you change the vowels, stuff like that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating, that's fascinating. In general, there's what, two morphemes per word? One or two, or three.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, in English it's one or two. In English, it tends to be one or two. There can be more. In other languages, a language like Finnish which has a very elaborate morphology, there may be 10 morphemes on the end of a root, okay. And so there may be millions of forms of a given word.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, okay. I'll ask the same question over and over, just sometimes to understand things like morphemes, it's nice to just ask the question, how do these kinds of things evolve? You have a great book studying how the cognitive processing, how language used for communication, so the mathematical notion of how effective language is for communication, what role that plays in the evolution of language. But just high level, how does a language evolve where English is two morphemes or one or two morphemes per word, and then Finnish has infinity per word. How does that happen? Is it just people ...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's a really good question.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot of good solutions to the problem of communication. I believe as you hinted, that language is an invented system by humans for communicating their ideas. And I think it comes down to we label things we want to talk about. Those are the morphemes and words, those are the things we want to talk about in the world. And we invent those things and then we put them together in ways that are easy for us to convey, to process. But that's a naive view, and I don't ... I think it's probably right, it's naive and probably right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I don't know if it's naive. I think it's simple.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Simple, yeah. [inaudible 00:23:47].<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think naive is an indication that's incorrect somehow. It's trivial, too simple. I think it could very well be correct. But it's interesting how sticky ... It feels like two people got together. It just feels like once you figure out certain aspects of a language that just becomes sticky and the tribe forms around that language, maybe the tribe forms first and then the language evolves. And then you just agree and you stick to whatever that is.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sweater off the rack, not the green sweater.\" There's two. And so those things will be identical because these are things we made and they're dyed and there's nothing different about them. And so in industrialized society, everything we've got is pretty much arbitrarily colored. But you go to a non-industrialized group, that's not true. And so it's not like they're not interested in color. If you bring bright colored things to them, they like them just like we like them. Bright colors are great, they're beautiful, but they just don't need to talk about them, they don't have to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Probably color words is a good example of how language evolves from function, when you need to communicate the use of something, then you invent different variations. And basically you can imagine that the evolution of a language has to do with what the early tribe's doing, what problems are facing them and they're quickly figuring out how to efficiently communicate the solution to those problems, whether it's aesthetic or functional, all that stuff, running away from a mammoth or whatever. But I think what you're pointing to is that we don't have data on the evolution of language because many languages have formed a long time ago, so you don't get the chatter.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>see things like that. But most languages don't even have a writing system. Of the 7,000, only a small subset of those have a writing system. And even if they have a writing system, it's not a very modern writing system and so they don't have it ... For Mandarin, for Chinese, we have a lot of evidence for a long time, and for English, and not for much else. German a little bit but not for a whole lot of ... Long-term language evolution, we don't have a lot. We have snapshots, is what we've got of current languages.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You get an inkling of that from the rapid communication on certain platforms. On Reddit, there's different communities and they'll come up with different slang, usually from my perspective, driven by a little bit of humor or maybe mockery or whatever, just talking shit in different kinds of ways. And you could see the evolution of language there because I think a lot of things on the internet, you don't want to be the boring mainstream. You want to deviate from the proper way of talking. And so you get a lot of deviation, rapid deviation. Then when communities collide, you get ... Just like you said, humans adapt to it. And you could see it through the lines of humor. It's very difficult to study but you can imagine a hundred years from now, well, if there's a new language born for example, we'll get really high resolution data.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in the sounds of British English, the way she was talking was changing. The vowels were changing slightly. That's just in the sounds there was change. We're all interested in what's driving any of these changes. The word order of English changed a lot over a thousand years. It used to look like German. It used to be a verb final language with case marking. And it shifted to a vermedial language, a lot of contact, a lot of contact with French. And it became a vermedial language with no case marking. And so it became this verb initially thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's evolving.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It totally evolved. It doesn't evolve maybe very much in 20 years, is maybe what you're talking about but over 50 and 100 years, things change a lot, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We'll now have good data on it, which is great.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's for sure. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you talk to what is syntax and what is grammar? You wrote a book on syntax.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>depending on the formalization of those grammars, you might get more complicated or less complicated languages. He said, \"Well, these are things called context-free languages that rule.\" He thought human languages tend to be what he calls context-free languages. But there are simpler languages, which are so-called regular languages. And they have a more constrained form to the rules of the phrase structure of these particular rules. So he basically discovered and invented ways to describe the language. And those are phrase structure, a human language. And he was mostly interested in English initially in his work in the '50s.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So quick questions around all of this. So formal language theory is the big field of just studying language formally?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes. And it doesn't have to be human language there. We can have a computer languages, any kind of system which is generating some set of expressions in a language. And those could be the statements in a computer language, for example. It could be that, or it could be human language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So technically you can study programming languages?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes. And heavily studied using this formalism. There's a big field of programming language within the formal language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then phrase structure, grammar is this idea that you can break down language into this S NP VP type of thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the one who figured that stuff out back in the '50s. And that's equivalent actually, the context-free grammar is actually, is equivalent in the sense that it generates the same sentences as a dependency grammar would. The dependency grammar is a little simpler in some way. You just have a root and it goes... We don't have any of these, the rules are implicit, I guess. And we just have connections between words. The free structure grammar is a different way to think about the dependency grammar. It's slightly more complicated, but it's kind of the same in some ways.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To clarify, dependency grammar is the framework under which you see language and you make a case that this is a good way to describe language.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's correct.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And Noam Chomsky is watching this, is very upset right now, so let's... Just kidding. Where's the place of disagreement between phrase structure grammar and dependency grammar?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They're very close. So phrase structure grammar and dependency grammar aren't that far apart. I like dependency grammar because it's more perspicuous, it's more transparent about representing the connections between the words. It's just a little harder to see in phrase structure grammar. The place where Chomsky sort of devolved or went off from this is he also thought there was something called movement. And that's where we disagree. That's the place where I would say we disagree. I mean maybe we'll get into that later. But the idea is... Do you want me to explain that now?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I would love, can you explain movement?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Movement. Okay.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're saying so many interesting things.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>words. And so if I say, it was, \"The two dogs can or will enter the room,\" there's two forms of will. One is, will, declarative. And then, okay, I've got my subject to the left, it comes before me, and the verb comes after me in that one. And then the will, interrogative, it's like, oh, I go first. Interrogative will is first, and then I have the subject immediately after and then the verb after that. And so you can just generate from one of those words another word with a slightly different argument structure with different ordering.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And these are just lexical copies. They're not necessarily moving from one to another?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's no movement.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a romantic notion that you have one main way to use a word and then you could move it around, which is essentially what movement is implying?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but he was one of the proponents of the non-movement of the lexical copying story. And so that is that a great advantage is, well Chomsky really famously, in 1971 showed that the movement story leads to learnability problems. It leads to problems for how language is learned. It's really, really hard to figure out what the underlying structure of a language is if you have both phrase structure and movement. It's really hard to figure out what came from what. There's a lot of possibilities there. If you don't have that problem, the learning problem gets a lot easier.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just say there's lexical copies. When we say the learning problem, do you mean humans learning a new language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>all auxiliaries might move. It might depend on the word, and that turns out to be true. So there's words that don't really work as auxiliary. They work in declarative and not in interrogative. So I can say, I'll give you the opposite first. I can say, \" Aren't I invited to the party?\" And that's an interrogative form. But it's not from, \"I aren't invited to the party.\" There is no, \"I aren't.\" So that's interrogative only. And then we also have forms like, ought. \"I ought to do this.\" And I guess some old British people can say...<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"Ought I?\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Exactly. It doesn't sound right, does it? For me it sounds ridiculous. I don't even think ought is great. But I mean, I totally recognize, \"I ought to do it.\" Ought is not too bad actually. I can say, \"Ought to do this.\" That sounds pretty good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If I'm trying to sound sophisticated, maybe?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know. It just sounds completely odd to me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Ought I?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Anyway. So there are variants here. And a lot of these words just work in one versus the other. And that's fine under the lexical copying story. It's like, well, you just learn the usage, whatever the usage is, is what you do with this word. But it's a little bit harder in the movement story. The movement story, that's an advantage I think, of lexical copying. In all of these different places, there's all these usage variants which make the movement story a little bit harder to work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One of the main divisions here is the movement story versus the lexical copy story, that has to do about the auxiliary words and so on. But if you just rewind to the phrase structure grammar versus dependency grammar...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Those are equivalent in some sense in that for any dependency grammar, I can generate a phrase structure grammar, which generates exactly the same sentences. I just like the dependency grammar formalism because it makes something really salient. Which is the lengths of dependencies between words, which isn't so obvious in the phrase structure. In the phrase structure, it's just hard to see. It's in there, it's just very, very, it's opaque.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Technically, I think phrase structure grammar is mappable to dependency grammar.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And vice versa.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And vice versa. But there's these little labels, S, NP, VP.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For a particular dependency grammar you can make a phrase structure grammar, which generates exactly those same sentences and vice versa. But there are many phrase structure grammars, which you can't really make a dependency grammar. I mean, you can do a lot more in a phrase structure grammar. But you get many more of these extra nodes, basically. You can have more structure in there. And some people like that. And maybe there's value to that. I don't like it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, for you, so would you clarify. So dependency grammar is just, well, one word depends on only one other word. And you form these trees, and that makes, it really puts priority on those dependencies, just like as a tree that you can then measure the distance of the dependency from one word to the other. They can then map to the cognitive processing of the sentences, how easy it is to understand and all that kind of stuff. So it just puts the focus on just the mathematical distance of dependence between words. So it's just a different focus.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Absolutely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just to continue on the thread of Chomsky because it's really interesting. As you're discussing disagreement to the degree there's disagreement, you're also telling the history of the study of language, which is really awesome. So you mentioned context-free versus regular. Does that distinction come into play for dependency grammars?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, not at all. I mean, regular languages are too simple for human languages. It's a part of the hierarchy. But human languages are, in the phrase structure world are definite. They're at least context-free, maybe a little bit more, a little bit harder than that. So there's something called context-sensitive as well, where you can have, this is just the formal language description. In a context-free grammar, you have one... This is a bunch of formal language theory we're doing here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a right context. I have two or more stuff on the left tells you how to expand those things in that way. So it's context-sensitive. A regular language is just more constrained, and so it doesn't allow anything on the right. Basically it's one very complicated rule is what a regular language is. And so it doesn't have any... I just say the long distance dependencies, it doesn't allow recursion, for instance. There's no recursion. Yeah, recursion is where, which human languages have recursion, they have embedding. Well, it doesn't allow center-embedded recursion, which human languages have, which is what...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Center-embedded recursion, so within a sentence? Within a sentence?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I know. I listen to maybe some language I don't know, or I listen to muffled speech or I read sentences and I read non-words. I can do anything like this, anything that's really like English and anything that's not very like English. So I've got something like it, and not, that I control. And the voxels, which is just the 3D pixels in my brain that are responding most is a language area. And that's this left-lateralized area in my head. And wherever I look in that network, if you look for the combinations versus the words it's everywhere.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the same.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's the same.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a little while to get to those cells, so it takes on the order of seconds. So I talk fast, I probably listen fast. And I can probably understand things really fast. So a lot of stuff happens in two seconds. And so to say that we know what's going on, that the words right now in that network, our best guess is that whole network is doing something similar. But maybe different parts of that network are doing different things. And that's probably the case. We just don't have very good methods to figure that out at this moment.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Since we're kind of talking about the history of the study of language, what other interesting disagreements, and you're both at MIT or were for a long time. What interesting disagreements there, tension of ideas are there, between you and Noam Chomsky? And we should say that Noam was in the linguistics department, and you're, I guess for a time were affiliated there, but primarily brain and cognitive science department. Which is another way of studying language. You've been talking about fMRI. Is there something else interesting to bring to the surface about the disagreement between the two of you, or other people at this point?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>just doesn't do that. And so he has never once been associated with any experiment or corpus work ever. And so it's all thought experiments. It's his own intuitions. So I just don't think that's the way to do things. That's across the street, they're across the street from us, difference between brain and cog-sci and linguistics. I mean, not all linguists, some of the linguists, depending on what you do, more speech-oriented, they do more quantitative stuff. But in the meaning words and well, it's combinations of words, syntax, semantics, they tend not to do experiments and corpus analyses.<|eot_id|><|start_header_id|>assistant<|end_header_id|>On the linguistic side probably, well, but the method is a symptom of a bigger approach. Which is a psychology philosophy side on Noam. For you it's more data-driven, almost like mathematical approach.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I mean, I'm a psychologist, so I would say we're in psychology. Brain cognitive sciences is MIT's old psychology department. It was a psychology department up until 1985, and it became the Brain and Cognitive Science Department. My training is math and computer science, but I'm a psychologist. I mean, I don't know what I am.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, data-driven psychologist, you are.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I am what I am. But I'm happy to be called a linguist. I'm happy to be called a computer scientist. I'm happy to be called a psychologist, any of those things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But in the actual, like how that manifests itself outside of the methodology is these differences, these subtle differences about the movement story versus the lexical copy story.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Those are theories.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Those are theories.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So the theories are... But I think the reason we differ in part is because of how we evaluate the theories. And so I evaluate theories quantitatively and Noam doesn't.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Got it. Okay. Well, let's explore the theories that you explore in your book. Let's return to this dependency grammar framework of looking at language. What's a good justification why the dependency grammar framework is a good way to explain language? What's your intuition?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or something like that. So I can modify dog. If I say something more about dog after it, then what I'm doing is indirectly I'm lengthening the dependence between dog and entered, by adding more stuff to it. So just make it explicit here, if I say, \"The boy who the cat scratched, cried.\" We're going to have a mean cat here. And so what I've got here is, the boy cried, would be a very short, simple sentence. And I just told you something about the boy, and I told you it was the boy who the cat scratched.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the cry is connected to the boy. The cry at the end, it's connected to the boy in the beginning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>get lengthened out there. The dependencies get longer, on the outside ones get long, and even the ones in between get kind of long. What's fascinating is that that's bad. That's really horrendous in English. But that's horrendous in any language. No matter what language you look at, if you do, just figure out some structure where I'm going to have some modification following some head, which is connected to some later head, and I do it again, it won't be good. Guaranteed. 100% that will be uninterpretable in that language in the same way that was uninterpretable in English.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just to clarify, the distance of the dependencies is whenever the boy cried, there's a dependence between two words and then you're counting the number of what morphemes between them?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's a good question. I just say words. Your words are morphemes between, we don't know that. Actually that's a very good question. What is the distance metric? But let's just say it's words. Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. And you're saying the longer the distance of that dependence, no matter the language, except legalese.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Even legalese.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Even legalese, okay we'll talk about it. But that the people will be very upset that speak that language. Not upset, but they'll either not understand it, or they'd be like, their brain will be working in overtime.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They'll have a hard time either producing or comprehending it. They might tell you that's not their language. It's sort of their language. They'll agree with each of those pieces is part of their language, but somehow that combination will be very, very difficult to produce and understand.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that a chicken or the egg issue here?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so what the behavioral consequence there, I mean, this is kind of methods, like how do we get at this? You could try to do experiments to get people to produce these things. They're going to have a hard time producing them. You can try to do experiments to get them to understand them, and see how well they understand them. Can they understand them? Another method you can do is give people partial materials and ask them to complete them, those center embedded materials. And they'll fail. So I've done that. I've done all these kinds of things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you mean? So center embedding, meaning you can take a normal sentence like, \"The boy cried,\" and inject a bunch of crap in the middle.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That separates the boy and the cried. Okay. That's center embedding. And nesting is on top of that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Same thing. No, no, nesting is the same thing. Center embedding, those are totally equivalent terms. I'm sorry, I sometimes use one and sometimes...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Got it, got it. Totally equivalent.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They don't mean anything different.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Got it. And then what you're saying is there's a bunch of different kinds of experiments you can do. I mean, I'd like the understanding one is like have more embedding, more center embedding, is it easier or harder to understand? But then you have to measure the level of understanding, I guess?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, you could. I mean there's multiple ways to do that. I mean there's the simplest way is just ask people how good does it sound? How natural does it sound? That's a very blunt, but very good measure. It's very reliable. People will do the same thing. And so it's like, \"I don't know what it means exactly, but it's doing something.\" Such that we're measuring something about the confusion, the difficulty associated with those.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And those are giving you a signal. That's why you can say they're... Okay. What about the completion with the center embedding?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If you give them a partial sentence, say I say, \"The book, which the author who,\" and I ask you to now finish that off for me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That breaks people's brain.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of the time, they'll just leave off one of those verb phrases. Even with that simple... So say, \"The book, which the author who...\" And they'll say, \"Was...\" You need three verbs, I need three verbs. \"Who I met, wrote, was good.\" And they'll give me two. They'll say, \"Who was famous was good.\" Or something like that. They'll just give me two. And that'll happen about 60% of the time. So 40%, maybe 30%, they'll do it correctly. Correctly, meaning they'll do a three-verb phrase. I don't know what's correct or not. This is hard. It's a hard task.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, actually I'm struggling with it in my head.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, it's easier written.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you stare at it, it's easier?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>look at it and figure it out. It's easier in many dimensions, in some ways, depending on the person. It's easier to gather written data for... I mean most, I work in psycholinguistics, psychology of language and stuff. And so a lot of our work is based on written stuff because it's so easy to gather data from people doing written kinds of tasks. Spoken tasks are just more complicated to administer and analyze because people do weird things when they speak. And it's harder to analyze what they do, but they generally point to the same kinds of things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the universal theory of language by Ted Gibson is that you can form dependency, you can form trees for many sentences.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You can measure the distance in some way of those dependencies. And then you can say that most languages have very short dependencies.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>All languages.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All languages.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that it only gives you projective, non-crossed, the same thing holds. So still human languages are much shorter than this kind of a control. So what it means is that in every language, we're trying to put things close, relative to this kind of a control. It doesn't matter about the word order. Some of these are verb final. Some of them these are verb medial like English. And some are even verb initial, there are a few languages in the world which have VSO, word order verb, subject, object, languages. Haven't talked about those. It's like 10% of them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And even in those languages...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It doesn't matter.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's still short dependencies?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Short dependencies is rules.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, so what are some possible explanations for that? For why languages have evolved that way? So that's one of the, I suppose, disagreements you might have with Chomsky. So you consider the evolution of language in terms of information theory. And for you, the purpose of language is ease of communication. For you, the purpose of language is ease of communication, right, and processing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right. That's right. The story here is just about communication. It is just about production, really. It's about ease of production, is the story.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When you say production, can you-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>these words together, and it's easier for me to do that, to say something where the words are closely connected in a dependency as opposed to separated by putting something in between and over and over again. It's just hard for me to keep that in my head. That's the whole story. The story is basically the dependency grammar sort of gives that to you, just like long is bad, short is good. It's easier to keep in mind because you have to keep it in mind probably for production, probably matters in comprehension as well. Also matters in comprehension.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's on both sides of it, the production and the-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>who is it for? Is it for me the speaker, or is it for you, the listener? Part of my language is for you. The way I talk to you is going to be different from how I talk to different people, so I'm definitely angling what I'm saying to who I'm saying. It's not like I'm just talking the same way to every single person. And so I am sensitive to my audience, but does that work itself out in the dependency length differences? I don't know. Maybe that's about just the words, that part, which words I select.<|eot_id|><|start_header_id|>assistant<|end_header_id|>My initial intuition is that you optimize language for the audience.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But it's both.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's just kind of messing with my head a little bit to say that some of the optimization may be the primary objective. The optimization might be the ease of production.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We have different senses, I guess. I'm very selfish and I think it's all about me. I'm like, \"I'm just doing what's easiest for me at all times.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's easiest for me.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But I have to, of course, choose the words that I think you're going to know. I'm not going to choose words you don't know. In fact, I'm going to fix that. But maybe for the syntax, for the combinations, it's just about me. I don't know though. It's very hard to-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Wait, wait, wait. But the purpose of communication is to be understood-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Absolutely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... is to convince others and so on. So the selfish thing is to be understood. It's about the listener.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Okay. It's a little circular there too then. Okay.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. The ease of production-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Helps me be understood then. I don't think it's circular.<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, I think the primary objective is about the listener because otherwise, if you're optimizing for the ease of production, then you're not going to have any of the interesting complexity of language. You're trying to explain-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, let's control for what it is I want to say. I'm saying let's control for the thing, the message. Control for the message I want to tell you-<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that means the message needs to be understood. That's the goal.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But that's the meaning. So I'm still talking about just the form of the meaning. How do I frame the form of the meaning is all I'm talking about. You're talking about a harder thing, I think. It's like trying to change the meaning. Let's keep the meaning constant.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Got it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If you keep the meaning constant, how can I phrase whatever it is I need to say? I got to pick the right words and I'm going to pick the order so it's easy for me. That's what I think is probably-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think I'm still tying meaning and form together in my head, but you're saying if you keep the meaning of what you're saying constant, the optimization, it could be The primary objective of that optimization is for production. That's interesting. I'm struggling to keep constant meaning. I'm a human, right? So for me, without having introspected on this, the form and the meaning are tied together deeply because I'm a human. For me when I'm speaking, because I haven't thought about language in a rigorous way, about the form of language-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Dalmatian and a poodle or something. I could have been more specific or less specific about what they are, and I could have been more abstract about the number. So I am trying to keep the meaning, which is this event, constant. And then how am I going to describe that to get that to you? It kind of depends on what you need to know and what I think you need to know. But let's get control for all that stuff and I'm just choosing, but I'm doing something simpler than you're doing, which is just forms, just words.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To you, specifying the breed of dog and whether they're cute or not is changing the meaning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That might be, yeah. Well, that would be changing the meaning for sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. So you're just-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's changing the meaning. But say even if we keep that constant, we can still talk about what's easier or hard for me, the listener. Which phrase structures I use, which combinations.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is so fascinating and just a really powerful window into human language, but I wonder still throughout this how vast the gap between meaning and form. I just have this maybe romanticized notion that they're close together, that they evolve close, hand in hand. That you can't just simply optimize for one without the other being in the room with us. Well, it's kind of like an iceberg. Form is the tip of the iceberg and the rest, the meaning is the iceberg, but you can't separate.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>why these large language models are so successful, is because good at form and form isn't that hard in some sense. And meaning is tough still, and that's why they don't understand. We're going to talk about that later maybe, but we can distinguish, forget about large language models, talking humans, maybe you'll talk about that later too, is the difference between language, which is a communication system, and thinking, which is meaning. So language is a communication system for the meaning. It's not the meaning. And there's a lot of interesting evidence we can talk about relevant to that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that's a really interesting question. What is the difference between language written communicated versus thought? What to you is the difference between them?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>on this question about what's the connection between language and thought. And so she uses, I was referring earlier to MRI, fMRI, that's her primary method. And so she has been really fascinated by this question about what language is. And so as I mentioned earlier, you can localize my language area or your language area in a few minutes, like 15 minutes. I can listen to language, listen to non-language or backward speech or something, and we'll find areas left lateralized network in my head, which especially is very sensitive to language as opposed to whatever that control was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you specify what you mean by language? Like communicating language? What is language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Just sentences. I'm listening to English of any kind, a story, or I can read sentences. Anything at all that I understand, if I understand it, then it'll activate my language network.<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 01:18:42]<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>My language network is going like crazy when I'm talking and when I'm listening to you because we're communicating.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that's pretty stable.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's incredibly stable. So I happen to be married to this woman at Fedorenko, and so I've been scanned by her over and over and over since 2007 or '06 or something, and so my language network is exactly the same a month ago as it was back in 2007.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, wow.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's amazingly stable. It's astounding. It's a really fundamentally cool thing. And so my language network is like my face. Okay? It's not changing much over time inside my head.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can I ask a quick question? Sorry, it's a small tangent. At which point as you grow up from baby to adult does it stabilize?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people are bothered by it and little kids don't really like it and they don't like to lie still, and you have to be really still because if you move around, that messes up the coordinates of where everything is. And so your question is how and when are language developing? How does this left lateralized system come to play? And it's really hard to get a two-year-old to do this task, but you can maybe they're starting to get three and four and five-year-olds to do this task for short periods and it looks like it's there pretty early.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So clearly, when you lead up to a baby's first words, before that there's a lot of fascinating turmoil going on about figuring out what are these people saying and you're trying to make sense, how does that connect to the world and all that kind of stuff. That might be just fascinating development that's happening there. That's hard to introspect. But anyway-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>all. At all. There's no overlap. They're highly activated in other parts of the brain. There's a bilateral network, which I think she tends to call the multiple demands network, which does anything kind of hard. And so anything that's kind of difficult in some ways will activate that multiple demands network. Music will be in some music area, there's music specific kinds of areas, but none of them are activating the language area at all unless there's words. So if you have music and there's a song and you can hear the words, then you get the language area.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are we talking about speaking and listening or are we also talking about reading?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>This is all comprehension of any kind-<|eot_id|><|start_header_id|>assistant<|end_header_id|>That is fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>same network. And so this is about high level the comprehension of language in this case. Production's a little harder to run the scanner, but the same thing happens in production. You get the same network, so production's a little harder. You have to figure out how do you run a task in the network such that you're doing some kind of production? And I can't remember, they've done a bunch of different kinds of tasks there where you get people to produce things, figure out how to produce, and the same network goes on there exactly the same place.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Wait, wait, so if you read random words-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If you read things like-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Gibberish.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... Lewis Carroll's, \"'Twas brillig,\" Jabberwocky, right? They call that Jabberwocky speech-<|eot_id|><|start_header_id|>assistant<|end_header_id|>The network doesn't get activated.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not as much. There are words in there. There's function words and stuff, so it's lower activation.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So basically the more language it is, the higher it goes in the language network. And that network is there from when you speak, from as soon as you learn language and it's there, you speak multiple languages, the same network is going for your multiple languages. So you speak English and you speak Russian, both of them are hitting that same network if you're fluent in those languages.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Programming-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not at all. Isn't that amazing? Even if you're a really good programmer, that is not a human language. It is just not conveying the same information, and so it is not in the language network.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that as mind-blowing as I think? That's weird.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's pretty cool. It is amazing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's pretty weird.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So that's one set of data. Hers shows that what you might think is thinking is not language. Language is just this conventionalized system that we've worked out in human languages. Oh, another fascinating little tidbit is that even if they're these constructed languages like Klingon or I don't know the languages from Game of Thrones, I'm sorry, I don't remember those languages. Maybe you-<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a lot of people offended right now.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... there's people that speak those languages. They really speak those languages because the people that wrote the languages for the shows, they did an amazing job of constructing something like a human language and that lights up the language area because they can speak pretty much arbitrary thoughts in a human language. It's a constructed human language, and probably it's related to human languages because the people that were constructing them were making them like human languages in various ways, but it also activates the same network, which is pretty cool. Anyway.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sorry to go into a place where you may be a little bit philosophical, but is it possible that this area of the brain is doing some kind of translation into a deeper set of almost like concepts?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That what it has to be doing. It's doing in communication. It is translating from thought, whatever that is, it's more abstract, and that's what it's doing. That is kind of what it is doing. It's a meaning network, I guess.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, like a translation network. But I wonder what is at the core at the bottom of it? What are thoughts? Thoughts and words, are they neighbors or is it one turtle sitting on top of the other, meaning is there a deep set of concepts that we-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, there's connections between what these things mean and then there's probably other parts of the brain, but what these things mean. And so when I'm talking about whatever it is I want to talk about, it'll be represented somewhere else. That knowledge of whatever that is will be represented somewhere else.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I wonder if there's some stable-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's meaning.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... nicely compressed encoding of meanings-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... that's separate from language. I guess the implication here is that we don't think in language.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Most people have an inner voice. I don't, and so I always find this strange. So when people talk about an inner voice, I always thought this was a metaphor. And they hear, I know most of you, whoever's listening to this thinks I'm crazy now because I don't have an inner voice and I just don't know what you're listening to. It sounds so kind of annoying to me, to have this voice going on while you're thinking, but I guess most people have that, and I don't have that and we don't really know what that connects to.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I wonder if the inner voice activates that same network. I wonder.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know. This could be speechy, right? So that's like you hear. Do you have an inner voice?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't think so.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh. A lot of people have this sense that they hear themselves and then say they read someone's email, I've heard people tell me that they hear that this other person's voice when they read other people's emails and I'm like, \"Wow, that sounds so disruptive.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>I do think I vocalize what I'm reading, but I don't think I hear a voice.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, you probably don't have an inner voice.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't think I have an inner voice.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>People have an inner voice. People have this strong percept of hearing sound in their heads when they're just thinking.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I refuse to believe that's the majority of people.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Majority, absolutely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's like two-thirds or three-quarters. It's a lot. Whenever I ask the class and I went internet, they always say that. So you're in a minority.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It could be a self-report flaw.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It could be.<|eot_id|><|start_header_id|>assistant<|end_header_id|>When I'm reading inside my head, I'm kind of saying the words, which is probably the wrong way to read, but I don't hear a voice. There's no percept of a voice. I refuse to believe the majority of people have it. Anyway, the human brain is fascinating, but it still blew my mind that that language does appear, comprehension does appear to be separate from thinking.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a conventionalized language system, and so math isn't there. And so they can do math. They do just as well as their control age matching controls and all these tasks. This is Rosemary Varley over in University College of London who has a bunch of patients who she's shown this. So that sort of combination suggests that language isn't necessary for thinking. It doesn't mean you can't think in language. You could think in language because language allows a lot of expression, but it's just you don't need it for thinking. It suggests that language is a separate system from-<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is kind of blowing my mind right now.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's cool, isn't it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm trying to load that in because it has implications for large language models.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It sure does, and they've been working on that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, let's take a stroll there. You wrote that the best current theories of human language are arguably large language models, so this has to do with form.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's a kind of a big theory, but the reason it's arguably the best is that it does the best at predicting what's English, for instance. It's incredibly good, better than any other theory, but there's not enough detail.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, it's opaque. You don't know what's going on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You don't know what's going on.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Black box.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's in a black box. But I think it is a theory.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's your definition of a theory? Because it's a gigantic black box with a very large number of parameters controlling it. To me, theory usually requires a simplicity, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, I don't know, maybe I'm just being loose there. I think it's not a great theory, but it's a theory. It's a good theory in one sense in that it covers all the data. Anything you want to say in English, it does. And so that's how it's arguably the best, is that no other theory is as good as a large language model in predicting exactly what's good and what's bad in English. Now, you're saying is it a good theory? Well, probably not because I want a smaller theory than that. It's too big, I agree.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You could probably construct mechanism by which it can generate a simple explanation of a particular language, like a set of rules. Something like it could generate a dependency grammar for a language, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You could probably just ask it about itself.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so there's work from a guy called Chris Manning and colleagues over at Stanford in natural language. And they looked at I don't know how many large language model types, but certainly BERT and some others, where you do some kind of fancy math to figure out exactly what kind of abstractions of representations are going on, and they were saying it does look like dependency structure is what they're constructing. It's actually a very, very good map, so they are constructing something like that. Does it mean that they're using that for meaning? Probably, but we don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You write that the kinds of theories of language that LLMs are closest to are called construction-based theories. Can you explain what construction-based theories are?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>idea. What's a construction? Construction's either a simple word, so a morpheme plus its meaning or a combination of words. It's basically combinations of words, the rules, but it's unspecified as to what the form of the grammar is underlyingly. And so I would argue that the dependency grammar is maybe the right form to use for the types of construction grammar. Construction grammar typically isn't formalized quite, and so maybe the a formalization of that, it might be in dependency grammar. I would think so, but it's up to other researchers in that area if they agree or not.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think that large language models understand language? Are they mimicking language? I guess the deeper question there is, are they just understanding the surface form or do they understand something deeper about the meaning that then generates the form?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>goes through the forms that it's seen before so many times on these cases where yes, you should trade because your odds have shifted from one in three now to two out of three to being that thing. It doesn't have any way to remember that actually, you have 100% probability behind that door number one. You know that. That's not part of the scheme that it's seen hundreds and hundreds of times before. And so even if you try to explain to it that it's wrong, that they can't do that, it'll just keep giving you back the problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But it's also possible the larger language model would be aware of the fact that there's sometimes over-representation of a particular kind of formulation, and it's easy to get tricked by that. And so you could see if they get larger and larger, models be a little bit more skeptical, so you see over-representation. It just feels like training on form can go really far in terms of being able to generate things that look like the thing understands deeply the underlying world model, of the kind of mathematical world, physical world, psychological world that would generate these kinds of sentences. It just feels like you're creeping close to the meaning part, easily fooled, all this kind of stuff, but that's humans too. So it just seems really impressive how often it seems like it understands concepts.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You don't have to convince me of that. I am very, very impressed. You're giving a possible world where maybe someone's going to train some other versions such that it'll be somehow abstracting away from types of forms, I don't think that's happened. And so-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, no, no, no, I'm not saying that. I think when you just look at anecdotal examples and just showing a large number of them where it doesn't seem to understand and it's easily fooled, that does not seem like a scientific data-driven analysis of how many places is it damn impressive in terms of meaning and understanding and how many places is easily fooled?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's not the inference, so I don't want to make that. The inference I wouldn't want to make was that inference. The inference I'm trying to push is just that is it like humans here? It's probably not like humans here, it's different. So humans don't make that error. If you explain that to them, they're not going to make that error. They don't make that error. And so it's doing something different from humans that they're doing. In that case,<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the mechanism by which humans figure out that it's an error?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm just saying the error there is if I explained to you there's a hundred percent chance that the car is behind this door, do you want to trade people say no, but this thing will say yes because it's that trick, it's so wound up on the form. That's an error that a human doesn't make, which is kind of interesting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Less likely to make, I should say.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Less likely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because you're asking a system to understand 100%, you're asking some mathematical concepts.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But the places where language models are, the form is amazing. So let's go back to nested structure, center-embedded structures. If you ask a human to complete those, they can't do it. Neither can a large language model. They're just like humans in that. If I ask a large language model-<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating, by the way. The central embedding struggles with anyone-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this door? Oh, this is the thing I want,\" humans don't mess that up as much. Here, the form is just like. The form of the match is amazingly similar without being trained to do that. It's trained in the sense that it's getting lots of data, which is just like human data, but it's not being trained on bad sentences and being told what's bad. It just can't do those. It'll actually say things like, \"Those are too hard for me to complete or something,\" which is kind of interesting, actually. How does it know that? I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But it really often doesn't just complete, it very often says stuff that's true and sometimes says stuff that's not true. And almost always the form is great, but it's still very surprising that with really great form, it's able to generate a lot of things that are true based on what it's trained on and so on. So it's not just form that is generating, it's mimicking true statements-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right, that's right. I think that's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... from the internet. I guess the underlying idea there is that on the internet, truth is overrepresented versus falsehoods.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think that's probably right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the fundamental thing it's trained on you're saying is just form, and it's really-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think so.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, to me, that's still a little bit of an open question. I probably lean agreeing with you, especially now you've just blown my mind that there's a separate module in the brain for language versus thinking. Maybe there's a fundamental part missing from the large language model approach that lacks the thinking, the reasoning capability.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, that's what this group argues. So the same group, Fedorenko's group has a recent paper arguing exactly that. There's a guy called Kyle Mahowald who's here in Austin, Texas, actually. He's an old student of mine, but he's a faculty in linguistics at Texas, and he was the first author on that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating. Still to me, an open question. What to you are the interesting limits of LLMs?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't see any limits to their form. Their form is perfect.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Impressive, perfect.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's pretty close to being-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, you said ability to complete central embeddings.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. It's just the same as humans. It seems the same as humans.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that's not perfect, right? It should be able to-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's good. No, but I want it to be like humans. I want a model of humans.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, wait, wait. Oh, so perfect to you is as close to humans as possible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I got it. But if you're not human, you're superhuman, you should be able to complete central embedded sentences, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The mechanism is, if it's modeling, I think it's kind of really interesting that it can't.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That is really interesting.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's potentially underlying modeling something like the way the form is processed.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The form of human language and how humans process the language.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes. I think that's plausible.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And how they generate language, process language and generate language. That's fascinating. So in that sense, they're perfect. If we can just linger on the center embedding thing, that's hard for LLMs to produce and that seems really impressive because hard for humans to produce. And how does that connect to the thing we've been talking about before, which is the dependency grammar framework in which you view language, and the finding that short dependencies seem to be a universal part of language? So why is it hard to complete center embeddings?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So what I like about dependency grammar is it makes the cognitive cost associated with longer distance connections very transparent. Basically, it turns out there is a cost associated with producing and comprehending connections between words, which are just not beside each other. The further apart they are, the worse it is. We can measure that and there is a cost associated with that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you just linger on what do you mean by cognitive cost and how do you measure it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>brain here. We could look at the language network. We could look at the language network and we could look at the activation in the language network and how big the activation is, depending on the length of the dependencies. It turns out in just random sentences that you're listening to, if you listen, as it turns out there are people listening to stories here. The longer the dependency is, the stronger the activation in the language network. So, there's some measure... There's a bunch of different measures we could do. That's kind of a neat measure actually of actual-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Activation.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... activation in the brain.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So then you can somehow in different ways convert it to a number. I wonder if there's a beautiful equation connecting cognitive costs and length of dependency. E equals MC squared kind of thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>figure out the cost. It's complicated. Another issue you raised before was how do you measure distance? Is it words? It probably isn't, is part of the problem, is that some words matter than more than others meaning nouns might matter. And then it maybe depends on which kind of noun. Is it a noun we've already introduced or a noun that's already been mentioned? Is it a pronoun versus a name? All these things probably matter. So, probably the simplest thing to do is just like, oh, let's forget about all that and just think about words or morphemes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>For sure. But there might be some insight in the kind of function that fits the data. Meaning like quadratic... What-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it's an exponential.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exponential.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>ago. So you've got a bunch of dependencies. If you've got a bunch of them that are being connected at some point, at the ends of those, the cost is some exponential function of those is my guess. But because the reason it's probably an exponential is it's not just the distance between two words. I can make a very, very long subject verb dependency by adding lots and lots of noun phrases and prepositional phrases and it doesn't matter too much. It's when you do nested, when I have multiple of these, then things go really bad, go south.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's probably somehow connected to working memory or something like this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>theory, which is just, let's count words, it's not right, but it's close. Then maybe you're right though. There's some sort of an exponential or something to figure out the total so we can figure out a function for any given sentence in any given language. But it's funny, people haven't done that too much, which I do think is... I'm interested that you find that interesting. I really find that interesting and a lot of people haven't found it interesting. I don't know why I haven't got people to want to work on that. I really like that too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a beautiful idea, and the underlying idea is beautiful, that there's a cognitive cost that correlates with the length of dependency. It feels like, I mean, language is so fundamental to the human experience. This is a nice, clean theory of language where it's like, \"Wow, okay, so we like our words close together, depend words close together.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, that's why I like it too. It's so simple. It's so simple.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, the simplicity of the theory is good.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And yet it explains some very complicated phenomena. If I write these very complicated sentences, it's kind of hard to know why they're so hard. And you can like, oh, nail it down. I can give you a math formula for why each one of them is bad and where, and that's kind of cool. I think that's very neat.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Have you gone through the process... Is there, if you take a piece of text and then simplify, there's an average length of dependency and then you reduce it and see comprehension on the entire, not just a single sentence, but you go from James Joyce to Hemingway or something.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, no. Simple answer is no. There's probably things you can do in that kind of direction.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fun.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We're going to talk about legalese at some point, and so maybe we'll talk about that kind of thinking with applied to legalese.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let's talk about legalese because you mentioned that as an exception. We're just taking a tangent upon tangent, that's an interesting one, you give it as an exception.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's an exception.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That you say that most natural languages, as we've been talking about, have local dependencies with one exception, legalese.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, what is legalese first of all?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, well, legalese is what you think it is. It's just any legal language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I actually know very little about the kind of language that lawyers use.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So, I'm just thinking about language in laws and language and contracts.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Got it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's the most common thing that humans have, that adults in our industrialized society have to deal with a lot. That's what we pulled and we didn't know what was hard about them, but it turns out that the way they're written is very center embedded. It has nested structures in them. So, it has low frequency words as well. That's not surprising. Lots of texts have low... It does have surprising slightly lower frequency words than other kinds of control texts, even academic texts. Legalese is even worse. It is the worst that we were being able to find-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Fascinating. You just reveal the game that lawyers are playing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They're not though.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That they're optimizing a different... Well-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's interesting. Now you're getting at why. So, now you're saying they're doing intentionally. I don't think they're doing intentionally, but let's-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's an emergent phenomena. Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>can think of, including, you think, people think, oh, technical, academic texts. No, people don't write center embedded sentences in technical academic texts. They do a little bit, but it's on the 20%, 30% realm as opposed to 70. So, there's that and there's low frequency words. Then people, oh, maybe it's passive. People don't like the passive. Passive for some reason, the passive voice in English has a bad rap, and I'm not really sure where that comes from. And there is a lot of passive, there's much more passive voice in legalese than there is in other texts-<|eot_id|><|start_header_id|>assistant<|end_header_id|>And passive voice accounts for some of the low frequency words?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, no. Those separate. Those are separate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 01:54:28] I apologize. Oh, so passive voice sucks. Low frequency words sucks.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well sucks is different. So, these are different-<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a judgment I'm passing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah, yeah. Drop the judgment. It's just like, these are frequent. These are things which happen in legalese text. Then we can ask, the dependent measure is how well you understand those things with those features. And it turns out the passive makes no difference. So, it has zero effect on your comprehension ability, on your recall ability. Nothing at all, it has no effect. The words matter a little bit. Low frequency words are going to hurt you in recall and understanding, but what really hurts is the center of embedding.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Center embedding.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>process it just as well as if it was normal. No, no, they're much better than laypeople. So, they can much better recall, much better at understanding. But they have the same main effects as laypeople. Exactly the same. So, they also much prefer the non-center... So, we constructed non-center embedded versions of each of these. We constructed versions which have higher frequency words in those places. And we did, we un-passivized, we turned them into active versions. The passive/active made no difference. The words made a little difference. And the un-center embedding makes big differences in all the populations.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Un-center embedding. How hard is that process, by the way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not very hard.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sorry, dumb question, but how hard is it to detect center embedding?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh, easy. Easy to detect. [inaudible 01:56:21]-<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're just looking at long dependencies or is there a real-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So, there's automatic parsers for English, which are pretty good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And they can detect center embedding?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh yeah, very good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Or I guess nesting?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Perfectly. [inaudible 01:56:32]. Pretty much.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, you're not just looking for long dependencies, you're just literally looking for center embedding.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, we are in this case, in these cases. But long dependencies, they're highly correlated, these things to this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. So, like center embedding is a big bomb you throw inside of a sentence that just blows out, that makes super-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Can I read a sentence for you from these things?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'll see if I can find. I mean, this is just one of the things that... This is just [inaudible 01:56:53]-<|eot_id|><|start_header_id|>assistant<|end_header_id|>My eyes might glaze over in mid-sentence. No, I understand that. I mean, legalese is hard.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Here we go. It goes, \"In the event that any payment or benefit by the company, all such payments and benefits, including the payments and benefits under section 3(A) hereof being here and after referred to as a total payments would be subject to the excise tax, then the cash severance payments shall be reduced.\" So that's something we pulled from a regular text from a contract.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Wow.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>opposed to in the middle of the sentence? That's very, very common, by the way. That's what happens. You just throw your definitions, you use a word, a couple of words, and then you define it and then you continue the sentence. Just don't write like that. So then we asked lawyers, we thought, \"Oh, maybe lawyers like this.\" Lawyers don't like this, they don't like this. They don't want to write like this. We asked them to rate materials which are with the same meaning, with un-center embedded and center embedded, and they much preferred the un-center embedded versions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>On comprehension, on the reading side.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And we asked them, \"Would you hire someone who writes like this or this?\" We asked them all kinds of questions and they always preferred the less complicated version, all of them. So, I don't even think they want it this way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But how did it happen?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>How did it happen? That's a very good question. And the answer is, I still don't know, but-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I have some theories.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>center embedding and the style, which tells you it's legalese. We think that that's the kind of a style which tells you it's legalese, it's a reasonable guess. And maybe it's just, so for instance, it's like a magic spell. We kind of call this the magic spell hypothesis. When you tell someone to put a magic spell on someone, what do you do? People know what a magic spell is and they do a lot of rhyming. That's kind of what people will tend to do. They'll do rhyming and they'll do some kind of poetry kind of thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Abracadabra type of thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. And maybe there's a syntactic reflex here of a magic spell, which is center embedding. And so that's like, \"Oh, it's trying to tell you this is something which is true,\" which is what the goal of law is. It's telling you something that we want you to believe is certainly true. That's what legal contracts are trying to enforce on you. And so maybe that's a form which has... This is like an abstract, very abstract form center embedding, which has a meaning associated with it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, don't you think there's an incentive for lawyers to generate things that are hard to understand?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That was one of our working hypotheses. We just couldn't find any evidence of that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, lawyers also don't understand it, but you're creating space-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But when you ask lawyers-<|eot_id|><|start_header_id|>assistant<|end_header_id|>You ask in a communist Soviet Union, the individual members, their self-report is not going to correctly reflect what is broken about the gigantic bureaucracy then leads to Chernobyl or something like this. I think the incentives under which you operate are not always transparent to the members within that system. So, it just feels like a strange coincidence that there is benefit if you just zoom out, look at the system as opposed to asking individual lawyers, that making something hard to understand is going to make a lot of people money. You're going to need a lawyer to figure that out, I guess from the perspective of the individual. But then that could be the performative aspect. It could be as opposed to the incentive driven to be complicated. It could be performative to where, \"We lawyers speak in this sophisticated way and you regular humans don't understand it, so you need to hire a lawyer.\" Yeah, I don't know which one it is, but it's suspicious. Suspicious that it's hard to understand and that everybody's eyes glaze over and they don't read.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm suspicious as well. I'm still suspicious and I hear what you're saying. It could be kind, no individual and even average of individuals. It could just be a few bad apples in a way which are driving the effect in some way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Influential bad apples that everybody looks up to or whatever, they're central figures in how-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But it is kind of interesting that-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... among our hundred lawyers, they did not share that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>They didn't want this. That's fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They really didn't like it. And so it gave us hope-<|eot_id|><|start_header_id|>assistant<|end_header_id|>And they weren't better than regular people at comprehending it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They were much-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Or they were on average better-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But they had the same difference.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... but the same difference.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the subject verb relation in that particular example, and that's pretty general. What they're doing is just throwing stuff in there, which you didn't have to put in there. There's extra words involved, typically. You may need a few extra words to refer to the things that you're defining outside in some way. If you only use it in that one sentence, then there's no reason to introduce extra terms. So, we might have a few more words, but it'll be easier to understand. I have hope that now that maybe we can make legalese less convoluted in this way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, maybe the next President of the United States can, instead of saying generic things, say-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Say exactly-<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"I ban center embeddings, and make Ted the language czar of-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, make Eric. Martinez is the guy you should really put in there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Eric Martinez, yeah. But center embeddings are the bad thing to have.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you get rid of that-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That'll do a lot of it. That'll fix a lot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating. That is so fascinating. And just really fascinating on many fronts that humans are just not able to deal with this kind of thing and that language because of that evolved in the way you did. It's fascinating. So, one of the mathematical formulations you have when talking about languages communication is this idea of noisy channels. What's a noisy channel?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And he was concerned or interested in what was a robust way to do that. And so that assuming we both speak the same language, we both already speak English, whatever the language is, we speak that what is a way that I can say the language so that it's most likely to get the signal that I want to you. And then the problem there in the communication is the noisy channel, is that I make... There's a lot of noise in the system. I don't speak perfectly. I make errors. That's noise. There's background noise, you know that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Like literal.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and this was his master's thesis of all things. It's crazy how much he did for his master's thesis in 1948 I think, or '49 something. And he wanted to keep working in language and it just wasn't a popular communication as a reason source for what language was, wasn't popular at the time. So, Chomsky was moving in there and he just wasn't able to get a handle there, I think. And so he moved to [inaudible 02:06:04] and worked on communication from a mathematical point of view and did all kinds of amazing work. And so he's just-<|eot_id|><|start_header_id|>assistant<|end_header_id|>More on the signal side versus the language side.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It would've been interesting to see if he pursued the language side. That's really interesting.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>He was interested in that. His examples in the '40s are like, they're very language-like things. We can kind of show that there's a noisy channel process going on in, when you're listening to me, you can often guess what you think I meant, given what I said. And I mean, with respect to sort of why language looks the way it does, there might be, as I alluded to, there might be ways in which word orders is somewhat optimized because of the noisy channel in some way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's really cool to model if you don't hear certain parts of a sentence or have some probability of missing that part, how do you construct a language that's resilient to that, that's somewhat robust to that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, that's the idea.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then you're kind saying the word order and the syntax of language, the dependency length are all helpful to do-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>can't see how languages change or anything because we've got these snapshots of a few hundred or a few thousand languages. We can't do the right kinds of modifications to test things experimentally. And so just take this with a grain of salt from here, this stuff. The dependency stuff, I'm much more solid on and here's what the lengths are and here's what's hard, here's what's easy, and this is a reasonable structure. I think I'm pretty reasonable. Why does the word order look the way it does is, we're now into shaky territory, but it's kind of cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We're talking about, just to be clear, we're talking about maybe just actually the sounds of communication. You and I are sitting in a bar, it's very loud, and you model with a noisy channel, the loudness, the noise, and we have the signal that's coming across and you're saying word to order might have something to do with optimizing that when there's presence of noise.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's really interesting. To me, it's interesting how much you can load into the noisy channel. How much can you bake in? You said cognitive load on the receiver end-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We think that there's at least three different kinds of things going on there. We probably don't want to treat them all as the same. And so I think that the right model, a better model of a noisy channel would have three different sources of noise, which are background noise, speaker-inherent noise and listener-inherent noise. And those are all different things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure. But then underneath it, there's a million other subsets of what-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's true.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... on the receiving, I just mentioned cognitive load on both sides. Then there's speech impediments or just everything, worldview. The meaning. We'll start to creep into the meaning realm of we have different worldviews.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, how about just form still though? Just what language you know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 02:09:35].<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>So, how well you know the language, and so if it's second language for you versus first language and how maybe what other languages you know. These are still just form stuff and that's potentially very informative. And how old you are, these things probably matter. So, child learning a language is as a noisy representation of English grammar, depending on how old they are. Maybe when they're six, they're perfectly formed, but...<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mentioned one of the things is a way to measure a language is learning problems. So, what's the correlation between everything we've been talking about and how easy it's to learn a language? Is a short dependencies correlated to ability to learn a language? Is there some kind of... Or the dependency grammar, is there some kind of connection there? How easy it is to learn?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, all the languages in the world's language, none is right now we know is any better than any other with respect to optimizing dependency lengths, for example. They're all kind of do it, do it well. They all keep low. So, I think of every human language as some kind of an optimization problem, a complex optimization problem to this communication problem. And so they've solved it. They're just noisy solutions to this problem of communication. There's just so many ways you can do this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, they're not optimized for learning. They're probably optimized for communication.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And learning. So yes, one of the factors which-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Uh-oh.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that are connected right beside each other. But we don't do that. There are word order rules. And depending on the language, they're more and less strict. So, you speak Russian, they're less strict than English. English is very rigid word order rules. We order things in a very particular way. And so why do we do that? That's probably not about communication. That's probably about learning. Then we're talking about learning. It's probably easier to learn regular things, things which are very predictable and easy. So, that's probably about learning is our guess because that can't be about communication.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can it be just noise? Can it be just the messiness of the development of a language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If it were just a communication, then we should have languages which have very, very free word order. And we don't have that. We have free-er, but not free. There's always-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, no, but what I mean by noise is cultural, sticky cultural things. Like the way you communicate, there's a stickiness to it, that it's an imperfect, it's a noisy optimist... Stochastic, the function over which you're optimizing is very noisy. Because it feels weird to say that learning is part of the objective function, because some languages are way harder to learn than others. Or that's not true?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's not true.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's interesting.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I mean, yes-<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's the public perception, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Right? Yes, that's true for a second language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>For second language, [inaudible 02:13:13]-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But that depends on what you started with, right? So, it really depends on how close that second language is to the first language you've got. And so yes, it's very, very hard to learn Arabic if you've started with English or it's hard to learn Japanese or if you've started with... Chinese, I think is the worst. There's Defense Language Institute in the United States has a list of how hard it is to learn what language from English, I think Chinese is the worse-<|eot_id|><|start_header_id|>assistant<|end_header_id|>But this is the second language. You're saying babies don't care.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No. There's no evidence that there's anything harder or easy about any language learned, by three or four, they speak that language. So, there's no evidence of anything hard or easy about human language. They're all kind of equal.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To what degree is language, this is returning to Chomsky a little bit, is innate. You said that for Chomsky, you used the idea that language is, some aspects of language are innate to explain away certain things that are observed. How much are we born with language at the core of our mind brain?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>if you just throw out the movement and just think about that in a different way, then you get some messiness. But the messiness is human language, which it actually fits better. That messiness isn't a problem. It's actually, it's a valuable asset of the theory. And so I think I don't really see a reason to postulate much innate structure. And that's kind of why I think these large language models are learning so well is because I think you can learn the form, the forms of human language from the input. I think that's likely to be true.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So, that part of the brain that lights up when you're doing all the comprehension, that could be learned? That could be just, you don't need any-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a section of her brain and was scanned in her lab, and she happened to be a writer for the New York Times. And there was an article in New York Times just about the scanning procedure, about what might be learned by the general process of MRI and language, not necessarily language. And because she's writing for the New York Times, then all these people started writing to her who also have similar kinds of deficits because they've been accidentally scanned for some reason and found out they're missing some section. And they say they volunteer to be scanned.<|eot_id|><|start_header_id|>assistant<|end_header_id|>These are natural experiments, you said?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Natural experiments. They're kind of messy, but natural experiments, it's kind of cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The brain.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>She calls it Interesting Brains.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first few hours, days, months of human life are fascinating. Well, inside the womb actually, that development, that machinery, whatever that is, seems to create powerful humans that are able to speak, comprehend, think, all that kind of stuff, no matter what happens... Not no matter what, but robust to the different ways that the brain might be damaged and so on. That's really interesting. But what would Chomsky say about the fact, the thing you're saying now, that language seems to be happening separate from thought? Because as far as I understand, maybe you can correct me, he thought that language underpins a thought.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, he thinks so. I don't know what he'd say.<|eot_id|><|start_header_id|>assistant<|end_header_id|>He would be surprised, because for him, the idea is that language is the foundation of thought.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right. Absolutely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's pretty mind-blowing to think that it could be completely separate from thought.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>say that the language network is doing any of these tasks. There's- The language network is doing any of these tasks, you should look at those data. It's like there's no chance that you can say that those networks are overlapping. They're not overlapping, they're just completely different. And so you can always make, oh, it's only two people, it's four people or something for the patients, and there's something special about them we don't know. But these are just random people and with lots of them, and you find always the same effects and it's very robust, I'd say.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that's a fascinating effect. You mentioned Bolivia. What's the connection between culture and language? You've also mentioned that much of our study of language comes from W-E-I-R-D, WEIRD people, western, educated, industrialized rich, and democratic. So when you study remote cultures such as around the Amazon jungle, what can you learn about language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>something in my backyard. So it's not big field farming, it's just farming for a family. A few things you do that. So that's the kind of farming they do. And the other group I've worked with are the Piraha, which are also in the Amazon and happened to be in Brazil. And that's with a guy called Dan Everett, who was a linguist anthropologist who actually lived and worked in the, I mean, he was a missionary actually, initially back in the seventies working with trying to translate languages so they could teach them the Bible, teach them Christianity.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What can you say about that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>for teal or a word for aquamarine in the Amazon jungle for the most part because I don't have two things which differ on those colors. I just don't have that. And so numbers are really another fascinating source of information here where you might naively, I certainly thought that all humans would have words for exact counting, and the Piraha don't. Okay, so they don't have any words for even one. There's not a word for one in their language. And so there's certainly not word for two, three or four. So that kind of blows people's minds often.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that's blowing my mind.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's pretty weird, isn't it?<|eot_id|><|start_header_id|>assistant<|end_header_id|>How are you going to ask, I want two of those?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we had a set of objects. These happen to be spools of thread. It doesn't really matter what they are, identical objects, and I sort of start off here. I just give you one of those and say, what's that? Okay, so you're a Piraha speaker and you tell me what it is, and then I give you two and say, what's that? And nothing's changing in the set except for the number. And then I just ask you to label these things. And we just do this for a bunch of different people. And frankly, I did this task.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do exactly the same labeling for one up. And then we just do the same thing down on random order. Actually, we do some of them up, some of them down first, instead of one to 10, we do 10 down to one. I give them 10, 9, at 8, they start saying the word for some. And then when you get to four, everyone is saying the word for few, which we thought was one. So the context determined what that quantifier they used was. So it's not a count word. They're not count words, they're just approximate words-<|eot_id|><|start_header_id|>assistant<|end_header_id|>And they're going to be noisy when you interview a bunch of people, the definition of few. And there's going to be a threshold in the context.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I don't know what that means. That's going to depend on the context. I think that's true in English too. If you ask an English person what a few is, I mean, that's depend on the context.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And it might actually be at first hard to discover because for a lot of people, the jump from one to two will be few. Right? So it's the jump.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it might be still be there. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean that's fascinating. That's fascinating. The numbers don't present themselves.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then they would do that just perfectly. And so we'd move it up. We'd do some sort of random number of items up to 10, and they basically do perfectly on that. They'd never get that wrong. I mean, that's not a counting task that is just a match. You just put one against it doesn't matter how many, I don't need to know how many there are there to do that correctly. And they would make mistakes, but very, very few and no more than MIT undergrads, just going to say, these are low stakes. So you make mistakes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Counting is not required to complete the matching class.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>interesting is so then we do a bunch of tasks where you need some way to encode the set. So one of them is just, I just put a opaque sheet in front of the things. I put down a bunch, a set of these things, and I put an opaque sheet down. And so you can't see them anymore. And I tell you, do the same thing you were doing before. It's easy if it's two or three, it's very easy, but if I don't have the words for eight, it's a little harder maybe with practice. Well, no.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because you have to count-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They don't have words for this thing. And so they would do approximate. It's totally fascinating. So they would get them approximately right after four or five, because basically you always get four right, three or four that looks, that's something we can visually see. But after that, you have its approximate number. And there's a bunch of tasks we did, and they all failed. I mean, failed. They did approximate after five on all those tasks. And it kind of shows that the words, you kind of need the words to be able to do these kinds of tasks.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But there's a little bit of a chicken and egg thing there, because if you don't have the words, then maybe they'll limit you in the kind of a little baby Einstein there won't be able to come up with a counting task. You know what I mean? The ability to count enables you to come up with interesting things probably. So yes, you develop counting because you need it, but then once you have counting, you can probably come up with a bunch of different inventions, how to, I don't know what kind of thing they do matching really well for building purposes, building some kind of hut or something like this. So it's interesting that language is a limiter on what you're able to do.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, language is the words. Here is the words. The words for exact count is the limiting factor here. They just don't have them in this-<|eot_id|><|start_header_id|>assistant<|end_header_id|>But that's what I mean. That limit is also a limit on the society of what they're able to build.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>well, don't these people have... Don't they have kids? They have a lot of children. I'm like, yeah, they have a lot of children. And they do. They often have families of three or four, five kids, and they go, well, they need the numbers to keep track of their kids. And I always ask this person who says this, do you have children? And the answer is always no, because that's not how you keep track of your kids. You care about their identities. It's very important to me when I go, I have five children, it doesn't matter.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You don't think one, two, three, four?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It matters which five. If you replaced one with someone else, I would care. A goat, maybe not. Right? That's the kind of point. It's an abstraction. Something that looks very similar to the one wouldn't matter to me probably.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But, if you care about goats, you're going to know them actually individually also.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, you will.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, cows, goats, if it's a source of food and milk and all that kind stuff. You're going to actually care-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, yeah, yeah. You're actually, you're absolutely right. But I'm saying it is an abstraction such that you don't have to care about their identities to do this thing fast. That's the hypothesis, not mine from anthropologists are guessing about where words for counting came from is from farming maybe. Any way...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. Do you have a sense why universal languages like Esperanto have not taken off? Why do we have all these different languages?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and then you make money. And so they want Spanish things. And so Moseten is in danger and is dying, and that's normal. Basically, the problem is that people, the reason we learn language is to communicate. We use it to make money and to do whatever it is to feed our families. If that's not happening, then it won't take off. It's not like a game or something. This is something we, why is English so popular? It's not because it's an easy language to learn. Maybe it is, I don't really know. But that's not why it's popular.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But because the United States is gigantic economy therefore-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, it's big economies that do this. It's all it is. It's all about money. And that's what, so there's a motivation to learn Mandarin. There's a motivation to learn Spanish. There's a motivation to learn English. These languages are very valuable to know because there's so many speakers all over the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's fascinating.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's less of a value economically. It's kind of what drives this, it's not just for fun. I mean, there are these groups that do want to learn language just for language's sake, and there's something to that. But those are rarities in general. Those are a few small groups that do that. Not most people don't do that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, if that was a primary driver, then everybody was speaking English or speaking one language. There's also a tension-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's happening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We're moving towards fewer and fewer languages. Exactly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We are. I wonder if, you're right. Maybe, this is slow, but maybe that's where we're moving, but there is a tension. You're saying language that infringes, but if you look at geopolitics and superpowers, it does seem that there's another thing in tension, which is a language is a national identity sometimes for certain nations. That's the war in Ukraine, language, Ukrainian language is a symbol of that war in many ways, like a country fighting for its own identity. So it's not merely the convenience. I mean, those two things are at attention is the convenience of trade and the economics and be able to communicate with neighboring countries and trade more efficiently with neighboring countries, all that kind of stuff. But also identity of the group.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's right. I completely agree.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because language is the way for every community like dialects that emerge are a kind of identity for people and sometimes a way for people to say F you to the more powerful people. It's interesting. So in that way, language can be used as that tool.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I completely agree. And there's a lot of work to try to create that identity. So people want to do this. As a cognitive scientist and language expert, I hope that continues because I don't want languages to die. I want languages to survive because they're so interesting for so many reasons. But I mean, I find them fascinating just for the language part, but I think there's a lot of connections to culture as well, which is also very important.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you have hope for machine translation that it can break down the barriers of language? So while all these different diverse languages exist, I guess there's many ways of asking this question, but basically how hard is it to translate in an automated way for one language to another?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>what the things they talk about, things it's going to have to do with their life as opposed to my industrial life, which is going to be different. And so there's going to be problems like that always. Maybe it's not so bad in the case of some of these spaces, and maybe it's going to be hard or others. And so it's pretty bad in number. It's extreme, I'd say in the number space, exact number space. But in the color dimension, that's not so bad. But it's a problem that you don't have to talk about the concepts.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And there might be entire concepts that are missing. So to you, it's more about the space of concept versus the space of form. Like form you can probably map.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes. Yeah. So you were talking earlier about translation and about how translations, there's good and bad translations. I mean, now we're talking about translations of form, right? So what makes writing good, right? It's not-<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's the music to the form.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's not just the content, it's how it's written and translating that that sounds difficult.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We shouldn't should say that, there is, I hesitate to say meaning, but there's a music and a rhythm to the form. When you look at the broad picture, like the difference between Dostoevsky and Tolstoy or Hemingway, Bukowski, James Joyce, like I mentioned, there's a beat to it. There's an edge to it that is in the form.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We can probably get measures of those.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's interesting.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm optimistic that we could get measures of those things. And so maybe that's-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Translatable?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know. I don't know though. I have not worked on that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Actually, I would love to see you translate-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That sounds totally fascinating.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Translation to, I mean, Hemingway is probably the lowest. I would love to see different authors, but the average per sentence dependency length for Hemingway is probably the shortest.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's your sense, huh? It's simple sentences?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Simple short sentences-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Short. Yeah. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, that's one. If you have really long sentences, even if they don't have center embedding-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They can have longer connections.<|eot_id|><|start_header_id|>assistant<|end_header_id|>They can have longer connections.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They don't have to. You can't have a long, long sentence with a bunch of local words, but it is much more likely to have the possibility of long dependencies with long sentences. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I met a guy named Aza Raskin, who does a lot of cool stuff, really brilliant, works with Tristan Harris on a bunch of stuff, but he was talking to me about communicating with animals. He co-founded Earth Species Project where you're trying to find the common language between whales, crows and humans. And he was saying that there's a lot of promising work that even though the signals are very different, the actual, if you have embeddings of the languages, they're actually trying to communicate similar type things. Is there something you can comment on that? Is there promise to that in everything you've seen in different cultures, especially remote cultures, that this is a possibility or no? That we can talk to whales?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>part of that. It's possible. But people have often talked about how, like Chomsky, in fact, has talked about how human, only human language has this compositionality thing that he thinks is sort of key in language. And the problem with that argument is he doesn't speak whale, and he doesn't speak crow, and he doesn't speak monkey. They say things like, well, they're making a bunch of grunts and squeaks. And their reasoning is like, that's bad reasoning. I'm pretty sure if you asked a whale what we're saying, they'd say, well, I'm making a bunch of weird noises.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Exactly.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be making that human language is special because we're the only ones who have human language. I'm like, well, we don't know what those other, we can't talk to them yet. And so there are probably a signal in there, and it might very well be something complicated like human language. I mean, sure with a small brain in lower species, there's probably not a very good communication system. But in these higher species where you have what seems to be abilities to communicate something, there might very well be a lot more signal there than we might've otherwise thought.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But also if we have a lot of intellectual humility here, somebody formerly from MIT and Neri Oxman, who I admire very much, has talked a lot about, has worked on communicating with plants. So yes, the signal there is even less than, but it's not out of the realm of possibility that all nature has a way of communicating. And it's a very different language, but they do develop a kind of language through the chemistry, through some way of communicating with each other. And if you have enough humility about that possibility, I think it would be a very interesting, in a few decades, maybe centuries, hopefully not a humbling possibility of being able to communicate not just between humans effectively, but between all of living things on earth.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, I mean, I think some of them are not going to have much interesting to say-<|eot_id|><|start_header_id|>assistant<|end_header_id|>But [inaudible 02:46:07] still?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But some of them will. We don't know. We certainly don't know. I think-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think if we're humble, there could be some interesting trees out there.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>some other conspecific as opposed to us. And so there probably is, there may be some signal there. So there are people out there. Actually, it's pretty common to say that human language is special and different from any other animal communication system, and I just don't think the evidence is there for that claim. I think it's not obvious. We just don't know because we don't speak these other communication systems until we get better. I do think there are people working on that, as you pointed out though, people working on whale speak, for instance. That's really fascinating.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let me ask you a wild out there sci-fi question. If we make contact with an intelligent alien civilization and you get to meet them, how surprised would you be about their way of communicating? Do you think you would be recognizable? Maybe there's some parallels here to when you go to the remote tribes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I would want Dan Everett with me. He is amazing at learning foreign languages, and so this is an amazing feat to be able to go, this is a language, the Piraha, which has no translators before him. I mean, there-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, wow. So he just shows up?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>He was a missionary that went there. Well, there was a guy that had been there before, but he wasn't very good. And so he learned the language far better than anyone else had learned before him. He's good at, he's a very social person. I think that's a big part of it is being able to interact. So I don't know. It kind of depends on this species from outer space, how much they want to talk to us.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there something you could say about the process he follows? How do you show up to a tribe and socialize? I mean, I guess colors and counting is one of the most basic things to figure out.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>count words in this language because they didn't know, this wasn't interesting. I mean, it was kind of weird, they'd say some or something, the same word over and over again. But that is a standard thing. You just try to, but you have to be pretty out there socially willing to talk to random people, which these are really very different people from you. And he is very social. And so I think that's a big part of this is that's how a lot of people know a lot of languages is they're willing to talk to other people.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a tough one where you just show up knowing nothing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Oh god.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's beautiful that humans are able to connect in that way. You've had an incredible career exploring this fascinating topic. What advice would you give to young people about how to have a career like that or a life that they can be proud of?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>right, I got to find another group. And so we searched around and we were able to find the, because I wanted to keep working on this kind of problem, and so we found the Tsimane and just go there. We didn't have content. We had a little bit of contact and brought someone, and you just kind of try things. I say it's like a lot of that's just like ambition. Just try to do something that other people haven't done. Just give it a shot, is what I mean. I do that all the time. I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I love it. And I love the fact that your pursuit of fun has landed You here talking to me. This was an incredible conversation that you're just a fascinating human being. Thank you for taking a journey through human language with me today. This is awesome.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thank you very much, Lex, it's been a pleasure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Edward Gibson. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Wittgenstein. The limits of my language mean the limits of my world. Thank you for listening and hope to see you next time.<|eot_id|>"}
