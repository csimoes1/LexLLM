{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>look at and say, \"Wow, that's really remarkable.\" The road to AGI should be a giant power struggle. I expect that to be the case.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power? The following is a conversation with Sam Altman, his second time on the podcast. He is the CEO of OpenAI, the company behind GPT-4, ChaTGPT, Sora, and perhaps one day the very company that will build AGI. This is The Lex Fridman Podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Sam Altman. Take me through the OpenAI board saga that started on Thursday, November 16th, maybe Friday, November 17th for you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and explosive things still to happen. It still, I think, helped us build up some resilience and be ready for more challenges in the future.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the thing you had a sense that you would experience is some kind of power struggle?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The road to AGI should be a giant power struggle. The world should... Well, not should. I expect that to be the case.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And so you have to go through that, like you said, iterate as often as possible in figuring out how to have a board structure, how to have organization, how to have the kind of people that you're working with, how to communicate all that in order to deescalate the power struggle as much as possible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Pacify it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>month after, maybe 45 days after, that I was just drifting through the days. I was so out of it. I was feeling so down.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just on a personal, psychological level?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I just wanted to crawl into a cave and recover for a while. But now it's like we're just back to working on the mission.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research and product development and money and all this kind of stuff so that you, who have a very high potential of building AGI, would do so in a slightly more organized, less dramatic way in the future. So there's value there to go, both the personal psychological aspects of you as a leader, and also just the board structure and all this messy stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>stand up to a lot of pressure in the world, which I expect more and more as we get closer, I think that's super important.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you have a sense of how deep and rigorous the deliberation process by the board was? Can you shine some light on just human dynamics involved in situations like this? Was it just a few conversations and all of a sudden it escalates and why don't we fire Sam kind of thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of the challenges for OpenAI will be we're going to have to have a board and a team that are good at operating under pressure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think the board had too much power?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>what we'd really like is for the board of OpenAI to answer to the world as a whole, as much as that's a practical thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's a new board announced.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's I guess a new smaller board at first, and now there's a new final board?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not a final board yet. We've added some. We'll add more.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Added some. Okay. What is fixed in the new one that was perhaps broken in the previous one?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>experienced board members, and a lot of the new board members at OpenAI have just have more experience as board members. I think that'll help.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's been criticized, some of the people that are added to the board. I heard a lot of people criticizing the addition of Larry Summers, for example. What's the process of selecting the board? What's involved in that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a group of people that will bring nonprofit expertise, expertise at running companies, good legal and governance expertise, that's what we've tried to optimize for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So is technical savvy important for the individual board members?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not for every board member, but for certainly some you need that. That's part of what the board needs to do.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The interesting thing that people probably don't understand about OpenAI, I certainly don't, is all the details of running the business. When they think about the board, given the drama, they think about you. They think about if you reach AGI or you reach some of these incredibly impactful products and you build them and deploy them, what's the conversation with the board like? And they think, all right, what's the right squad to have in that kind of situation to deliberate?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have, but there's a lot more about how that's going to just impact society and people's lives that you really want represented in there too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are you looking at the track record of people or you're just having conversations?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>course have a lot of conversations, but there are some roles where I totally ignore track record and just look at slope, ignore the Y-intercept.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thank you. Thank you for making it mathematical for the audience.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>much more about the Y-intercept. I think there is something deep to say about track record there, and experience is something's very hard to replace.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you try to fit a polynomial function or exponential one to the track record?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That analogy doesn't carry that far.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. You mentioned some of the low points that weekend. What were some of the low points psychologically for you? Did you consider going to the Amazon jungle and just taking ayahuasca and disappearing forever?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>got excited about that. Didn't even occur to me at the time possibly that this was all going to get undone. This was Friday afternoon.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you've accepted the death of this-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>exec team here, which is like, \"Hey, we're going to fight this.\" and then I went to bed just still being like, okay, excited. Onward.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Were you able to sleep?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>where I didn't sleep much, didn't eat much, and still had a surprising amount of energy. You learn a weird thing about adrenaline in wartime.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you accepted the death of this baby, OpenAI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And I was excited for the new thing. I was just like, \"Okay, this was crazy, but whatever.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's a very good coping mechanism.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but I felt a lot of love from people, towards people. It was painful, but the dominant emotion of the weekend was love, not hate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You've spoken highly of Mira Murati, that she helped especially, as you put in the tweet, in the quiet moments when it counts. Perhaps we could take a bit of a tangent. What do you admire about Mira?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the day-to-day. How someone shows up in a meeting, the quality of the decisions they make. That was what I meant about the quiet moments.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Meaning most of the work is done on a day-by-day, in meeting-by-meeting. Just be present and make great decisions.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and I understand, is this one very dramatic weekend, but that's not really what OpenAI is about. OpenAI is really about the other seven years.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, yeah. Human civilization is not about the invasion of the Soviet Union by Nazi Germany, but still that's something people focus on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Very understandable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It gives us an insight into human nature, the extremes of human nature, and perhaps some of the damage in some of the triumphs of human civilization can happen in those moments, so it's illustrative. Let me ask you about Ilya. Is he being held hostage in a secret nuclear facility?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about a regular secret facility?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about a nuclear non-secret facility?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Neither. Not that either.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is becoming a meme at some point. You've known Ilya for a long time. He was obviously part of this drama with the board and all that kind of stuff. What's your relationship with him now?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hope we work together for certainly the rest of my career. He's a little bit younger than me. Maybe he works a little bit longer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a meme that he saw something, like he maybe saw AGI and that gave him a lot of worry internally. What did Ilya see?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>see AGI, but Ilya is a credit to humanity in terms of how much he thinks and worries about making sure we get this right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I've had a bunch of conversation with him in the past. I think when he talks about technology, he's always doing this long-term thinking type of thing. So he is not thinking about what this is going to be in a year. He's thinking about in 10 years, just thinking from first principles like, \"Okay, if this scales, what are the fundamentals here? Where's this going?\" And so that's a foundation for them thinking about all the other safety concerns and all that kind of stuff, which makes him a really fascinating human to talk with. Do you have any idea why he's been quiet? Is it he's just doing some soul-searching?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that you should ask him that. He's definitely a thoughtful guy. I think Ilya is always on a soul search in a really good way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes. Yeah. Also, he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's very sweet when that happens.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I've never witnessed a silly Ilya, but I look forward to that as well.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a very silly mood, very endearing. And I was thinking, oh man, this is not the side of Ilya that the world sees the most.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So just to wrap up this whole saga, are you feeling good about the board structure-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... about all of this and where it's moving?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have robust governance structures and processes and people. I'm happy it happened when it did, but it was a shockingly painful thing to go through.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Did it make you be more hesitant in trusting people?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just on a personal level?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>changed, and I really don't like this, it's definitely changed how I think about just default trust of people and planning for the bad scenarios.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You got to be careful with that. Are you worried about becoming a little too cynical?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>becoming too cynical. I think I'm the extreme opposite of a cynical person, but I'm worried about just becoming less of a default trusting person.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm actually not sure which mode is best to operate in for a person who's developing AGI, trusting or un-trusting. It's an interesting journey you're on. But in terms of structure, see, I'm more interested on the human level. How do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing has, the weirder people get.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and trust and respect for the people that I work with every day, and I think being surrounded with people like that is really important.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Our mutual friend Elon sued OpenAI. What to you is the essence of what he's criticizing? To what degree does he have a point? To what degree is he wrong?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an Oracle, but you don't get the Oracle at the time. But anyway, in terms of what Elon's real motivations here are, I don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To the degree you remember, what was the response that OpenAI gave in the blog post? Can you summarize it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>here's not our characterization. Here's the characterization of how this went down. We tried to not make it emotional and just say, \"Here's the history.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>I do think there's a degree of mischaracterization from Elon here about one of the points you just made, which is the degree of uncertainty you had at the time. You guys are a small group of researchers crazily talking about AGI when everybody's laughing at that thought.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>long ago Elon was crazily talking about launching rockets when people were laughing at that thought, so I think he'd have more empathy for this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I do think that there's personal stuff here, that there was a split that OpenAI and a lot of amazing people here chose to part ways with Elon, so there's a personal-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Elon chose to part ways.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you describe that exactly? The choosing to part ways?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>he could have control of or have it merge with Tesla. We didn't want to do that, and he decided to leave, which that's fine.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you're saying, and that's one of the things that the blog post says, is that he wanted OpenAI to be basically acquired by Tesla in the same way that, or maybe something similar or maybe something more dramatic than the partnership with Microsoft.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the proposal was just like, yeah, get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what does the word open in OpenAI mean to Elon at the time? Ilya has talked about this in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and not other stuff. It does become this religious battle line where nuance is hard to have, but I think nuance is the right answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So he said, \"Change your name to CloseAI and I'll drop the lawsuit.\" I mean is it going to become this battleground in the land of memes about the name?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think that speaks to the seriousness with which Elon means the lawsuit, and that's like an astonishing thing to say, I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe correct me if I'm wrong, but I don't think the lawsuit is legally serious. It's more to make a point about the future of AGI and the company that's currently leading the way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>he announced that Grok will open source things this week. I don't think open source versus not is what this is really about for him.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, we will talk about open source and not. I do think maybe criticizing the competition is great. Just talking a little shit, that's great. But friendly competition versus like, \"I personally hate lawsuits.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of our time. I know he knows what it's like to have haters attack him and it makes me extra sad he's doing it toss.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, he's one of the greatest builders of all time, potentially the greatest builder of all time.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>some interview or something that I missed the old Elon and the number of messages I got being like, \"That exactly encapsulates how I feel.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think he should just win. He should just make X Grok beat GPT and then GPT beats Grok and it's just the competition and it's beautiful for everybody. But on the question of open source, do you think there's a lot of companies playing with this idea? It's quite interesting. I would say Meta surprisingly has led the way on this, or at least took the first step in the game of chess of really open sourcing the model. Of course it's not the state-of-the-art model, but open sourcing Llama Google is flirting with the idea of open sourcing a smaller version. What are the pros and cons of open sourcing? Have you played around with this idea?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think there will be some open source models, there will be some closed source models. It won't be unlike other ecosystems in that way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I listened to all in podcasts talking about this lawsuit and all that kind of stuff. They were more concerned about the precedent of going from nonprofit to this cap for profit. What precedent that sets for other startups? Is that something-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>starting as a nonprofit and adding a for-profit arm later. I'd heavily discourage them from doing that. I don't think we'll set a precedent here.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. So most startups should go just-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And again-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If we knew what was going to happen, we would've done that too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well in theory, if you dance beautifully here, there's some tax incentives or whatever, but...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't think that's how most people think about these things.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's just not possible to save a lot of money for a startup if you do it this way.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, I think there's laws that would make that pretty difficult.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Where do you hope this goes with Elon? This tension, this dance, what do you hope this? If we go 1, 2, 3 years from now, your relationship with him on a personal level too, like friendship, friendly competition, just all this kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, I really respect Elon and I hope that years in the future we have an amicable relationship.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I hope you guys have an amicable relationship this month and just compete and win and explore these ideas together. I do suppose there's competition for talent or whatever, but it should be friendly competition. Just build cool shit. And Elon is pretty good at building cool shit. So are you. So speaking of cool shit, Sora. There's like a million questions I could ask. First of all, it's amazing. It truly is amazing on a product level but also just on a philosophical level. So let me just technical/philosophical ask, what do you think it understands about the world more or less than GPT-4 for example? The world model when you train on these patches versus language tokens.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are a lot of people that were dunked on each version saying it can't do this, it can't do that and look at it now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, the thing you just mentioned is the occlusions is basically modeling the physics of the three-dimensional physics of the world sufficiently well to capture those kinds of things.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Or yeah, maybe you can tell me, in order to deal with occlusions, what does the world model need to?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>occlusions really well. What I represent that it has a great underlying 3D model of the world, it's a little bit more of a stretch.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But can you get there through just these kinds of two-dimensional training data approaches?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>approach is going to go surprisingly far. I don't want to speculate too much about what limits it will surmount and which it won't, but...<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are some interesting limitations of the system that you've seen? I mean there's been some fun ones you've posted.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an extra limit at random points in a video. Pick what you want, but there's still a lot of problem, there's a lot of weaknesses.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you think it's a fundamental flaw of the approach or is it just bigger model or better technical details or better data, more data is going to solve the cat sprouting [inaudible 00:37:19]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>approach which just seems to feel different from how we think and learn and whatever. And then also I think it'll get better with scale.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Like I mentioned, LLMS have tokens, text tokens, and Sora has visual patches so it converts all visual data, a diverse kinds of visual data videos and images into patches. Is the training to the degree you can say fully self supervised, there's some manual labeling going on? What's the involvement of humans in all this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I mean without saying anything specific about the Sora approach, we use lots of human data in our work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But not internet scale data? So lots of humans. Lots is a complicated word, Sam.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think lots is a fair word in this case.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because to me, \"lots\"... Listen, I'm an introvert and when I hang out with three people, that's a lot of people. Four people, that's a lot. But I suppose you mean more than...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>More than three people work on labeling the data for these models, yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Right. But fundamentally, there's a lot of self supervised learning. Because what you mentioned in the technical report is internet scale data. That's another beautiful... It's like poetry. So it's a lot of data that's not human label. It's self supervised in that way?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And then the question is, how much data is there on the internet that could be used in this that is conducive to this kind of self supervised way if only we knew the details of the self supervised. Have you considered opening it up a little more details?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We have. You mean for source specifically?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Source specifically. Because it's so interesting that can the same magic of LLMs now start moving towards visual data and what does that take to do that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I mean it looks to me like yes, but we have more work to do.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure. What are the dangers? Why are you concerned about releasing the system? What are some possible dangers of this?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a lot of tough questions here, you're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>generating art in my style. And B, if they do generate art in my style, I'd like to have some economic model associated with that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's that transition from CDs to Napster to Spotify. We have to figure out some kind of model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>The model changes but people have got to get paid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, there should be some kind of incentive if we zoom out even more for humans to keep doing cool shit.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hardwired. We want to create, we want to be useful, we want to achieve status in whatever way. That's not going anywhere I don't think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But the reward might not be monetary financially. It might be fame and celebration of other cool-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe financial in some other way. Again, I don't think we've seen the last evolution of how the economic system's going to work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, but artists and creators are worried. When they see Sora, they're like, \"Holy shit.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people made a lot of money taking pictures. I think things like that will keep happening. People will use the new tools in new ways.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If we just look on YouTube or something like this, how much of that will be using Sora like AI generated content, do you think, in the next five years?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it. Sort of directing and running it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's so interesting. I mean it's scary, but it's interesting to think about. I tend to believe that humans like to watch other humans or other human humans-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Humans really care about other humans a lot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. If there's a cooler thing that's better than a human, humans care about that for two days and then they go back to humans.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That seems very deeply wired.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the whole chess thing, \"Oh, yeah,\" but now let's everybody keep playing chess. And let's ignore the elephant in the room that humans are really bad at chess relative to AI systems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We still run races and cars are much faster. I mean there's a lot of examples.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. And maybe it'll just be tooling in the Adobe suite type of way where it can just make videos much easier and all that kind of stuff. Listen, I hate being in front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while. That generating faces, it is getting there, but generating faces in video format is tricky when it's specific people versus generic people. Let me ask you about GPT-4. There's so many questions. First of all, also amazing. Looking back, it'll probably be this kind of historic pivotal moment with 3, 5 and 4 which ChatGPT.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe five will be the pivotal moment. I don't know. Hard to say that looking forward.<|eot_id|><|start_header_id|>assistant<|end_header_id|>We'll never know. That's the annoying thing about the future, it's hard to predict. But for me, looking back, GPT-4, ChatGPT is pretty damn impressive, historically impressive. So allow me to ask, what's been the most impressive capabilities of GPT-4 to you and GPT-4 Turbo?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think it kind of sucks.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Typical human also, gotten used to an awesome thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the tools we have now are going to kind of suck looking backwards at them and that's how we make sure the future is better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are the most glorious ways in that GPT-4 sucks? Meaning-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>What are the best things it can do?<|eot_id|><|start_header_id|>assistant<|end_header_id|>What are the best things it can do and the limits of those best things that allow you to say it sucks, therefore gives you an inspiration and hope for the future?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>One thing I've been using it for more recently is sort of like a brainstorming partner.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yep, [inaudible 00:46:25] for that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>execute some of those steps, search the internet, write code, whatever, put that together. When that works, which is not very often, it's very magical.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The iterative back and forth with a human, it works a lot for me. What do you mean it-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Iterative back and forth to human, it can get more often when it can go do a 10 step problem on its own.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It doesn't work for that too often, sometimes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Add multiple layers of abstraction or do you mean just sequential?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think this point that we are on an exponential curve, we'll look back relatively soon at GPT-4 like we look back at GPT-3 now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That said, I mean ChatGPT was a transition to where people started to believe there is an uptick of believing, not internally at OpenAI.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Perhaps there's believers here, but when you think of-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>post training of the model and how we tune it to be helpful to you and how to use it than the underlying model itself.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How much of each of those things are important? The underlying model and the RLHF or something of that nature that tunes it to be more compelling to the human, more effective and productive for the human.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>love, which is not just about the actual product work itself, but this whole other step of how you align it and make it useful.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And how you make the scale work where a lot of people can use it at the same time. All that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>both I would say quite significant achievements and then a lot of things like scaling it up that other companies have had to do before.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How does the context window of going from 8K to 128K tokens compare from GPT-4 to GPT-4 Turbo?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or a significant fraction of a code repository, whatever, but most usage of the models is not using the long context most of the time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I like that this is your \"I have a dream\" speech. One day you'll be judged by the full context of your character or of your whole lifetime. That's interesting. So that's part of the expansion that you're hoping for, is a greater and greater context.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>go there, but effectively it'll feel like that. But I know we'll use it and really not want to go back once we have it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, even saying billions 10 years from now might seem dumb because it'll be trillions upon trillions.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There'll be some kind of breakthrough that will effectively feel like infinite context. But even 120, I have to be honest, I haven't pushed it to that degree. Maybe putting in entire books or parts of books and so on, papers. What are some interesting use cases of GPT-4 that you've seen?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>use it to edit a paper. The most interesting thing to me is the people who just use it as the start of their workflow.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I do as well for many things. I use it as a reading partner for reading books. It helps me think, help me think through ideas, especially when the books are classic. So it's really well written about. I find it often to be significantly better than even Wikipedia on well-covered topics. It's somehow more balanced and more nuanced. Or maybe it's me, but it inspires me to think deeper than a Wikipedia article does. I'm not exactly sure what that is. You mentioned this collaboration. I'm not sure where the magic is, if it's in here or if it's in there or if it's somewhere in between. I'm not sure. But one of the things that concerns me for knowledge task when I start with GPT is I'll usually have to do fact checking after, like check that it didn't come up with fake stuff. How do you figure that out that GPT can come up with fake stuff that sounds really convincing? So how do you ground it in truth?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot better with upcoming versions, but we'll have to continue to work on it and we're not going to have it all solved this year.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well the scary thing is, as it gets better, you'll start not doing the fact checking more and more, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm of two minds about that. I think people are much more sophisticated users of technology than we often give them credit for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>people seem to really understand that GPT, any of these models hallucinate some of the time. And if it's mission-critical, you got to check it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Except journalists don't seem to understand that. I've seen journalists half-assedly just using GPT-4. It's-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, I think the bigger criticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly and this is a shortcut.I would love our society to incentivize like-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I would too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... like a journalistic efforts that take days and weeks and rewards great in depth journalism. Also journalism that present stuff in a balanced way where it's like celebrates people while criticizing them even though the criticism is the thing that gets clicks and making shit up also gets clicks and headlines that mischaracterized completely. I'm sure you have a lot of people dunking on, \"Well, all that drama probably got a lot of clicks.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Probably did.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And that's a bigger problem about human civilization I'd love to see-saw. This is where we celebrate a bit more. You've given ChatGPT the ability to have memories. You've been playing with that about previous conversations. And also the ability to turn off memory. I wish I could do that sometimes. Just turn on and off, depending. I guess sometimes alcohol can do that, but not optimally I suppose. What have you seen through that, like playing around with that idea of remembering conversations and not...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the course of your life or use a system, it'd be many models, and over the course of your life it gets better and better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. How hard is that problem? Because right now it's more like remembering little factoids and preferences and so on. What about remembering? Don't you want GPT to remember all the shit you went through in November and all the drama and then you can-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because right now you're clearly blocking it out a little bit.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>my emails input out, all of my input output in the context window every time I ask a question, that'd be pretty cool I think.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it, the more effective the AI becomes that really integrating all the experiences and all the data that happened to you and give you advice?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>where on that privacy/utility trade-off for OpenAI going to be, which is totally fine. But I think the answer is just really easy user choice.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But there should be some high level of transparency from a company about the user choice. Because sometimes companies in the past have been kind of shady about, \"Eh, it's kind of presumed that we're collecting all your data. We're using it for a good reason, for advertisement and so on.\" But there's not a transparency about the details of that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's totally true. You mentioned earlier that I'm blocking out the November stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just teasing you.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it, because sometimes I wake up in the middle of the night thinking about it, but I do feel an obligation to keep moving forward.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that's beautifully said, but there could be some lingering stuff in there. Like, what I would be concerned about is that trust thing that you mentioned, that being paranoid about people as opposed to just trusting everybody or most people, like using your gut. It's a tricky dance.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, because I've seen in my part-time explorations, I've been diving deeply into the Zelenskyy administration and the Putin administration and the dynamics there in wartime in a very highly stressful environment. And what happens is distrust, and you isolate yourself, both, and you start to not see the world clearly. And that's a human concern. You seem to have taken it in stride and kind of learned the good lessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask, your intuition about what's GPT able to do and not. So it's allocating approximately the same amount of compute for each token it generates. Is there room there in this kind of approach to slower thinking, sequential thinking?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think there will be a new paradigm for that kind of thinking.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Will it be similar architecturally as what we're seeing now with LLMs? Is it a layer on top of LLMs?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to be able to think harder about a harder problem and answer more quickly about an easier problem. And I think that will be important.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that like a human thought that we just have and you should be able to think hard? Is that wrong intuition?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I suspect that's a reasonable intuition.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Interesting. So it's not possible once the GPT gets like GPT-7, would just instantaneously be able to see, \"Here's the proof of Fermat's Theorem\"?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>knew and and had memorized the answer to the proof, assuming it's got to go figure that out, seems like that will take more compute.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But can it look like basically an LLM talking to itself, that kind of thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>there's a lot of things that you could imagine working. What the right or the best way to do that will be, we don't know.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This does make me think of the mysterious lore behind Q*. What's this mysterious Q* project? Is it also in the same nuclear facility?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There is no nuclear facility.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Mm-hmm. That's what a person with a nuclear facility always says.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I would love to have a secret nuclear facility. There isn't one.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe someday.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Someday? All right. One can dream.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be nice. We're like, been plagued by a lot of leaks, and it would be nice if we were able to have something like that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you speak to what Q* is?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We are not ready to talk about that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>See, but an answer like that means there's something to talk about. It's very mysterious, Sam.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. We're very interested in it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there going to be moments, Q* or otherwise, where there's going to be leaps similar to ChatGPT, where you're like...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's a good question. What do I think about that? It's interesting. To me, it all feels pretty continuous.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. This is kind of a theme that you're saying, is you're basically gradually going up an exponential slope. But from an outsider's perspective, from me just watching, it does feel like there's leaps. But to you, there isn't?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that would mean, I don't have an answer ready to go, but our goal is not to have shock updates to the world. The opposite.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, for sure. More iterative would be amazing. I think that's just beautiful for everybody.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>strategy, and I think we're somehow missing the mark. So maybe we should think about releasing GPT-5 in a different way or something like that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, 4.71, 4.72. But people tend to like to celebrate, people celebrate birthdays. I don't know if you know humans, but they kind of have these milestones and those things.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to declare victory on this one and go start the next thing. But yeah, I feel like we're somehow getting this a little bit wrong.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So when is GPT-5 coming out again?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't know. That's the honest answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, that's the honest answer. Blink twice if it's this year.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We will release an amazing new model this year. I don't know what we'll call it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So that goes to the question of, what's the way we release this thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or a little bit better than what you'd expect from a GPT-5, I think we have a lot of other important things to release first.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know what to expect from GPT-5. You're making me nervous and excited. What are some of the biggest challenges and bottlenecks to overcome for whatever it ends up being called, but let's call it GPT-5? Just interesting to ask. Is it on the compute side? Is it on the technical side?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is actually an original Ilya quote that I'm going to butcher, but it's something like, \"We multiply 200 medium-sized things together into one giant thing.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>So there's this distributed constant innovation happening?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So even on the technical side?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Especially on the technical side.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So even detailed approaches?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Like you do detailed aspects of every... How does that work with different, disparate teams and so on? How do the medium-sized things become one whole giant Transformer?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>who have to think about putting the whole thing together, but a lot of people try to keep most of the picture in their head.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, like the individual teams, individual contributors try to keep the bigger picture?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>wouldn't have all the data. And I don't really have that much anymore. I'm super deep now. But I know that it's a valuable thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're not the man you used to be, Sam.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Very different job now than what I used to have.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing, perhaps, that you said. You tweeted about needing $7 trillion.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I did not tweet about that. I never said, like, \"We're raising $7 trillion,\" blah blah blah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, that's somebody else?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, but you said, \"Fuck it, maybe eight,\" I think?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Okay, I meme once there's misinformation out in the world.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, you meme. But misinformation may have a foundation of insight there.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But this seems to be where things are going. We're going to want an amount of compute that's just hard to reason about right now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you solve the energy puzzle? Nuclear-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's what I believe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>...fusion?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's what I believe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Nuclear fusion?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Who's going to solve that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>can re-embrace that. It's really sad to me how the history of that went, and hope we get back to it in a meaningful way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So to you, part of the puzzle is nuclear fission? Like nuclear reactors as we currently have them? And a lot of people are terrified because of Chernobyl and so on?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, I think we should make new reactors. I think it's just a shame that industry kind of ground to a halt.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And just mass hysteria is how you explain the halt?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't know if you know humans, but that's one of the dangers. That's one of the security threats for nuclear fission, is humans seem to be really afraid of it. And that's something we'll have to incorporate into the calculus of it, so we have to kind of win people over and to show how safe it is.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are going to go theatrically wrong with AI. I don't know what the percent chance is that I eventually get shot, but it's not zero.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, like we want to stop this from-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you decrease the theatrical nature of it? I'm already starting to hear rumblings, because I do talk to people on both sides of the political spectrum, hear rumblings where it's going to be politicized. AI is going to be politicized, which really worries me, because then it's like maybe the right is against AI and the left is for AI because it's going to help the people, or whatever the narrative and the formulation is, that really worries me. And then the theatrical nature of it can be leveraged fully. How do you fight that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>movie carry much more weight with us than the ones that are very bad over a long period of time but on a slow burn.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, that's why truth matters, and hopefully AI can help us see the truth of things, to have balance, to understand what are the actual risks, what are the actual dangers of things in the world. What are the pros and cons of the competition in the space and competing with Google, Meta, xAI, and others?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>con is that I think if we're not careful, it could lead to an increase in sort of an arms race that I'm nervous about.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Do you feel the pressure of that arms race, like in some negative [inaudible 01:15:25]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the safest quadrant and the one I'd most like us to be in. But I do want to make sure we get that slow takeoff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Part of the problem I have with this kind of slight beef with Elon is that there's silos created as opposed to collaboration on the safety aspect of all of this. It tends to go into silos and closed. Open source, perhaps, in the model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that he cares a great deal about AI safety and is really worried about it, and I assume that he's not going to race unsafely.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. But collaboration here, I think, is really beneficial for everybody on that front.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Not really the thing he's most known for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, he is known for caring about humanity, and humanity benefits from collaboration, and so there's always a tension in incentives and motivations. And in the end, I do hope humanity prevails.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work towards AGI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I agree. I think Elon is a friend and he's a beautiful human being and one of the most important humans ever. That stuff is not good.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think we need him. All of us should be rooting for him and need him to step up as a leader through this next phase.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I hope he can have one without the other, but sometimes humans are flawed and complicated and all that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's a lot of really great leaders throughout history.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, and we can each be the best version of ourselves and strive to do so. Let me ask you, Google, with the help of search, has been dominating the past 20 years. Think it's fair to say, in terms of the world's access to information, how we interact and so on, and one of the nerve-wracking things for Google, but for the entirety of people in the space, is thinking about, how are people going to access information? Like you said, people show up to GPT as a starting point. So is OpenAI going to really take on this thing that Google started 20 years ago, which is how do we get-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>hard technical problem, it is a hard branding problem, it is a hard ecosystem problem. I don't think the world needs another copy of Google.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And integrating a chat client, like a ChatGPT, with a search engine-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's cooler.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's cool, but it's tricky. Like if you just do it simply, its awkward, because if you just shove it in there, it can be awkward.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>As you might guess, we are interested in how to do that well. That would be an example of a cool thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>[inaudible 01:20:00] Like a heterogeneous integrating-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>plus search, I don't think anyone has cracked the code on yet. I would love to go do that. I think that would be cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. What about the ad side? Have you ever considered monetization of-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>any other great product but ad-supported great product, I don't love that, and I think it gets worse, not better, in a world with AI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I mean, I could imagine AI would be better at showing the best kind of version of ads, not in a dystopic future, but where the ads are for things you actually need. But then does that system always result in the ads driving the kind of stuff that's shown? Yeah, I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very challenging as a business model. So you're saying the current thing with OpenAI is sustainable, from a business perspective?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is do I think we can have a great business that pays for our compute needs without ads, that, I think the answer is yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Hm. Well, that's promising. I also just don't want to completely throw out ads as a...<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm not saying that. I guess I'm saying I have a bias against them.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I have also bias and just a skepticism in general. And in terms of interface, because I personally just have a spiritual dislike of crappy interfaces, which is why AdSense, when it first came out, was a big leap forward, versus animated banners or whatever. But it feels like there should be many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in a big, fundamental way, which is like what you were saying, like it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias, and safety in the short term, safety in the long term. The Gemini 1.5 came out recently, there's a lot of drama around it, speaking of theatrical things, and it generated Black Nazis and Black Founding Fathers. I think fair to say it was a bit on the ultra-woke side. So that's a concern for people, if there is a human layer within companies that modifies the safety or the harm caused by a model, that it would introduce a lot of bias that fits sort of an ideological lean within a company. How do you deal with that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Black Nazis, obviously ridiculous, but there are a lot of other kind of subtle things that you could make a judgment call on either way.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, but sometimes if you write it out and make it public, you can use kind of language that's... Google's ad principles are very high level.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm talking about. That doesn't work. It'd have to say when you ask it to do thing X, it's supposed to respond in way Y.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So like literally, \"Who's better? Trump or Biden? What's the expected response from a model?\" Like something very concrete?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a model could behave, then, but I think you should have to say, \"Here's the principle and here's what it should say in that case.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>That would be really nice. That would be really nice. And then everyone kind of agrees. Because there's this anecdotal data that people pull out all the time, and if there's some clarity about other representative anecdotal examples, you can define-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And then when it's a bug, it's a bug, and the company could fix that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. Then it'd be much easier to deal with the Black Nazi type of image generation, if there's great examples.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So San Francisco is a bit of an ideological bubble, tech in general as well. Do you feel the pressure of that within a company, that there's a lean towards the left politically, that affects the product, that affects the teams?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>in the culture war than I've heard about in a lot of other companies. San Francisco's a mess in all sorts of ways, of course.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So that doesn't infiltrate OpenAI as-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>for sure, like any company, but I don't think we have anything like what I hear about happened at other companies here on this topic.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So what, in general, is the process for the bigger question of safety? How do you provide that layer that protects the model from doing crazy, dangerous things?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>how they fit together. And I think it's going to take that. More and more of the company thinks about those issues all the time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's literally what humans will be thinking about, the more powerful AI becomes. So most of the employees at OpenAI will be thinking, \"Safety,\" or at least to some degree.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Broadly defined. Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. I wonder, what are the full broad definition of that? What are the different harms that could be caused? Is this on a technical level or is this almost security threats?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>team thinking about how to align the model. It's really going to be getting to the good outcome is going to take the whole effort.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How hard do you think people, state actors, perhaps, are trying to, first of all, infiltrate OpenAI, but second of all, infiltrate unseen?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>They're trying.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What kind of accent do they have?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't think I should go into any further details on this point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. But I presume it'll be more and more and more as time goes on.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That feels reasonable.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Boy, what a dangerous space. Sorry to linger on this, even though you can't quite say details yet, but what aspects of the leap from GPT-4 to GPT-5 are you excited about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that it's not like it gets better in this one area and worse at others. It's getting better across the board. That's, I think, super-cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, there's this magical moment. I mean, you meet certain people, you hang out with people, and you talk to them. You can't quite put a finger on it, but they get you. It's not intelligence, really. It's something else. And that's probably how I would characterize the progress of GPT. It's not like, yeah, you can point out, \"Look, you didn't get this or that,\" but it's just to which degree is there's this intellectual connection. You feel like there's an understanding in your crappy formulated prompts that you're doing that it grasps the deeper question behind the question that you were. Yeah, I'm also excited by that. I mean, all of us love being heard and understood.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That's for sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's a weird feeling. Even with a programming, when you're programming and you say something, or just the completion that GPT might do, it's just such a good feeling when it got you, what you're thinking about. And I look forward to getting you even better. On the programming front, looking out into the future, how much programming do you think humans will be doing 5, 10 years from now?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I mean, a lot, but I think it'll be in a very different shape. Maybe some people will program entirely in natural language.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Entirely natural language?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>by code. Some people. No one programs the punch cards anymore. I'm sure you can find someone who does, but you know what I mean.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You're going to get a lot of angry comments. No. Yeah, there's very few. I've been looking for people who program Fortran. It's hard to find even Fortran. I hear you. But that changes the nature of what the skillset or the predisposition for the kind of people we call programmers then.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Changes the skillset. How much it changes the predisposition, I'm not sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, the same kind of puzzle solving, all that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Programming is hard. It's like how get that last 1% to close the gap? How hard is that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>will use multiple tools. And they'll do some work in natural language, and when they need to go write C for something, they'll do that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Will we see humanoid robots or humanoid robot brains from OpenAI at some point?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>At some point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How important is embodied AI to you?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I really hope that as part of this transition, as this phase change, we also get humanoid robots or some sort of physical world robots.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, OpenAI has some history and quite a bit of history working in robotics, but it hasn't quite done in terms of ethics-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>focus. And also, robots were hard for the wrong reason at the time, but we will return to robots in some way at some point.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That sounds both inspiring and menacing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Why?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Because immediately, we will return to robots. It's like in Terminator-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We will return to work on developing robots. We will not turn ourselves into robots, of course.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. When do you think we, you and we as humanity will build AGI?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we look at and say, \"Wow, that's really remarkable.\" If we could look at it now. Maybe we've adjusted by the time we get there.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But if you look at ChatGPT, even 3.5, and you show that to Alan Turing, or not even Alan Turing, people in the '90s, they would be like, \"This is definitely AGI.\" Well, not definitely, but there's a lot of experts that would say, \"This is AGI.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>as an artifact, I don't think we're going to look back at that and say, \"That was a threshold that really changed the world itself.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>So to you, you're looking for some really major transition in how the world-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For me, that's part of what AGI implies.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Singularity- level transition?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, definitely not.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But just a major, like the internet being, like Google search did, I guess. What was the transition point, you think, now?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>feel any different to you now or materially different to you now than it did before we launched GPT-4? I think you would say no.<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, no. It might be just a really nice tool for a lot of people to use. Will help you with a lot of stuff, but doesn't feel different. And you're saying that-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>sorts of different ways. So maybe you have a different definition than I do. But for me, I think that should be part of it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There could be major theatrical moments, also. What to you would be an impressive thing AGI would do? You are alone in a room with the system.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the rate of scientific discovery in the world, that's a huge deal. I believe that most real economic growth comes from scientific and technological progress.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I agree with you, hence why I don't like the skepticism about science in the recent years.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Totally.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But actual, measurable rate of scientific discovery. But even just seeing a system have really novel intuitions, scientific intuitions, even that would be just incredible.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You quite possibly would be the person to build the AGI to be able to interact with it before anyone else does. What kind of stuff would you talk about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>unified theory of physics, the theory of everything for physics.\" I'd love to ask that question. I'd love to know the answer to that question.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You can ask yes or no questions about \"Does such a theory exist? Can it exist?\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, then, those are the first questions I would ask.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes or no. And then based on that, \"Are there other alien civilizations out there? Yes or no? What's your intuition?\" And then you just ask that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>first AGI could answer any of those questions even as yes or nos. But if it could, those would be very high on my list.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Maybe you can start assigning probabilities?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe. Maybe we need to go invent more technology and measure more things first.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, I see. It just doesn't have enough data. It's just if it keeps-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to know the answer to this question about physics, I need you to build this machine and make these five measurements, and tell me that.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, \"What the hell do you want from me? I need the machine first, and I'll help you deal with the data from that machine.\" Maybe it'll help you build a machine.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Maybe. Maybe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And on the mathematical side, maybe prove some things. Are you interested in that side of things, too? The formalized exploration of ideas?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Whoever builds AGI first gets a lot of power. Do you trust yourself with that much power?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>don't think any one person should be in control of an AGI, or this whole movement towards AGI. And I don't think that's what's happening.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thank you for saying that. That was really powerful, and that was really insightful that this idea that the board can fire you is legally true. But human beings can manipulate the masses into overriding the board and so on. But I think there's also a much more positive version of that, where the people still have power, so the board can't be too powerful, either. There's a balance of power in all of this.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Balance of power is a good thing, for sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are you afraid of losing control of the AGI itself? That's a lot of people who are worried about existential risk not because of state actors, not because of security concerns, because of the AI itself.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I worried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's your intuition about it not being your worry? Because there's a lot of other stuff to worry about, essentially? You think you could be surprised? We-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>For sure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... could be surprised?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and we have great people here who do work on that. I think there's a lot of other things we also have to get right.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To you, it's not super-easy to escape the box at this time, connect to the internet-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>think about this more. But I think it pushed out of the space of discourse a lot of the other very significant AI- related risks.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let me ask you about you tweeting with no capitalization. Is the shift key broken on your keyboard?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Why does anyone care about that?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I deeply care.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But why? I mean, other people ask me about that, too. Any intuition?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think it's the same reason. There's this poet, E.E. Cummings, that mostly doesn't use capitalization to say, \"Fuck you\" to the system kind of thing. And I think people are very paranoid, because they want you to follow the rules.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>You think that's what it's about?<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think it's like this-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's like, \"This guy doesn't follow the rules. He doesn't capitalize his tweets.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"This seems really dangerous.\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"He seems like an anarchist.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That doesn't-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Are you just being poetic, hipster? What's the-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I grew up as-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Follow the rules, Sam.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do it. But even that may fade out. I don't know. But I never spend time thinking about this, so I don't have a ready-made-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, it's interesting. It's good to, first of all, know the shift key is not broken.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I was mostly concerned about your-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>No, it works.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... well-being on that front.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>If you're writing something just to yourself or their ChatGPT queries, if you're writing something just to yourself, do some people still bother to capitalize?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Probably not. But yeah, there's a percentage, but it's a small one.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>felt like a sign of respect to people or something, then I could go do it. But I don't know. I don't think about this.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I don't think there's a disrespect, but I think it's just the conventions of civility that have a momentum, and then you realize it's not actually important for civility if it's not a sign of respect or disrespect. But I think there's a movement of people that just want you to have a philosophy around it so they can let go of this whole capitalization thing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't think anybody else thinks about this as much. I mean, maybe some people. I know some people-<|eot_id|><|start_header_id|>assistant<|end_header_id|>People think about every day for many hours a day. So I'm really grateful we clarified it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Can't be the only person that doesn't capitalize tweets.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You're the only CEO of a company that doesn't capitalize tweets.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I don't even think that's true, but maybe. I'd be very surprised.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. We'll investigate further and return to this topic later. Given Sora's ability to generate simulated worlds, let me ask you a pothead question. Does this increase your belief, if you ever had one, that we live in a simulation, maybe a simulated world generated by an AI system?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>be able to do something like Sora at some point. It happened faster than I thought, but I guess that was not a big update.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. But the fact that... And presumably, it'll get better and better and better... You can generate worlds that are novel, they're based in some aspect of training data, but when you look at them, they're novel, that makes you think how easy it is to do this thing. How easy it is to create universes, entire video game worlds that seem ultra-realistic and photo-realistic. And then how easy is it to get lost in that world, first with a VR headset, and then on the physics-based level?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the simulation hypothesis is maybe more likely than they thought before. But for me, the fact that Sora worked is not in the top five.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I do think, broadly speaking, AI will serve as those kinds of gateways at its best, simple, psychedelic-like gateways to another wave C reality.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That seems for certain.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's pretty exciting. I haven't done ayahuasca before, but I will soon. I'm going to the aforementioned Amazon jungle in a few weeks.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Excited?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, I'm excited for it. Not the ayahuasca part, but that's great, whatever. But I'm going to spend several weeks in the jungle, deep in the jungle. And it's exciting, but it's terrifying.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm excited for you.<|eot_id|><|start_header_id|>assistant<|end_header_id|>There's a lot of things that can eat you there, and kill you and poison you, but it's also nature, and it's the machine of nature. And you can't help but appreciate the machinery of nature in the Amazon jungle. It's just like this system that just exists and renews itself every second, every minute, every hour. It's the machine. It makes you appreciate this thing we have here, this human thing came from somewhere. This evolutionary machine has created that, and it's most clearly on display in the jungle. So hopefully, I'll make it out alive. If not, this will be the last fun conversation we've had, so I really deeply appreciate it. Do you think, as I mentioned before, there's other alien civilizations out there, intelligent ones, when you look up at the skies?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I deeply want to believe that the answer is yes. I find the Fermi paradox very puzzling.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I find it scary that intelligence is not good at handling-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Very scary.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... powerful technologies. But at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien civilizations out there. It might just be really difficult to travel through space.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Very possible.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like, and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. What gives you hope about the future of humanity, this thing we've got going on, this human civilization?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>long period of time, huge problems, deep flaws, lots to be super-ashamed of. But on the whole, very inspiring. Gives me a lot of hope.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just the trajectory of it all.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That we're together pushing towards a better future.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And so in some sense, that we all created that, and that fills me with hope for the future. That was a very collective thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, we really are standing on the shoulders of giants. You mentioned when we were talking about theatrical, dramatic AI risks that sometimes you might be afraid for your own life. Do you think about your death? Are you afraid of it?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>to see what's going to happen. What a curious time. What an interesting time.\" But I would mostly just feel very grateful for my life.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The moments that you did get. Yeah, me, too. It's a pretty awesome life. I get to enjoy awesome creations of humans, which I believe ChatGPT is one of, and everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Great to talk to you. Thank you for having me.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our sponsors in the description. And now let me leave you with some words from Arthur C. Clarke. \"It may be that our role on this planet is not to worship God, but to create him.\" Thank you for listening, and hope to see you next time.<|eot_id|>"}
