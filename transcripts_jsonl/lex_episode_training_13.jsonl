{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>"}
{"text": "<|eot_id|><|start_header_id|>assistant<|end_header_id|>The following is a conversation with the founding members of the Cursor team, Michael Truell, Sualeh Asif, Arvid Lunnemark, and Aman Sanger. Cursor is a code editor based on VS Code that adds a lot of powerful features for AI-assisted coding. It has captivated the attention and excitement of the programming and AI communities. So I thought this is an excellent opportunity to dive deep into the role of AI in programming. This is a super technical conversation that is bigger than just about one code editor. It's about the future of programming and in general, the future of human AI collaboration in designing and engineering complicated and powerful systems. This is the Lex Fridman podcast. To support it, please check out our sponsors in the description. And now, dear friends, here's Michael, Sualeh, Arvid and Aman. All right, this is awesome. We have Michael, Aman, Sualeh, Arvid here from the Cursor team. First up, big ridiculous question. What's the point of a code editor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is going to change a lot over the next 10 years as what it means to build software maybe starts to look a bit different.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I think also a code editor should just be fun.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>them out because they're not fun. And so a big part of being fun is being fast a lot of the time. Fast is fun.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, fast is... That should be a T-shirt.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>group together and coding is this amazing thing where it's you and the computer and that alone, you can build really cool stuff really quickly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So for people who don't know, Cursor is this super cool new editor that's a fork of VS Code. It would be interesting to get your explanation of your own journey of editors. I think all of you were big fans of VS Code with Copilot. How did you arrive to VS Code and how did that lead to your journey with Cursor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Code was more than good enough to convince me to switch. And so that kind of was the default until we started working on Cursor.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And maybe we should explain what Copilot does. It's a really nice auto complete. As you start writing a thing, it suggests one or two or three lines how to complete the thing. And there's a fun experience in that. You know like when you have a close friendship and your friend completes your sentences? When it's done well, there's an intimate feeling. There's probably a better word than intimate, but there's a cool feeling of holy shit, it gets me. And then there's an unpleasant feeling when it doesn't get you. And so there's that kind of friction. But I would say for a lot of people, the feeling that it gets me overpowers that it doesn't.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>it. I mean, the other underrated part of Copilot for me was just the first real AI product. So the first language model consumer product.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So Copilot was kind of like the first killer app for LMs.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah. And the beta was out in 2021.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right. Okay. So what's the origin story of Cursor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we didn't have any more ideas, it looked like you could make these models a lot better if you had more compute and more data.<|eot_id|><|start_header_id|>assistant<|end_header_id|>By the way, we'll probably talk for three to four hours on the topic of scaling loss. But just to summarize, it's a paper in a set of papers in a set of ideas that say bigger might be better for model size and data size in the realm of machine learning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's bigger and better, but predictably better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, that's another topic of conversation.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of 2022, had this bet on whether the 2024 June or July you were going to win a gold medal in the IMO with models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>IMO is the International Math Olympiad.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>delusional. And to be honest, I mean, I was, to be clear, very wrong. But that was maybe the most prescient bet in the group.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the new results from DeepMind, it turned out that you were correct.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>back then and before, Aman had this scaling loss T-shirt that he would wear around where it had the charts and the formulas on it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you felt the AGI or you felt the scaling loss.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>can work really, really well and I think you could have systems that are perhaps very superhuman in math and still not technically have AGI.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, so can we take it all the way to Cursor. And what is Cursor? It's a fork of VS Code and VS Code is one of the most popular editors for a long time. Everybody fell in love with it. Everybody left Vim, I left DMAX for it. Sorry. So unified in some fundamental way the developer community. And then you look at the space of things, you look at the scaling laws, AI is becoming amazing and you decided okay, it's not enough to just write an extension via VS Code because there's a lot of limitations to that. If AI is going to keep getting better and better and better, we need to really rethink how the AI is going to be part of the editing process. And so you decided to fork VS Code and start to build a lot of the amazing features we'll be able to talk about. But what was that decision like? Because there's a lot of extensions, including Copilot, of VS Code that are doing sort of AI type stuff. What was the decision like to just fork VS Code?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>coding environment and we didn't want to get locked in by those limitations. We wanted to be able to just build the most useful stuff.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Well then the natural question is, VS Code is kind of with Copilot a competitor, so how do you win? Is it basically just the speed and the quality of the features?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>things, but I don't think they're in a great place to really keep innovating and pushing on this in the way that a startup can.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Just rapidly implementing features.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I'm one of these people that really want to try and use new things and there was no new thing for a very long while.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it's interesting. I don't know how you put that into words, but when you compare a Cursor with Copilot, Copilot pretty quickly started to feel stale for some reason.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>person even. Yeah, often even the same person. You can create things that are sort of not possible if you're not talking, you're not experimenting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And you're using, like you said, Cursor to write Cursor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Of course. Oh yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well let's talk about some of these features. Let's talk about the all-knowing the all-powerful praise be to the Tab, auto complete on steroids basically. So how does Tab work? What is Tab?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>low entropy actions you take inside of the editor. When the intent is effectively determined, let's just jump you forward in time, skip you forward.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, what's the intuition and what's the technical details of how to do next Cursor prediction? That jump, that's not so intuitive I think to people.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>decoding that we built out called speculative edits. These are two, I think, important pieces of what make it quite high quality and very fast.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, so MOE [inaudible 00:20:22], the input is huge, the output is small.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. So what else can you say about how to make... Does caching play a role-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that they're caching aware. And then yeah, you need to reuse the KV cache across requests just so that you're spending less work, less compute.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Again, what are the things that Tab is supposed to be able to do in the near term, just to linger on that? Generate code, fill empty space, also edit code across multiple lines and then jump to different locations inside the same file and then-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the definition of something and then take you back so that you have all the requisite knowledge to be able to accept the next completion.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So providing the human the knowledge.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Right.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I just gotten to know a guy named Primeagen who I believe has an... You can order coffee via SSH.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh yeah. We did that. We did that.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So can also the model do that and provide you with caffeine? Okay. So that's the general framework.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>do and you're like, okay, that's good, that's good, that's good, that's good, and you can just sort of tap, tap through these big changes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>As we're talking about this, I should mention one of the really cool and noticeable things about Cursor is that there's this whole diff interface situation going on. So the model suggests with the red and the green of here's how we're going to modify the code, and in the chat window you can apply and it shows you the diff and you can accept the diff. So maybe can you speak to whatever direction of that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>but in auto-complete your eyes are focused in one area, you can't be in too many... The humans can't look in too many different places.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you're talking about on the interface side?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the option button and then you would see the new suggestion. And if you release the option button, you would then see your original code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So by the way, that's pretty nice, but you have to know to hold the option button.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And by the way, I'm not a Mac user, but I got it. Option. It's a button I guess you people have.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this with a little red squiggly and say, you should probably review this part of the diff. Ideas in that vein I think are exciting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that's a really fascinating space of UX design engineering. So you're basically trying to guide the human programmer through all the things they need to read and nothing more, optimally.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>verification work. It gets more and more and more... You need to help them out. I don't want to spend all my time reviewing code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you say a little more across multiple files [inaudible 00:28:19]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>want a model to. And you don't want to have to figure out that yourself. You want a model to guide you through the thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And is the step of creation going to be more and more natural language, is the goal versus with actual writing the book?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>so I think natural language will have a place. I think it will definitely not be the way most people program most of the time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I'm really feeling the AGI with this editor. It feels like there's a lot of machine learning going on underneath. Tell me about some of the ML stuff that makes it all work?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>out this rough code block that indicates what the change will be and we train a model to then Apply that change to the file.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And we should say that Apply is the model looks at your code, it gives you a really damn good suggestion of what new things to do. And the seemingly for humans trivial step of combining the two, you're saying is not so trivial.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the apply model. Maybe we should talk about how to make it fast if you like. Fast is always an interesting detail. Fast is good.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, how do you make it fast?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>streaming, you can just also start reviewing the code before it's done so there's no big loading screen. Maybe that is part of the advantage.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So the human can start reading before the thing is done.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>idea nowadays. It's not only in language models. There's obviously speculation in CPUs and there's speculation for databases and there's speculation all over the place.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, let me ask the ridiculous question of which LLM is better at coding? GPT, Claude, who wins in the context of programming? And I'm sure the answer is much more nuanced because it sounds like every single part of this involves a different model.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>at maintaining that same capability. You have the same capability in the benchmark as when you try to instruct it to do anything with coding.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Another ridiculous question is the difference between the normal programming experience versus what benchmarks represent? Where do benchmarks fall short, do you think, when we're evaluating these models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>we also qualitatively assess these models and actually lean on that a lot in addition to private emails that we have. It's like the vibe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The vibe, yeah, the vibe.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's like the vibe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>The vibe benchmark, human benchmark, the humans. You pull in the humans to do a vibe check.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. That's what I do. Just reading online forums and Reddit and X. Well, I don't know how to properly load in people's opinions because they'll say things like, \"I feel like Claude or GPT has gotten dumber,\" or something. They'll say, \"I feel like...\" And then I sometimes feel like that too, but I wonder if it's the model's problem or mine.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that Claude's degraded performance had to do with maybe using the quantized version that existed on AWS Bedrock versus whatever was running on Anthropics GPUs.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I interview a bunch of people that have conspiracy theories. I'm glad you spoke to this conspiracy.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Well, it's not like conspiracy theory as much as humans. Humans are humans and there's these details-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yes.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>queasy amount of flops and chips are messy and man, you can just have bugs. It's hard to overstate how hard bugs are to avoid.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the role of a good prompt in all of this? We mentioned that benchmarks have really structured, well-formulated prompts. What should a human be doing to maximize success and what's the importance of what the humans... You wrote a blog post on... You called it Prompt Design.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>have the raw data that went into the prompt, and then you can see, \"Did my change actually improve it for this entire eval set?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>So do you literally prompt with JSX?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that is farther away. And then eventually when it's rendered, it figures out how many lines can actually fit and it centers around that thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's amazing.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>blocks from the entire code base, you could use retrieval and things like embedding and re-ranking scores to add priorities for you through these components.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So should humans when they ask questions, also try to use something like that? Would it be beneficial to write JSX in the problem or the whole idea is this should be loose and messy?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you, and then our job is to figure out how do we actually retrieve the relative event things so that your thinking actually makes sense?<|eot_id|><|start_header_id|>assistant<|end_header_id|>Well, this is the discussion I had with Aravind of Perplexity is his whole idea is you should let the person be as lazy as he wants. That's a beautiful thing, but I feel like you're allowed to ask more of programmers, right?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yes.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So if you say, \"Just do what you want,\" humans are lazy. There's a tension between just being lazy versus provide more as be prompted... Almost like the system pressuring you or inspiring you to be articulate. Not in terms of the grammar of the sentences, but in terms of the depth of thoughts that you convey inside the prompts.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>generations, \"Given the uncertainty present in your query so far, why don't we just actually show you all of those and let you pick them?\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>How hard is it for the model to choose to talk back versus generally... It's hard, how deal with the uncertainty. Do I choose to ask for more information to reduce the ambiguity?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>both there's the phase where you're writing a prompt and there's... Before you even click, \"Enter,\" maybe we can help resolve some of the uncertainty.<|eot_id|><|start_header_id|>assistant<|end_header_id|>To what degree do you use agentic approaches? How useful are agents?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>We think agents are really, really cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>would love to have an agent that just goes off, does it, and then a day later, I come back and I review the thing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>You mean it goes, finds the right file?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think you actually want a system that's instant, that gives you an initial version instantly back and then you can iterate super, super quickly.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about something like that recently came out, replica agent, that does also setting up the development environment and solving software packages, configuring everything, configuring the databases and actually deploying the app. Is that also in the set of things you dream about?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I think so. I think that would be really cool. For certain types of programming, it would be really cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is that within scope of Cursor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>part of your PR, then you have some initial piece of code that you can iterate on. And so that would also be really cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One of the things we already talked about is speed, but I wonder if we can just linger on that some more in the various places that the technical details involved in making this thing really fast. So every single aspect of Cursor, most aspects of Cursor feel really fast. Like I mentioned, the Apply is probably the slowest thing. And for me from... I'm sorry, the pain on Arvid's face as I say that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>I know. It's a pain. It's a pain that we're feeling and we're working on fixing it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, it says something that feels... I don't know what it is, like one second or two seconds, that feels slow. That means that actually shows that everything else is just really, really fast. So is there some technical details about how to make some of these models, how to make the chat fast, how to make the diffs fast? Is there something that just jumps to mind?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then when they press enter, there's very few tokens it actually has to pre-fill and compute before starting the generation. This will significantly lower TTFT.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you explain how KV cache works?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah, so the way transformers work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I like it.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you're reusing those keys and values that have been computed, which is the only kind of sequential part or sequentially dependent part of the transformer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Is there higher level caching of caching of the prompts or that kind of stuff that could help?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>separate set of keys and values and query that kind of pairwise match up versus compressing that all into one in that interaction at least.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay, and all of that is dealing with being memory bound. I mean, ultimately, how does that map to the user experience? Trying to get the-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>parallel. So you could increase either those dimensions, right? The batch size or the size of your prompts without degrading the latency of generating tokens.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Arvid, you wrote a blog post Shadow Workspace: Iterating on Code in the Background. So what's going on [inaudible 01:04:59]?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that VS Code doesn't need to have all of the different languages built into VS Code but rather you can use the existing compiler infrastructure.<|eot_id|><|start_header_id|>assistant<|end_header_id|>For linting purposes, what-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's for linting. It's for going to definition and for seeing the right types that you're using.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it's doing type checking also.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you're working in a big project, you kind of need that. If you don't have that, it's really hard to code in a big project.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you say, again, how that's being used inside Cursor, the language server protocol communication thing?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>save it because it's still the same folder and then can get feedback from the linters and go to definition and iterate on their code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So literally run everything in the background as if... Right, maybe even run the code.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the lock from the language server if you're trying to do things concurrently or from the Shadow Workspace if you're trying to do things concurrently.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's such an exciting future by the way. It's a bit of a tangent, but to allow a model to change files, it's scary for people but it's really cool, to be able to just let the agent do a set of tasks and you come back the next day and kind of observe like it's a colleague or something like that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>agents you want for coding? Do you want them to find bugs? Do you want them to implement new features? What agents do you want?<|eot_id|><|start_header_id|>assistant<|end_header_id|>So by the way, when I think about agents, I don't think just about coding. I think so for this particular podcast, there's video editing and a lot of... If you look in Adobe, a lot... There's code behind. It's very poorly documented code, but you can interact with Premiere, for example, using code, and basically all the uploading, everything I do on YouTube, everything as you could probably imagine, I do all of that through code and including translation and overdubbing, all of this. So I envision all of those kinds of tasks. So automating many of the tasks that don't have to do directly with the editing, so that. Okay, that's what I was thinking about. But in terms of coding, I would be fundamentally thinking about bug finding, many levels of kind of bug finding and also bug finding like logical bugs, not logical like spiritual bugs or something. Ones like big directions of implementation, that kind of stuff.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>are so bad at bug finding when just naively prompted to find a bug. They're incredibly poorly calibrated. Even the smartest models. Exactly, even o1.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you explain that? Is there a good intuition?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>is the user and like- But even then if you're putting in a maximum paranoia, it still just doesn't quite get it. Yeah, yeah. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, but this is hard for humans too to understand which line of code is important, which is not. I think one of your principles on a website says if a code can do a lot of damage, one should add a comment that say, \"This line of code is dangerous.\"<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>And all caps, repeated 10 times.<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, you say for every single line of code inside the function you have to... And that's quite profound, that says something about human beings because the engineers move on, even the same person might just forget how it can sink the Titanic a single function. You might not intuit that quite clearly by looking at the single piece of code.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>dangerous, dangerous in every single line, the models will pay more attention to that and will be more likely to find bugs in that region.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's actually just straight up a really good practice of labeling code of how much damages can do.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>change and think, \"Oh, I know how to do this,\" and you really need to point it out to them so that doesn't slip through.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. You have to be reminded that you could do a lot of damage that's like we don't really think about that. You think about, \"Okay, how do I figure out how this works so I can improve it?\" You don't think about the other direction that it could-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>evolve the spec languages to capture some of the things that they don't really capture right now. I don't know. I think it's very exciting.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And you're speaking not just about single functions, you're speaking about entire code bases.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>could actually prove that a language model is aligned for example, or you can prove that it actually gives the right answer. That's the dream.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, that is... I mean, if it's possible. That's your I have a dream speech. If it's possible, that will certainly help with making sure your code doesn't have bugs and making sure AI doesn't destroy all human civilization. So the full spectrum of AI safety to just bug finding. So you said the models struggle with bug finding. What's the hope?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>zap that with tons and tons and tons of compute, and you're willing to put in $50 to solve that bug or something even more.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Have you thought about integrating money into this whole thing? I would pay probably a large amount of money if you found a bug or even generated code that I really appreciated. I had a moment a few days ago when I started using Cursor where it generated perfect three functions for interacting with the YouTube API to update captions for localization in different languages. The API documentation is not very good and the code across, if I... I googled it for a while. I couldn't find exactly, there's a lot of confusing information, and Cursor generated perfectly. I just sit back, I read the code, I was like, \"This is correct. I tested it, it's correct.\" I was like, \"I want to tip.\" I want a button that goes, \"Here's $5.\" One that's really good just to support the company and support what the interface is. And the other is that probably sends a strong signal like good job. So there's this much stronger signal than just accepting the code. You just actually send a strong good job. That and for bug finding, obviously, there's a lot of people that would pay a huge amount of money for a bug bounty thing, right? You guys think about that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>actually makes more sense to separate it out, and you pay some fee every month, and then you get all of these things for free.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But there could be a tipping component which is not like it cost this-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you could actually somehow verify, \"Oh, I have fixed the bug,\" maybe then the bounty system doesn't need to rely on the honor system too.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How much interaction is there between the terminal and the code? How much information is gained from if you run the code in the terminal? Can you do a loop where it runs the code and suggests how to change the code? If the code and runtime gets an error? Is right now there's separate worlds completely? I know you can do control K inside the terminal to help you write the code.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of sense. There's a question of whether it happens in the foreground too or if it happens in the background like what we've been discussing.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure. The background's pretty cool. I could be running the code in different ways. Plus there's a database side to this, which how do you protect it from not modifying the database, but okay.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>or something. It would be really interesting if you could branch a file system, right? Yeah. I feel like everything needs branching. It's like- Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. The problem with the multiverse, right? If you branch on everything that's like a lot.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>There's obviously these super clever algorithms to make sure that you don't actually use a lot of space or CPU or whatever.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. This is a good place to ask about infrastructure. So you guys mostly use AWS, what are some interesting details? What are some interesting challenges? Why'd you choose AWS? Why is AWS still winning? Hashtag.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>an AWS product, you just know that it's going to work. It might be absolute hell to go through the steps to set it up.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Why is the interface so horrible?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Because it's- It's just so good. It doesn't need to-<|eot_id|><|start_header_id|>assistant<|end_header_id|>It's the nature of winning.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>they're winning. Yeah, yeah. But AWS we can always trust, it will always work. And if there is a problem, it's probably your problem. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Okay. Is there some interesting challenges to... You guys are pretty new startup to scaling, to so many people and-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and so on. But you only do that in the scenario where things don't match. For most people, most of the time, the hashes match.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So it's like a hierarchical reconciliation-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... of hashes-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Something like that. Yeah, it's called a Merkle tree.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, Merkle. Yeah. Yeah, this is cool to see that you have to think through all these problems.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>any code on our servers at all. No code data is stored. We just store the vectors in the vector database and the vector cache.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's the biggest gains at this time you get from indexing the code base? Just out of curiosity, what benefit do users have? It seems like longer term, there'll be more and more benefit, but in the short term, just asking questions of the code base, what's the usefulness of that?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot on improving the quality of our retrieval. I think the ceiling for that is really, really much higher than people give the credit for.<|eot_id|><|start_header_id|>assistant<|end_header_id|>One question that's good to ask here, have you considered and why haven't you much done local stuff to where you can do the... It seems like everything was just discussed as exceptionally difficult to do. To go to the cloud, you have to think about all these things with the caching and the large code base where a large number of programmers are using the same code base. You have to figure out the puzzle of that. A lot of it, most software just does this heavy computational stuff locally. So have you considered doing embeddings locally?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>really, really hard to run for almost all people, locally. Don't you want the most capable model? You want [inaudible 01:38:55] too? And also o1-<|eot_id|><|start_header_id|>assistant<|end_header_id|>I like how you're pitching me.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>o1 is another-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Would you be satisfied with an inferior model? Listen, yes, I'm one of those, but there's some people that like to do stuff locally, especially like... Really, there's a whole obviously open source movement that resists. It's good that they exist actually because you want to resist the power centers that are growing our-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then you start doing bad things with a lot of the world's data. So I am very hopeful that we can solve homomorphic encryption for-<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, and-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>... language model inference.<|eot_id|><|start_header_id|>assistant<|end_header_id|>... doing privacy, preserving machine learning. But I would say, that's the challenge we have with all software these days. It's like there's so many features that can be provided from the cloud and all us increasingly rely on it and make our life awesome. But there's downsides, and that's why you rely on really good security to protect from basic attacks. But there's also only a small set of companies that are controlling that data, and they obviously have leverage and they could be infiltrated in all kinds of ways. That's the world we live in. So it's-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>use your own encryption keys, and AWS can't really do much. But here, it's just centralized actors that see the exact plain text of everything.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. On the topic of a context, that's actually been a friction for me. When I'm writing code in Python, there's a bunch of stuff imported. You could probably intuit the kind of stuff I would like to include in the context. How hard is it to auto figure out the context?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>separately train a really good open source model to be the retriever, to be the thing that feeds in the context to these larger models.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you speak a little more to post-training a model to understand the code base? What do you mean by that? Is this a synthetic data direction? Is this-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>then add those as instruction fine-tuning data points. And then in theory, this might unlock the model's ability to answer questions about that code base.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Let me ask you about OpenAI o1. What do you think is the role of that kind of test time compute system in programming?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the 99.9% of queries, then you have a way of inference time running it longer for those few people that really, really want max intelligence.<|eot_id|><|start_header_id|>assistant<|end_header_id|>How do you figure out which problem requires what level of intelligence? Is that possible to dynamically figure out when to use GPT-4, when to use a small model and when you need the o1?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you need to determine if the thing is too hard for the four level model? Maybe you need the o1 level model. It's really unclear.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But you mentioned this. So there's a pre-training process then there's post-training, and then there's test time compute. Is that fair to separate? Where's the biggest gains?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>post-training, but maybe the compute spent for this kind of... forgetting test time compute to work for a model is going to dwarf pre-training eventually.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So we don't even know if o1 is using just chain of thought or we don't know how they're using any of these? We don't know anything?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's fun to speculate.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If you were to build a competing model, what would you do?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>explore multiple paths of this chain of thought and then use these process reward models to evaluate how good is this branch that you're taking.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah. When the quality of the branch is somehow strongly correlated with the quality of the outcome at the very end, so you have a good model of knowing which branch to take. So not just in the short term, in the long term?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>papers. I haven't seen anything super that seems to work really well for using the process reward models creatively to do tree search and code.<|eot_id|><|start_header_id|>assistant<|end_header_id|>This is an AI safety, maybe a bit of a philosophy question. So OpenAI says that they're hiding the chain of thought from the user, and they've said that that was a difficult decision to make. Instead of showing the chain of thought, they're asking the model to summarize the chain of thought. They're also in the background saying they're going to monitor the chain of thought to make sure the model is not trying to manipulate the user, which is a fascinating possibility. But anyway, what do you think about hiding the chain of thought?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>chain of thought to replicate the technology, because pretty important data, like seeing the steps that the model took to get to the final results.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you could probably train on that also?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>the size of the models and pre-training and finding tricks there, you'll now have this other thread of getting search to work better and better.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So let me ask you about strawberry tomorrow eyes. So it looks like GitHub Copilot might be integrating o1 in some kind of way, and I think some of the comments are saying, does this mean Cursor is done? I think I saw one comment saying that.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>It's a time to shut down Cursor. Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Time to shut down Cursor.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>[inaudible 01:58:38].<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thank you. So is it time to shut down Cursor?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that you don't realize are working for you in every facet of the product, as well as the really thoughtful UX with every single feature.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. From that profound answer- All right, from that profound answer, let's descend back down to the technical. You mentioned you have a taxonomy of synthetic data.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Oh yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Can you please explain?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>perfect verifier feels really, really hard to do with just open-ended miscellaneous tasks. You give the model or more long horizon tasks, even in coding.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That's because you're not as optimistic as Arvid. But yeah, so yeah, that third category requires having a verifier.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>output. Yeah. No, that's- I'm sure it's somewhere in between. Yeah. I think that's the category that is most likely to result in massive gains.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about RL with feedback side RLHF versus RLAIF? What's the role of that in getting better performance on the models?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>with exactly with what you want. It looks different than I think normal RLHF where you're usually training these reward models in tons of examples.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What's your intuition when you compare generation and verification or generation and ranking? Is ranking way easier than generation?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>P does not equal NP, then there's this massive class of problems that are much, much easier to verify given proof, than actually proving it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I wonder if the same thing will prove P not equal to NP or P equal to NP.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>That would be really cool.<|eot_id|><|start_header_id|>assistant<|end_header_id|>That'd be a whatever Field's Medal by AI. Who gets the credit? Another the open philosophical question.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>one AI will get the Field's Medal will be. I actually don't have- Isn't this Aman's specialty? I don't know what Aman's bet here is.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Oh, sorry, Nobel Prize or Field's Medal first?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Field's Medal- Oh, Field's Medal level? Field's Medal comes first, I think. [inaudible 02:06:41].<|eot_id|><|start_header_id|>assistant<|end_header_id|>Field's Medal comes first. Well, you would say that, of course.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>But it's also this isolated system you verify and...<|eot_id|><|start_header_id|>assistant<|end_header_id|>Sure.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>versed in the space of theorem proving now. And two, less intuition about how close we are to solving these really, really hard open problems.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you think you'll be Field's Medal first? It won't be in physics or in-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>get Field's Medal before AGI. I mean, I'd be very happy. I'd be very happy. But I don't know if I... I think 2028, 2030.<|eot_id|><|start_header_id|>assistant<|end_header_id|>For Field's Medal?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Field's Medal.<|eot_id|><|start_header_id|>assistant<|end_header_id|>All right. It feels like forever from now, given how fast things have been going. Speaking of how fast things have been going, let's talk about scaling laws. So for people who don't know, maybe it's good to talk about this whole idea of scaling laws. What are they, where'd you think stand, and where do you think things are going?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>you care most about that inference budget for really long context windows. So it'll be interesting to see how people play with all these dimensions.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So yeah, I mean you speak to the multiple dimensions, obviously. The original conception was just looking at the variables of the size of the model as measured by parameters, and the size of the data as measured by the number of tokens, and looking at the ratio of the two.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Yeah.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And it's kind of a compelling notion that there is a number, or at least a minimum. And it seems like one was emerging. Do you still believe that there is a kind of bigger is better?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>spending the compute of literally training this 27 billion parameter model on all these tokens, just to get out this, I don't know, smaller model.<|eot_id|><|start_header_id|>assistant<|end_header_id|>And the distillation gives you just a faster model, smaller means faster.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>this smaller one. And maybe we can get more signal per token for this much smaller model than we would've originally if we trained it.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So if I gave you $10 trillion, how would you spend it? I mean you can't buy an island or whatever. How would you allocate it in terms of improving the big model versus maybe paying for HF in the RLHF? Or-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>lot of disbelief and assuming you had the know- how, or if you're saying you have to operate with the limited information you have now-<|eot_id|><|start_header_id|>assistant<|end_header_id|>No, no, no. Actually, I would say you swoop in and you get all the information, all the little heuristics, all the little parameters, all the parameters that define how the thing is trained.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Mm-hmm.<|eot_id|><|start_header_id|>assistant<|end_header_id|>If we look in how to invest money for the next five years in terms of maximizing what you called raw intelligence-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that we're sort of idea-limited, but there's always that like- But if you have a lot of compute, you can run a lot of experiments.<|eot_id|><|start_header_id|>assistant<|end_header_id|>So you would run a lot of experiments versus use that compute to trend a gigantic model?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>and engineering talent to exploring all these other ideas, when there's this core thing that will probably improve performance for some decent amount of time.<|eot_id|><|start_header_id|>assistant<|end_header_id|>But also, these big labs like winning. So they're just going wild. Okay, so big question, looking out into the future: You're now at the center of the programming world. How do you think programming, the nature of programming changes in the next few months, in the next year, in the next two years and the next five years, 10 years?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>of, you can even go down into the code, you can go at higher levels of abstraction, while also giving you these big productivity gains.<|eot_id|><|start_header_id|>assistant<|end_header_id|>It'd be nice if you can go up and down the abstraction stack.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>that's really well specified. But that's not most of programming, and that's also not most of the programming we think a lot of people value.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What about the fundamental skill of programming? There's a lot of people, like young people right now kind of scared, because they love programming, but they're scared about, \"Will I be able to have a future if I pursue this career path?\" Do you think the very skill of programming will change fundamentally?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>a little bit less about carefulness, which I think is really important today if you're a programmer. I think it'll be a lot more fun.<|eot_id|><|start_header_id|>assistant<|end_header_id|>What do you guys think?<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>getting in the code and taking a crack at seeing how it lays out and then iterating really quickly on that. That feels more fun.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Yeah, just speaking to generate the boilerplate, is great. So you just focus on the nuanced, difficult design decisions. Migration, I feel like this is a cool one. It seems like a larger language models is able to basically translate for one program language to another. Or translate, migrate in the general sense of what migrate is. But that's in the current moment. So mean the fear has to do with, okay, as these models get better and better, then you're doing less and less creative decisions. And is it going to kind of move to a place where you're operating in the design space of natural language where natural language is the main programming language? And I guess I could ask that by way of advice. If somebody's interested in programming now, what do you think they should learn? You guys started in some Java and I forget the... Oh, some PHP.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>PHP. PHP. Objective-C.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Objective-C. There you go. I mean in the end, we all know JavaScript was going to win and not TypeScript. It's going to be like vanilla JavaScript. It's just going to eat the world and maybe live with PHP. And I mean it also brings up the question of, I think Don Knuth has this idea that some percent of the population is geeks, and there's a particular kind of psychology in mind required for programming. And it feels like more and more that expands the kind of person that should be able to, can do great programming, might expand.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>programming, I think makes, really, the best programmers. And I think these types of people will really get into the details of how things work.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I guess the question I'm asking, that exact programmer, let's think about that person. When the super tab, the super awesome praise be the tab succeeds, and you keep pressing tab-<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>higher bandwidth. The communication to the computer just becomes higher and higher bandwidth as opposed to just typing as much lower bandwidth than communicating intent.<|eot_id|><|start_header_id|>assistant<|end_header_id|>I mean, this goes to your manifesto titled Engineering Genius. \"We are an applied research lab building extraordinary productive human AI systems.\" So speaking to this hybrid element. \"To start, we're building the engineer of the future, a human AI programmer that's an order of magnitude more effective than any one engineer. This hybrid engineer will have effortless control over their code base and no low entropy keystrokes. They will iterate at the speed of their judgment, even in the most complex systems. Using a combination of AI and human ingenuity they will outsmart and out engineer the best pure AI systems. We are a group of researchers and engineers. We build software and models to invent at the edge of what's useful and what's possible. Our work has already improved the lives of hundreds of thousands of programmers.\" And on the way to that, we'll at least make programming more fun. So thank you for talking today.<|eot_id|>"}
{"text": "<|begin_of_text|><|start_header_id|>user<|end_header_id|>Thank you. Thanks for having us. Thank you. Thank you.<|eot_id|><|start_header_id|>assistant<|end_header_id|>Thanks for listening to this conversation with Michael, Sualeh, Arvid and Aman. To support this podcast. Please check out our sponsors in the description. And now let me leave you with a random, funny and perhaps profound programming code I saw on Reddit. Nothing is as permanent as a temporary solution that works. Thank you for listening and hope to see you next time.<|eot_id|>"}
